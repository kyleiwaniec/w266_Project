python train.py --positive_data_file data/pos_data_train --negative_data_file data/neg_data_train --evaluate_every 100 --dev_sample_percentage .005

Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=64
CHECKPOINT_EVERY=100
DEV_SAMPLE_PERCENTAGE=0.005
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=128
EVALUATE_EVERY=100
FILTER_SIZES=3,4,5
L2_REG_LAMBDA=0.0
LOG_DEVICE_PLACEMENT=False
NEGATIVE_DATA_FILE=data/neg_data_train
NUM_EPOCHS=200
NUM_FILTERS=128
POSITIVE_DATA_FILE=data/pos_data_train

Loading data...
('l-positive_examples', 71399)
('l-negative_examples', 208903)
('type', 280302, 280302)
["i'm", 'going', 'to', 'try', 'to', 'explain', 'what', 'perfect', 'squares', 'are,', 'and', 'more', 'importantly,', 'why', 'they', 'are', 'so', 'important', 'for', 'radicals', 'anyway.', "i'll", 'go', 'through', 'detail', 'explaining', 'how', 'to', 'do', 'them,', 'so', 'that', 'hopefully', 'you', 'can', 'better', 'understand', 'the', 'perfect', 'square', 'part', 'of', 'simplifying', 'radicals.', 'this', 'will', 'be', 'a', 'bit', 'lengthy,', 'so', 'please', 'pardon', 'me', 'on', 'that,', '(you', 'may', 'want', 'to', 'print', 'this', 'out),', 'but', 'i', 'hope', 'that', 'it', 'will', 'give', 'you', 'an', 'understanding', 'of', 'how', 'to', 'do', 'them.', "i'll", 'also', 'show', 'you', 'why', 'the', 'techniques', 'actually', 'work,', 'which', 'is', 'something', 'that', 'some', 'teachers', 'forget', 'when', 'they', 'teach', 'these', 'kinds', 'of', 'concepts.', "\r\n\r\ni'll", 'start', 'by', 'defining', 'the', 'perfect', 'square.', "that's", 'the', 'easy', 'part,', 'actually.', '\r\n\r\na', 'perfect', 'square', 'is', 'just', 'a', 'number', 'where', 'the', 'square', 'root', 'of', 'that', 'number', 'is', 'a', 'whole', 'number.', 'people', 'make', 'it', 'sound', 'really', 'complicated,', 'but', "it's", 'really', 'a', 'very', 'simple', 'concept.', 'to', 'illustrate:', '\r\n\r\n9:', 'the', 'square', 'root', 'of', 'this', 'number', 'is', '3,', 'which', 'is', 'a', 'whole', 'number,', 'thus', '9', 'is', 'a', 'perfect', 'square.', '\r\n\r\n11:', 'the', 'square', 'root', 'of', '11', 'is', '3.317', '(rounded', 'to', 'the', 'nearest', 'thousandth)this', 'is', 'not', 'a', 'whole', 'number,', 'so', '11', 'is', 'not', 'a', 'perfect', 'square.', '\r\n\r\n64:', 'the', 'square', 'root', 'of', '64', 'is', '8,', 'which', 'is', 'a', 'whole', 'number,', 'so', '8', 'is', 'a', 'perfect', 'square.\r\n\r\n188:', 'the', 'square', 'root', 'of', '188', 'is', '13.711,', '(rounded', 'to', 'the', 'nearest', 'thousandth),', 'and', 'this', 'is', 'not', 'a', 'whole', 'number,', 'so', '188', 'is', 'not', 'a', 'perfect', 'square.', '\r\n\r\nthis', 'is', 'all', 'that', 'a', 'perfect', 'square', 'is.', "it's", 'just', 'a', 'number', 'that', 'has', 'a', 'whole', 'number', 'for', 'a', 'square', 'root.', 'most', 'people', 'define', 'it', 'in', 'a', 'more', 'confusing', 'way,', 'but', 'really,', 'this', 'definition', 'right', 'here', 'will', 'do', 'just', 'fine.', 'you', 'can', 'find', 'the', 'square', 'root', 'of', 'a', 'number', 'in', 'a', 'calculator', 'just', 'by', 'entering', 'the', 'number,', 'then', 'pressing', 'the', 'radical', 'sign', 'on', 'it.', '', '\r\n\r\nas', 'to', 'why', 'they', 'are', 'called', '"perfect"', 'squares,', 'you', 'can', 'think', 'of', 'it', 'like', 'this.', 'a', 'perfect', 'square', 'has', 'a', 'whole', 'number', 'for', 'a', 'square', 'root,', 'but', 'non-perfect', 'squares', 'have', 'decimal', 'numbers', 'for', 'square', 'roots.', 'not', 'only', 'that,', 'but', 'any', 'time', 'a', 'square', 'root', 'is', 'a', 'decimal', 'number,', 'it', 'is', 'also', 'irrational.', 'there', 'are', 'different', 'definitions', 'for', 'what', 'an', 'irrational', 'number', 'is,', 'but', 'basically', 'it', 'is', 'just', 'a', 'number', 'where', 'the', 'decimals', 'go', 'on', 'forever.', 'a', 'calculator', 'will', 'always', 'try', 'to', 'round', 'it', 'to', 'a', 'certain', 'number', 'of', 'places,', 'but', 'the', 'number', 'really', 'goes', 'on', 'forever', 'if', "it's", 'a', 'decimal.', '\r\n\r\nhaving', 'said', 'that,', 'with', 'non-perfect', 'squares,', 'since', 'you', "can't", 'write', 'an', 'infinite', 'number', 'of', 'digits,', 'you', 'can', 'never', 'get', '"perfectly"', 'accurate.', 'you', 'would', 'need', 'an', 'infinite', 'number', 'of', 'digits', 'to', 'get', 'perfectly', 'accurate', 'with', 'square', 'roots', 'that', 'have', 'decimals.', '', 'if', 'this', 'is', 'confusing,', "don't", 'worry', 'about', 'it,', 'but', 'just', 'stick', 'with', 'the', 'whole', 'number', 'definition', 'that', 'i', 'gave.', 'i', 'just', 'hope', 'that', 'it', 'gives', 'you', 'a', 'little', 'intuition', 'behind', 'why', 'they', 'are', 'even', 'called', 'perfect', 'squares.', ':)', '\r\n\r\nanyway...\r\nwhen', 'simplifying', 'radicals,', 'anything', 'you', 'take', 'out', 'of', 'the', 'radical', 'sign,', 'you', 'have', 'to', 'take', 'the', 'square', 'root', 'of', 'it.', 'the', 'reason', 'that', 'you', 'can', 'only', 'take', 'perfect', 'squares', 'out', 'is', 'that', 'perfect', 'squares', 'give', 'whole', 'numbers', 'for', 'square', 'roots,', 'while', 'non-perfect', 'squares', 'do', 'not.', 'we', "don't", 'want', 'big', 'irrational', 'numbers', 'out', 'in', 'front', 'of', 'our', 'radical', 'sign,', 'so', 'it', 'is', 'undoubtedly', 'convenient', 'to', 'only', 'be', 'taking', 'out', 'perfect', 'squares.', '\r\n\r\n', 'to', 'better', 'understand', 'this,', 'it', 'helps', 'to', 'see', 'the', 'radical', 'as', 'what', 'it', 'is.', 'when', 'you', 'see', 'a', 'number', 'inside', 'a', 'radical', 'sign,', 'what', 'it', 'is', 'actually', 'saying', 'is', 'take', 'the', 'square', 'root', 'of', 'whatever', 'is', 'in', 'there,', 'so', 'it', 'goes', 'that', 'way', 'with', 'factors', 'inside', 'that', 'number', 'too.', "i'll", 'show', 'you', 'how', 'to', 'identify', 'the', 'perfect', 'squares', 'later.', 'as', 'for', 'how', 'i', 'am', 'going', 'to', 'give', 'examples,', 'i', "can't", 'actually', 'write', 'a', 'radical', 'sign,', 'so', "i'm", 'going', 'to', 'have', 'to', 'improvise.', 'whenever', 'you', 'see', '"sqrt', '(?)",', 'it', 'is', 'the', 'same', 'thing', 'as', 'a', 'radical', 'sign', 'where', 'the', 'number', 'in', 'the', 'parentheses', 'is', 'under', 'the', 'radical', 'sign.', "\r\n\r\nlet's", 'look', 'at', 'sqrt', '(64).', 'remember', 'that,', 'in', 'this', 'notation,', 'it', 'is', 'the', 'same', 'thing', 'as', 'a', '64', 'under', 'the', 'radical', 'sign.', 'again,', 'i', "can't", 'write', 'radical', 'signs', 'in', 'this', 'answer', 'box.', '\r\n\r\nanyway,', 'what', 'this', 'is', 'representing', 'is', 'the', 'square', 'root', 'of', '64.', 'the', 'square', 'root', 'of', '64', 'is', 'just', '8,', 'so', 'sqrt', '(64)', 'is', 'the', 'same', 'thing', 'as', 'just', '8.', 'we', '"took', 'out"', 'the', '64,', 'and', 'took', 'the', 'square', 'root,', 'but', 'nothing', 'was', 'left,', 'so', 'the', 'radical', 'was', 'completely', 'eliminated.', 'building', 'on', 'this,', "let's", 'look', 'at', 'a', 'slightly', 'harder', 'problem', 'that', 'actually', 'requires', 'us', 'to', 'factor', 'it', 'out.', '(since', 'the', 'one', 'i', 'just', 'showed', 'you', 'is', 'just', 'to', 'help', 'you', 'get', 'familiar', 'with', 'these', 'concepts', 'so', 'far)', "\r\n\r\nlet's", 'try', 'sqrt', '(128).', 'here,', 'we', "can't", 'just', 'take', 'the', 'square', 'root', 'like', 'we', 'could', 'last', 'time.', 'we', 'have', 'to', 'factor,', 'and', "that's", 'because', '128', 'is', 'not', 'a', 'perfect', 'square.', 'this', 'may', 'seem', 'very', 'confusing', 'at', 'first,', 'but', 'please', 'stay', 'with', 'me.', "i'll", 'explain', 'what', "i'm", 'doing', 'in', 'just', 'a', 'moment,', 'so', "don't", 'get', 'discouraged.\r\n\r\nwe', 'can', 'factor', '128', 'into', 'a', '2,', 'and', '64.', 'you', 'may', 'notice', 'that', 'i', "didn't", 'fully', 'factor', 'them', 'down,', 'and', 'there', 'is', 'a', 'reason', 'for', 'that.', 'when', 'simplifying', 'radicals,', "it's", 'only', 'necessary', 'that', 'you', 'factor', 'it', 'down', 'to', 'find', 'perfect', 'squares.', 'if', '64', "didn't", 'immediately', 'jump', 'out', 'to', 'you', 'as', 'the', 'largest', 'perfect', 'square', 'in', '128,', "don't", 'worry.', "i'll", 'explain', 'a', 'little', 'shortcut', 'later', 'using', 'prime', 'factorization,', 'and', "it's", 'often', 'a', 'preferred', 'method.', 'for', 'now,', 'just', 'understand', 'that', '64', 'is', 'the', 'largest', 'perfect', 'square', 'that', 'is', 'also', 'a', 'factor', 'of', '128.', '\r\n\r\n', '', 'thus,', 'we', 'can', 'go', 'ahead', 'and', 'take', 'the', 'perfect', 'square', 'out,', 'take', 'the', 'square', 'root', 'of', 'it,', 'then', 'multiply', 'it', 'by', 'the', 'rest', 'of', "what's", 'left', 'in', 'the', 'radical', 'sign.\r\n\r\nso,', 'taking', 'the', 'square', 'root', 'of', 'that', 'big', 'perfect', 'square,', 'we', 'have', '8.', "let's", 'go', 'ahead', 'and', 'rewrite', 'it:\r\n\r\nthis', 'is', '8', 'sqrt', '(2).', "that's", 'equivalent', 'to', 'eight', 'times', 'the', 'square', 'root', 'of', 'two.', 'we', 'left', 'what', 'we', 'could', 'not', 'take', 'out', 'inside', 'the', 'square', 'root', 'sign.', '(in', 'our', 'case,', 'that', 'was', 'a', '2,', 'since', '2', 'is', 'not', 'a', 'perfect', 'square.', ')', 'note', 'that', 'we', 'did', 'not', 'need', 'to', 'write', 'the', 'multiplication', 'symbol', 'between', 'the', '8', 'and', 'the', 'sqrt', 'symbol.', "that's", 'always', 'implied,', 'so', 'when', 'you', 'see', 'something', 'like', 'that,', "you'll", 'know', 'that', 'there', 'is', 'really', 'an', 'imaginary', 'multiplication', 'symbol', 'between', 'the', 'number', 'out', 'in', 'front', 'and', 'the', 'sqrt', 'symbol.', '', '\r\n\r\nyou', 'may', 'be', 'wondering', 'about', 'how', 'to', 'identify', 'the', 'perfect', 'squares,', 'and', 'if', 'you', 'need', 'to', 'try', 'and', 'look', 'for', 'the', '"largest', 'perfect', 'square"', 'every', 'time.', 'sometimes', "it's", 'a', 'lot', 'simpler', 'just', 'to', 'do', 'the', 'prime', 'factorization', 'if', 'you', 'are', 'having', 'trouble', 'with', 'identifying', 'the', 'perfect', 'squares,', 'and', 'that', 'will', 'still', 'get', 'you', 'the', 'exact', 'same', 'answer,', 'although', 'it', 'will', 'mean', 'more', 'steps.', "it's", 'actually', 'a', 'very', 'interesting', 'method,', 'and', "i'll", 'actually', 'show', 'you', 'how', 'it', 'works', 'later,', 'after', 'a', 'few', 'examples,', 'but', "i'll", 'illustrate', 'how', 'to', 'do', 'it', 'first.', 'using', 'the', 'prime-factorization', 'method,', 'we', 'would', 'have', 'gotten:', '\r\n\r\n2', '*', '2', '*', '2', '*', '2', '*', '2', '*', '2', '*', '2.', "that's", '2^7,', 'or', 'seven', '2s', 'multiplied', 'out', 'together.', 'with', 'this', 'method,', 'we', 'can', 'take', 'any', 'pair', 'of', 'these', 'twos', 'and', 'make', 'it', 'a', 'perfect', 'square.', 'here,', 'we', 'actually', 'have', 'three', 'different', 'pairs', 'of', 'twos.', 'when', 'that', 'happens,', 'we', 'can', 'actually', 'just', 'multiply', 'these', 'pairs', 'together,', 'and', 'when', 'we', 'do', 'that,', 'we', 'end', 'up', 'with', '64,', 'which', 'probably', 'seems', 'quite', 'familiar,', 'lol,', 'considering', 'the', 'last', 'time', 'we', 'did', 'this', 'problem.', 'take', 'the', 'square', 'root', 'of', 'that,', 'and', 'we', 'end', 'up', 'with', '8,', 'and', 'the', 'only', 'factor', 'left', 'that', 'we', 'could', 'not', '"use"', 'was', 'that', 'one', '2', 'that', 'we', "couldn't", 'pair', 'up.', '\r\n\r\n', 'that', '2', 'that', 'is', 'left', 'does', 'not', 'have', 'another', '2', 'to', 'get', 'paired', 'with,', 'so', 'it', 'must', 'be', 'left', 'in', 'the', 'radical', 'sign,', 'but', 'then', 'we', 'can', 'multiply', 'the', '8', 'by', 'that', 'radical', 'that', "couldn't", 'be', 'eliminated.', 'we', 'get', 'the', 'same', 'answer', 'we', 'got', 'last', 'time,', 'which', 'is', '8', 'sqrt', '(2).', 'you', 'should', 'see', 'that', 'this', 'was', 'really', 'just', 'a', 'method', 'to', 'find', 'the', 'largest', 'perfect', 'square', 'factor', 'in', 'any', 'number,', 'but', 'it', 'works', 'and', 'it', 'is', 'very', 'helpful', 'if', 'you', 'have', 'trouble', 'identifying', 'the', 'perfect', 'squares.', "it's", 'even', 'more', 'helpful', 'when', 'you', 'are', 'asked', 'to', 'simplify', 'radicals', 'where', 'they', 'represent', 'the', 'cube', 'root,', 'and', 'the', 'like,', 'but', "i'm", 'not', 'going', 'to', 'cover', 'those', 'in', 'this', 'post.', "\r\n\r\ni'll", 'do', 'a', 'couple', 'more', 'examples,', 'and', "i'll", 'use', 'prime', 'factorization', 'to', 'identify', 'the', '"largest', 'perfect', 'square', 'factor"', 'again.', '\r\n\r\nsimplify:', 'sqrt', '(80)\r\n\r\nwe', 'can', 'factor', '80', 'down,', 'then', 'try', 'and', 'multiply', 'up', 'our', 'pairs', 'of', 'numbers.', "let's", 'do', 'the', 'prime', 'factorization', 'of', '80.\r\n\r\n2', '*', '2', '*', '2', '*', '2', '*', '5.\r\n\r\nthat', 'looks', 'like', 'our', 'prime', 'factorization', 'of', 'it.', 'when', 'these', 'numbers', 'are', 'multiplied', 'together,', 'they', 'all', 'equal', '80.\r\n\r\nhere,', 'if', 'you', 'try', 'and', 'pair', 'up', 'numbers,', "you'll", 'see', 'that', 'there', 'are', 'two', 'pairs', 'of', '2', 'available,', 'and', 'there', 'is', 'no', 'five', 'to', 'pair', 'the', 'five', 'with.', 'that', 'means', 'that', 'we', 'can', 'take', 'out', 'those', 'pairs', 'of', '2s,', 'but', 'the', '5', 'must', 'be', 'left', 'in', 'the', 'radical.', "let's", 'go', 'ahead', 'and', 'multiply', 'those', 'pairs', 'of', '2,', 'then', 'take', 'the', 'square', 'root', 'of', 'them.', '\r\n\r\n2', '*', '2', '*', '2', '*', '2', '=', '16.', 'the', 'square', 'root', 'of', '16', 'is', '4,', 'since', '4', '*', '4', '=', '16.', 'notice', 'that', 'we', 'once', 'again', 'ended', 'up', 'with', 'a', 'perfect', 'square', 'by', 'the', 'prime', 'factorization', 'of', 'it.', "that's", 'the', 'nice', 'thing', 'about', 'doing', 'this.', "you'll", 'always', 'end', 'up', 'with', 'the', 'largest', 'perfect', 'square', 'factor', 'of', 'any', 'number', 'you', 'try', 'it', 'on.', '\r\n\r\nanyway,', 'taking', 'the', 'square', 'root', 'of', 'that', '16,', 'we', 'have', '4.', 'so', 'then,', 'we', 'have', 'to', 'multiply', 'that', 'by', 'the', 'radical', 'that', 'could', 'not', 'be', 'eliminated.', 'we', 'get\r\n\r\n4', 'sqrt', '(5).', 'notice', 'that', '5', 'was', 'the', 'only', 'factor', 'that', 'could', 'not', 'be', 'eliminated', 'by', 'pairing.', "\r\n\r\nlet's", 'do', 'one', 'more.', "let's", 'try', 'a', 'bigger', 'problem...\r\n\r\nsimplify:', 'sqrt', '(720).', '720', 'is', 'not', 'a', 'perfect', 'square,', 'so', 'we', "can't", 'just', 'take', 'the', 'square', 'root', 'of', 'that.', "we'll", 'have', 'to', 'factor', 'this', 'number.', '\r\n\r\nit', 'turns', 'out', 'that', 'the', 'prime', 'factorization', 'of', 'this', 'number', 'is', '2', '*', '2', '*', '2', '*', '2', '*', '3', '*', '3', '*', '5.', 'this', 'time,', 'you', 'still', 'see', 'the', 'two', 'pairs', 'of', '2,', 'and', 'you', 'still', 'see', 'that', 'single', '5', 'that', "can't", 'be', 'paired,', 'but', 'you', 'also', 'see', 'one', 'pair', 'of', '3s.', '\r\n\r\n', 'even', 'though', 'we', 'now', 'have', 'both', '2s', 'and', '3s', 'in', 'our', 'list', 'of', 'paired', 'factors,', 'we', 'can', 'still', 'do', 'the', 'exact', 'same', 'thing.', 'we', 'just', 'multiply', 'these', 'pairs', 'up', 'to', 'find', 'the', 'largest', 'perfect', 'square.', '2', '*', '2', '*', '2', '*', '2', '*', '3', '*', '3', 'is', 'actually', '144.', 'it', 'may', 'not', 'immediately', 'jump', 'out', 'to', 'you', 'that', '144', 'is', 'a', 'perfect', 'square,', 'but', 'remember', 'that', 'this', 'method', 'will', 'always', 'give', 'you', 'the', 'largest', 'perfect-square', 'that', 'is', 'also', 'a', 'factor', 'of', 'any', 'number', 'you', 'try', 'it', 'on,', 'so', 'we', 'can', 'trust', 'that', '144', 'is', 'a', 'perfect', 'square.', 'you', 'should', 'be', 'able', 'to', 'see', 'by', 'now', 'that', 'we', 'have', 'to', 'take', 'the', 'square', 'root', 'of', 'this', 'perfect', 'square', 'and', 'write', 'it', 'out', 'front,', 'and', 'the', 'square', 'root', 'of', '144', 'is', 'actually', '12,', 'so', "we'll", 'write', 'that', 'out', 'in', 'front', 'of', 'the', 'square', 'root', 'symbol,', 'and', "we'll", 'leave', 'the', '5', 'that', 'we', "couldn't", 'pair', 'up', 'in', 'the', 'square', 'root', 'symbol.', 'that', 'leaves\r\n\r\n12', 'sqrt', '(5)\r\n\r\nthe', 'reason', 'that', 'this', 'actually', 'works', 'is', 'that,', 'when', 'you', 'form', 'pairs', 'like', 'that,', 'you', 'are', 'effectively', 'forming', '"squares"', 'or', 'numbers', 'multiplied', 'by', 'themselves.', 'furthermore,', 'you', 'are', 'forming', 'perfect', 'squares,', 'because', 'you', 'are', 'forming', 'pairs', 'where', 'whole', 'numbers', 'are', 'multiplied', 'by', 'themselves.', 'when', 'you', 'take', 'the', 'square', 'root', 'of', 'it', 'as', 'you', 'have', 'to', 'do', 'in', 'radicals,', 'you', 'are', 'just', '"unsquaring"', 'it,', 'and', 'doing', 'this', 'will', 'always', 'guarantee', 'that', 'the', 'number', 'will', 'be', '"unsquared"', 'perfectly', 'without', 'irrational', 'numbers,', 'even', 'if', 'you', 'multiply', 'them', 'together.\r\n\r\nto', 'better', 'illustrate,', 'this', 'works', 'because', 'when', 'you', 'multiply', 'a', 'whole', 'number', 'by', 'itself,', 'you', 'are', 'effectively', 'forming', 'perfect', 'squares,', 'and', 'anytime', 'that', 'you', 'multiply', 'two', 'perfect', 'squares', 'together,', 'you', 'end', 'up', 'with', 'another', 'perfect', 'square.', "it's", 'actually', 'a', 'law', 'of', 'mathematics.', "i'm", 'not', 'sure', 'if', "it's", 'got', 'a', 'name', 'or', 'not,', 'but', 'seriously,', 'anytime', 'that', 'you', 'multiply', 'two', 'perfect', 'squares', 'together,', 'you', 'get', 'another', 'perfect', 'square,', 'and', 'you', 'can', 'try', 'it', 'out', 'with', 'a', 'calculator.', 'when', 'you', 'take', 'a', 'pair', 'of', '2s,', 'and', 'put', 'them', 'together,', "you've", 'just', 'formed', 'a', 'perfect', 'square,', 'and', 'when', 'you', 'take', 'another', 'pair', 'of', '3s', 'that', "let's", 'say', 'is', 'in', 'the', 'same', 'number,', "you've", 'formed', 'another', 'perfect', 'square.', 'multiply', 'those', 'together,', 'and', 'you', 'get', 'yet', 'another', 'larger', 'perfect', 'square.', 'if', 'you', 'do', 'this', 'to', 'all', 'the', 'pairs', 'that', 'you', 'can', 'find', 'in', 'any', 'number,', 'you', 'will', 'always', 'end', 'up', 'with', 'the', 'largest', 'perfect', 'square', 'factor', 'in', 'any', 'number.', 'i', 'hope', 'this', 'gives', 'you', 'some', 'intuition', 'as', 'to', 'why', 'this', 'works,', 'since', 'i', 'have', 'yet', 'to', 'hear', 'a', 'teacher', 'actually', 'attempt', 'to', 'explain', 'why', 'it', 'works.', 'you', 'can', 'try', 'it', 'out', 'for', 'yourself', 'if', 'it', 'still', 'seems', 'somewhat', 'mystical.', ':)\r\n\r\n\r\nas', 'far', 'as', 'to', 'whether', 'you', 'wish', 'to', 'do', 'the', 'full', 'prime-factorization', 'of', 'the', 'numbers', 'or', 'just', 'to', 'factor', 'them', 'down', 'until', 'you', 'end', 'up', 'with', 'the', 'largest', 'perfect', 'square', 'is', 'just', 'up', 'to', 'you.', 'over', 'time,', "you'll", 'probably', 'more', 'quickly', 'identify', 'the', 'perfect', 'squares,', 'so', 'the', 'shorter', 'method', 'might', 'become', 'more', 'practical', 'over', 'time,', 'but', 'do', 'whatever', 'works', 'for', 'you.', 'i', 'tend', 'to', 'like', 'both', 'methods.', ':)\r\n\r\nlastly,', 'in', 'case', 'you', 'doubt', 'whether', 'the', 'simplified', 'forms', 'are', 'actually', 'still', 'the', 'same', 'thing,', "i'll", 'go', 'ahead', 'and', 'actually', 'prove', 'that', '8', 'sqrt', '(2)', 'is', 'actually', 'the', 'same', 'thing', 'as', 'sqrt', '(128),', 'in', 'case', 'all', 'of', 'this', 'still', 'seems', 'mystical.', 'using', 'a', 'calculator,', 'the', 'square', 'root', 'of', '128,', 'rounded', 'to', 'the', 'nearest', 'thousandth,', 'equals', '11.314,', 'and', '8', 'times', 'the', 'square', 'root', 'of', '2', 'also', 'equals', '11.314', '(rounded', 'to', 'the', 'nearest', 'thousandth.', ')', 'you', 'can', 'try', 'this', 'out', 'for', 'yourself', 'if', 'you', 'doubt', 'this.', '(in', 'fact,', 'i', 'encourage', 'you', 'to', 'do', 'so', 'if', 'you', 'are', 'still', 'confused.', 'just', 'enter', 'a', 'number', 'into', 'the', 'calculator', 'and', 'press', 'the', 'radical', 'symbol', 'to', 'get', 'the', 'calculator', 'to', 'calculate', 'the', 'square', 'root', 'of', 'that', 'number.', ')', 'the', 'other', 'examples', 'are', 'correct', 'too,', 'but', 'i', "won't", 'bother', 'to', 'prove', 'them', 'in', 'this', 'post.', 'this', 'post', 'is', 'already', 'lengthy', 'lol.', '', '\r\n\r\ni', 'really', 'hope', 'this', 'has', 'been', 'helpful.', ':)', 'is', 'any', 'of', 'this', 'still', 'confusing?']
Build vocabulary...
('max_document_length...', 2378)
('x shape', (280302, 2378))
Split train/test set...
Vocabulary Size: 162742
Train/Dev split: 278901/1401
Writing to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481470256

2016-12-11T10:31:02.606397: step 1, loss 2.91293, acc 0.328125
2016-12-11T10:31:06.914209: step 2, loss 1.96237, acc 0.453125
2016-12-11T10:31:11.351277: step 3, loss 1.16753, acc 0.734375
2016-12-11T10:31:15.659226: step 4, loss 1.94284, acc 0.703125
2016-12-11T10:31:19.842387: step 5, loss 1.51298, acc 0.6875
2016-12-11T10:31:24.021718: step 6, loss 2.0505, acc 0.6875
2016-12-11T10:31:28.266297: step 7, loss 1.89064, acc 0.671875
2016-12-11T10:31:32.457365: step 8, loss 3.00467, acc 0.59375
2016-12-11T10:31:37.176826: step 9, loss 1.45433, acc 0.765625
2016-12-11T10:31:41.825206: step 10, loss 1.60152, acc 0.703125
2016-12-11T10:31:46.265457: step 11, loss 1.43036, acc 0.65625
2016-12-11T10:31:51.316856: step 12, loss 1.34817, acc 0.75
2016-12-11T10:31:56.354652: step 13, loss 1.45981, acc 0.75
2016-12-11T10:32:00.949009: step 14, loss 1.42248, acc 0.75
2016-12-11T10:32:05.384200: step 15, loss 1.82301, acc 0.65625
2016-12-11T10:32:09.856820: step 16, loss 0.814218, acc 0.703125
2016-12-11T10:32:14.310225: step 17, loss 1.97118, acc 0.609375
2016-12-11T10:32:18.897037: step 18, loss 1.93489, acc 0.671875
2016-12-11T10:32:23.546374: step 19, loss 1.46766, acc 0.640625
2016-12-11T10:32:27.784369: step 20, loss 1.31686, acc 0.65625
2016-12-11T10:32:32.031718: step 21, loss 1.89712, acc 0.640625
2016-12-11T10:32:36.503505: step 22, loss 1.95057, acc 0.671875
2016-12-11T10:32:40.617355: step 23, loss 1.05051, acc 0.703125
2016-12-11T10:32:44.765449: step 24, loss 1.31136, acc 0.703125
2016-12-11T10:32:49.271207: step 25, loss 1.6519, acc 0.59375
2016-12-11T10:32:53.479514: step 26, loss 1.27442, acc 0.703125
2016-12-11T10:32:57.731492: step 27, loss 1.72329, acc 0.65625
2016-12-11T10:33:02.225505: step 28, loss 1.73823, acc 0.734375
2016-12-11T10:33:06.457890: step 29, loss 1.80975, acc 0.578125
2016-12-11T10:33:10.992284: step 30, loss 1.26644, acc 0.703125
2016-12-11T10:33:15.413105: step 31, loss 1.84141, acc 0.671875
2016-12-11T10:33:19.471618: step 32, loss 1.20829, acc 0.6875
2016-12-11T10:33:23.400242: step 33, loss 1.22499, acc 0.703125
2016-12-11T10:33:27.445513: step 34, loss 0.984084, acc 0.703125
2016-12-11T10:33:31.987414: step 35, loss 1.43274, acc 0.578125
2016-12-11T10:33:36.142944: step 36, loss 1.2667, acc 0.703125
2016-12-11T10:33:40.416722: step 37, loss 1.11546, acc 0.75
2016-12-11T10:33:44.951023: step 38, loss 2.21656, acc 0.53125
2016-12-11T10:33:49.651265: step 39, loss 1.42895, acc 0.609375
2016-12-11T10:33:54.114196: step 40, loss 1.73648, acc 0.546875
2016-12-11T10:33:58.131278: step 41, loss 1.42716, acc 0.625
2016-12-11T10:34:02.221911: step 42, loss 1.87342, acc 0.578125
2016-12-11T10:34:06.492734: step 43, loss 1.68441, acc 0.625
2016-12-11T10:34:10.956193: step 44, loss 1.15052, acc 0.703125
2016-12-11T10:34:15.565700: step 45, loss 2.09841, acc 0.59375
2016-12-11T10:34:19.934226: step 46, loss 1.8526, acc 0.59375
2016-12-11T10:34:24.416527: step 47, loss 1.5701, acc 0.59375
2016-12-11T10:34:28.586137: step 48, loss 1.20818, acc 0.6875
2016-12-11T10:34:32.790542: step 49, loss 1.48805, acc 0.671875
2016-12-11T10:34:36.832726: step 50, loss 1.12017, acc 0.765625
2016-12-11T10:34:40.805041: step 51, loss 1.43563, acc 0.671875
2016-12-11T10:34:44.722056: step 52, loss 2.11733, acc 0.59375
2016-12-11T10:34:48.674518: step 53, loss 1.75675, acc 0.53125
2016-12-11T10:34:52.762522: step 54, loss 1.64498, acc 0.640625
2016-12-11T10:34:56.670710: step 55, loss 1.12465, acc 0.6875
2016-12-11T10:35:00.679880: step 56, loss 1.60021, acc 0.65625
2016-12-11T10:35:04.706164: step 57, loss 1.47967, acc 0.625
2016-12-11T10:35:08.730139: step 58, loss 1.39922, acc 0.640625
2016-12-11T10:35:12.786572: step 59, loss 1.66868, acc 0.578125
2016-12-11T10:35:16.745324: step 60, loss 1.28207, acc 0.671875
2016-12-11T10:35:20.785256: step 61, loss 2.15133, acc 0.484375
2016-12-11T10:35:24.786291: step 62, loss 1.24131, acc 0.59375
2016-12-11T10:35:28.757883: step 63, loss 1.60448, acc 0.5625
2016-12-11T10:35:32.907890: step 64, loss 1.4753, acc 0.609375
2016-12-11T10:35:36.927523: step 65, loss 1.5698, acc 0.515625
2016-12-11T10:35:40.936247: step 66, loss 1.38139, acc 0.59375
2016-12-11T10:35:45.750871: step 67, loss 1.52751, acc 0.546875
2016-12-11T10:35:50.424888: step 68, loss 1.87908, acc 0.578125
2016-12-11T10:35:54.986426: step 69, loss 1.17681, acc 0.6875
2016-12-11T10:35:59.138847: step 70, loss 1.44889, acc 0.625
2016-12-11T10:36:03.102493: step 71, loss 1.73117, acc 0.546875
2016-12-11T10:36:07.047536: step 72, loss 1.34432, acc 0.625
2016-12-11T10:36:11.036676: step 73, loss 1.72476, acc 0.59375
2016-12-11T10:36:15.211470: step 74, loss 1.46812, acc 0.625
2016-12-11T10:36:19.304966: step 75, loss 1.02797, acc 0.765625
2016-12-11T10:36:23.302734: step 76, loss 1.13027, acc 0.703125
2016-12-11T10:36:27.416954: step 77, loss 1.35407, acc 0.703125
2016-12-11T10:36:31.444911: step 78, loss 1.70328, acc 0.671875
2016-12-11T10:36:35.451009: step 79, loss 1.55662, acc 0.640625
2016-12-11T10:36:39.541814: step 80, loss 1.43847, acc 0.625
2016-12-11T10:36:43.545163: step 81, loss 0.982604, acc 0.65625
2016-12-11T10:36:47.715697: step 82, loss 1.57474, acc 0.59375
2016-12-11T10:36:51.696802: step 83, loss 1.04087, acc 0.703125
2016-12-11T10:36:55.568060: step 84, loss 1.46564, acc 0.546875
2016-12-11T10:36:59.681519: step 85, loss 1.32885, acc 0.6875
2016-12-11T10:37:03.879796: step 86, loss 1.00295, acc 0.65625
2016-12-11T10:37:07.912035: step 87, loss 1.01454, acc 0.640625
2016-12-11T10:37:11.812214: step 88, loss 1.42776, acc 0.578125
2016-12-11T10:37:15.746455: step 89, loss 1.25349, acc 0.609375
2016-12-11T10:37:19.833605: step 90, loss 1.20814, acc 0.640625
2016-12-11T10:37:23.964212: step 91, loss 1.11342, acc 0.75
2016-12-11T10:37:28.033284: step 92, loss 1.42389, acc 0.640625
2016-12-11T10:37:32.170343: step 93, loss 1.55576, acc 0.5625
2016-12-11T10:37:36.150347: step 94, loss 1.32257, acc 0.640625
2016-12-11T10:37:40.217260: step 95, loss 1.31045, acc 0.578125
2016-12-11T10:37:44.227860: step 96, loss 1.4416, acc 0.609375
2016-12-11T10:37:48.316864: step 97, loss 1.34861, acc 0.625
2016-12-11T10:37:52.308784: step 98, loss 1.09837, acc 0.671875
2016-12-11T10:37:56.534241: step 99, loss 1.08029, acc 0.609375
2016-12-11T10:38:00.592897: step 100, loss 0.807013, acc 0.796875

Evaluation:
2016-12-11T10:41:29.276493: step 100, loss 0.824501, acc 0.748751

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481470256/checkpoints/model-100

2016-12-11T10:41:37.013277: step 101, loss 1.41805, acc 0.71875
2016-12-11T10:41:41.084002: step 102, loss 2.252, acc 0.5625
2016-12-11T10:41:45.292088: step 103, loss 0.868036, acc 0.75
2016-12-11T10:41:49.450965: step 104, loss 1.35485, acc 0.609375
2016-12-11T10:41:53.503929: step 105, loss 1.50694, acc 0.59375
2016-12-11T10:41:57.641506: step 106, loss 1.57513, acc 0.546875
2016-12-11T10:42:01.565964: step 107, loss 1.25732, acc 0.640625
2016-12-11T10:42:05.572788: step 108, loss 1.31932, acc 0.609375
2016-12-11T10:42:09.549778: step 109, loss 1.01854, acc 0.703125
2016-12-11T10:42:13.535520: step 110, loss 1.56822, acc 0.609375
2016-12-11T10:42:17.622340: step 111, loss 1.58585, acc 0.59375
2016-12-11T10:42:21.524561: step 112, loss 1.25417, acc 0.6875
2016-12-11T10:42:25.535462: step 113, loss 1.24628, acc 0.734375
2016-12-11T10:42:29.455712: step 114, loss 1.18394, acc 0.6875
2016-12-11T10:42:33.419250: step 115, loss 1.38692, acc 0.640625
2016-12-11T10:42:37.356397: step 116, loss 1.3776, acc 0.609375
2016-12-11T10:42:41.227382: step 117, loss 1.20383, acc 0.609375
2016-12-11T10:42:45.131168: step 118, loss 1.2593, acc 0.609375
2016-12-11T10:42:49.117556: step 119, loss 1.35042, acc 0.546875
2016-12-11T10:42:53.124627: step 120, loss 1.68509, acc 0.53125
2016-12-11T10:42:57.079222: step 121, loss 1.45875, acc 0.578125
2016-12-11T10:43:01.244562: step 122, loss 1.23574, acc 0.546875
2016-12-11T10:43:05.508026: step 123, loss 1.13021, acc 0.671875
2016-12-11T10:43:09.864091: step 124, loss 1.35465, acc 0.5625
2016-12-11T10:43:14.275690: step 125, loss 1.23684, acc 0.65625
2016-12-11T10:43:18.592277: step 126, loss 0.986744, acc 0.65625
2016-12-11T10:43:22.745212: step 127, loss 1.38778, acc 0.671875
2016-12-11T10:43:26.670081: step 128, loss 0.974122, acc 0.703125
2016-12-11T10:43:30.747530: step 129, loss 1.19659, acc 0.734375
2016-12-11T10:43:34.746301: step 130, loss 0.834824, acc 0.859375
2016-12-11T10:43:38.756533: step 131, loss 1.41864, acc 0.71875
2016-12-11T10:43:42.860554: step 132, loss 1.3813, acc 0.625
2016-12-11T10:43:46.801934: step 133, loss 1.61845, acc 0.625
2016-12-11T10:43:50.814877: step 134, loss 1.40277, acc 0.65625
2016-12-11T10:43:54.949116: step 135, loss 0.797092, acc 0.734375
2016-12-11T10:43:58.990708: step 136, loss 1.26756, acc 0.578125
2016-12-11T10:44:02.998516: step 137, loss 1.55749, acc 0.59375
2016-12-11T10:44:06.965737: step 138, loss 1.23694, acc 0.625
2016-12-11T10:44:10.922310: step 139, loss 1.24614, acc 0.5625
2016-12-11T10:44:14.902129: step 140, loss 1.03598, acc 0.5625
2016-12-11T10:44:18.850651: step 141, loss 0.918035, acc 0.65625
2016-12-11T10:44:22.851780: step 142, loss 1.04309, acc 0.640625
2016-12-11T10:44:26.921557: step 143, loss 1.30404, acc 0.703125
2016-12-11T10:44:31.058940: step 144, loss 1.43597, acc 0.578125
2016-12-11T10:44:35.063165: step 145, loss 0.855065, acc 0.71875
2016-12-11T10:44:38.960893: step 146, loss 1.49079, acc 0.6875
2016-12-11T10:44:42.895480: step 147, loss 1.43841, acc 0.59375
2016-12-11T10:44:46.836732: step 148, loss 1.39213, acc 0.703125
2016-12-11T10:44:50.816226: step 149, loss 0.947104, acc 0.703125
2016-12-11T10:44:54.735517: step 150, loss 1.11037, acc 0.65625
2016-12-11T10:44:58.858229: step 151, loss 0.903213, acc 0.6875
2016-12-11T10:45:02.955193: step 152, loss 1.09266, acc 0.6875
2016-12-11T10:45:06.902714: step 153, loss 1.16459, acc 0.609375
2016-12-11T10:45:10.895058: step 154, loss 0.888048, acc 0.703125
2016-12-11T10:45:14.841064: step 155, loss 1.55944, acc 0.53125
2016-12-11T10:45:18.878304: step 156, loss 1.22285, acc 0.671875
2016-12-11T10:45:22.794129: step 157, loss 0.944592, acc 0.75
2016-12-11T10:45:26.737874: step 158, loss 1.20691, acc 0.625
2016-12-11T10:45:30.829867: step 159, loss 1.13901, acc 0.59375
2016-12-11T10:45:34.923985: step 160, loss 1.09289, acc 0.703125
2016-12-11T10:45:38.930251: step 161, loss 1.11214, acc 0.734375
2016-12-11T10:45:43.182323: step 162, loss 1.17304, acc 0.6875
2016-12-11T10:45:47.203069: step 163, loss 1.26471, acc 0.671875
2016-12-11T10:45:51.251261: step 164, loss 1.01216, acc 0.625
2016-12-11T10:45:55.121955: step 165, loss 0.790482, acc 0.703125
2016-12-11T10:45:59.071170: step 166, loss 1.34449, acc 0.5625
2016-12-11T10:46:03.134559: step 167, loss 1.0548, acc 0.640625
2016-12-11T10:46:07.224763: step 168, loss 1.17765, acc 0.640625
2016-12-11T10:46:11.176886: step 169, loss 0.934499, acc 0.59375
2016-12-11T10:46:15.140413: step 170, loss 0.840078, acc 0.671875
2016-12-11T10:46:19.118281: step 171, loss 0.970256, acc 0.6875
2016-12-11T10:46:23.068661: step 172, loss 1.07188, acc 0.71875
2016-12-11T10:46:26.991597: step 173, loss 1.03402, acc 0.703125
2016-12-11T10:46:30.986693: step 174, loss 0.673064, acc 0.75
2016-12-11T10:46:35.090701: step 175, loss 1.11191, acc 0.671875
2016-12-11T10:46:39.184381: step 176, loss 1.43301, acc 0.703125
2016-12-11T10:46:43.126733: step 177, loss 1.14319, acc 0.703125
2016-12-11T10:46:47.201178: step 178, loss 1.06288, acc 0.71875
2016-12-11T10:46:51.117097: step 179, loss 1.12402, acc 0.625
2016-12-11T10:46:55.039307: step 180, loss 1.04407, acc 0.609375
2016-12-11T10:46:58.990038: step 181, loss 1.20964, acc 0.46875
2016-12-11T10:47:02.939978: step 182, loss 1.34341, acc 0.546875
2016-12-11T10:47:06.968658: step 183, loss 1.02462, acc 0.59375
2016-12-11T10:47:11.039023: step 184, loss 0.980576, acc 0.65625
2016-12-11T10:47:15.057900: step 185, loss 1.03625, acc 0.65625
2016-12-11T10:47:19.006709: step 186, loss 0.962851, acc 0.609375
2016-12-11T10:47:22.995906: step 187, loss 0.94869, acc 0.6875
2016-12-11T10:47:26.907602: step 188, loss 0.839339, acc 0.671875
2016-12-11T10:47:30.892837: step 189, loss 0.742948, acc 0.703125
2016-12-11T10:47:34.927774: step 190, loss 0.596156, acc 0.78125
2016-12-11T10:47:38.979020: step 191, loss 1.13117, acc 0.65625
2016-12-11T10:47:43.147816: step 192, loss 1.05676, acc 0.703125
2016-12-11T10:47:47.082056: step 193, loss 1.09844, acc 0.59375
2016-12-11T10:47:51.038990: step 194, loss 1.01337, acc 0.625
2016-12-11T10:47:55.030421: step 195, loss 1.27059, acc 0.640625
2016-12-11T10:47:58.955672: step 196, loss 0.815263, acc 0.65625
2016-12-11T10:48:03.431184: step 197, loss 1.30058, acc 0.578125
2016-12-11T10:48:07.768738: step 198, loss 0.899259, acc 0.6875
2016-12-11T10:48:12.105849: step 199, loss 0.937784, acc 0.609375
2016-12-11T10:48:16.365576: step 200, loss 1.00771, acc 0.65625

Evaluation:
2016-12-11T10:50:00.125581: step 200, loss 0.71851, acc 0.748751

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481470256/checkpoints/model-200

2016-12-11T10:50:07.498370: step 201, loss 1.18445, acc 0.671875
2016-12-11T10:50:11.420290: step 202, loss 1.13334, acc 0.625
2016-12-11T10:50:15.301144: step 203, loss 1.12605, acc 0.625
2016-12-11T10:50:19.336022: step 204, loss 0.976552, acc 0.640625
2016-12-11T10:50:23.326187: step 205, loss 0.844605, acc 0.671875
2016-12-11T10:50:27.197474: step 206, loss 0.876676, acc 0.6875
2016-12-11T10:50:31.079704: step 207, loss 0.820301, acc 0.65625
2016-12-11T10:50:35.099874: step 208, loss 1.13535, acc 0.609375
2016-12-11T10:50:39.160855: step 209, loss 1.06428, acc 0.578125
2016-12-11T10:50:43.056212: step 210, loss 0.886421, acc 0.65625
2016-12-11T10:50:46.964509: step 211, loss 1.22009, acc 0.515625
2016-12-11T10:50:50.918016: step 212, loss 1.09188, acc 0.5
2016-12-11T10:50:54.824172: step 213, loss 1.39983, acc 0.46875
2016-12-11T10:50:58.839631: step 214, loss 0.798788, acc 0.703125
2016-12-11T10:51:02.760077: step 215, loss 0.869645, acc 0.640625
2016-12-11T10:51:06.688957: step 216, loss 1.14157, acc 0.5625
2016-12-11T10:51:10.686996: step 217, loss 0.760528, acc 0.734375
2016-12-11T10:51:14.731206: step 218, loss 0.666714, acc 0.84375
2016-12-11T10:51:18.751763: step 219, loss 0.767933, acc 0.765625
2016-12-11T10:51:22.754356: step 220, loss 1.13438, acc 0.78125
2016-12-11T10:51:26.691350: step 221, loss 1.20238, acc 0.609375
2016-12-11T10:51:30.605493: step 222, loss 0.819945, acc 0.71875
2016-12-11T10:51:34.544170: step 223, loss 0.934762, acc 0.6875
2016-12-11T10:51:38.579997: step 224, loss 0.915508, acc 0.6875
2016-12-11T10:51:42.657156: step 225, loss 0.844954, acc 0.671875
2016-12-11T10:51:46.728271: step 226, loss 0.863095, acc 0.640625
2016-12-11T10:51:50.812095: step 227, loss 0.907275, acc 0.609375
2016-12-11T10:51:54.826424: step 228, loss 0.80071, acc 0.65625
2016-12-11T10:51:58.777380: step 229, loss 0.761152, acc 0.65625
2016-12-11T10:52:02.678301: step 230, loss 0.802779, acc 0.75
2016-12-11T10:52:06.682737: step 231, loss 0.911897, acc 0.609375
2016-12-11T10:52:10.609176: step 232, loss 1.28746, acc 0.53125
2016-12-11T10:52:14.649052: step 233, loss 0.968605, acc 0.640625
2016-12-11T10:52:18.777291: step 234, loss 0.94801, acc 0.59375
2016-12-11T10:52:22.634616: step 235, loss 0.688724, acc 0.734375
2016-12-11T10:52:26.624100: step 236, loss 1.0069, acc 0.65625
2016-12-11T10:52:30.560476: step 237, loss 1.0459, acc 0.640625
2016-12-11T10:52:34.567292: step 238, loss 0.701686, acc 0.71875
2016-12-11T10:52:38.572937: step 239, loss 0.969488, acc 0.6875
2016-12-11T10:52:42.479248: step 240, loss 1.23861, acc 0.59375
2016-12-11T10:52:46.508107: step 241, loss 0.706672, acc 0.734375
2016-12-11T10:52:50.593679: step 242, loss 0.649936, acc 0.75
2016-12-11T10:52:54.513834: step 243, loss 0.579412, acc 0.75
2016-12-11T10:52:58.539365: step 244, loss 0.822526, acc 0.703125
2016-12-11T10:53:02.936024: step 245, loss 0.840163, acc 0.65625
2016-12-11T10:53:07.366328: step 246, loss 1.03971, acc 0.671875
2016-12-11T10:53:11.799396: step 247, loss 0.726864, acc 0.65625
2016-12-11T10:53:15.840823: step 248, loss 0.747149, acc 0.640625
2016-12-11T10:53:20.032942: step 249, loss 0.824332, acc 0.609375
2016-12-11T10:53:24.081062: step 250, loss 0.872634, acc 0.59375
2016-12-11T10:53:28.073580: step 251, loss 0.911389, acc 0.65625
2016-12-11T10:53:32.155478: step 252, loss 0.860242, acc 0.6875
2016-12-11T10:53:36.201309: step 253, loss 0.741602, acc 0.75
2016-12-11T10:53:40.302613: step 254, loss 0.642875, acc 0.78125
2016-12-11T10:53:44.263541: step 255, loss 0.773677, acc 0.734375
2016-12-11T10:53:48.297140: step 256, loss 0.695215, acc 0.640625
2016-12-11T10:53:52.458429: step 257, loss 0.738366, acc 0.71875
2016-12-11T10:53:56.450307: step 258, loss 0.808123, acc 0.78125
2016-12-11T10:54:00.356423: step 259, loss 0.878721, acc 0.71875
2016-12-11T10:54:04.316069: step 260, loss 0.901473, acc 0.640625
2016-12-11T10:54:08.318222: step 261, loss 1.00665, acc 0.59375
2016-12-11T10:54:12.285569: step 262, loss 0.979372, acc 0.578125
2016-12-11T10:54:16.274627: step 263, loss 0.766251, acc 0.671875
2016-12-11T10:54:20.293502: step 264, loss 0.97818, acc 0.53125
2016-12-11T10:54:24.961418: step 265, loss 0.868572, acc 0.640625
2016-12-11T10:54:28.864307: step 266, loss 0.747064, acc 0.609375
2016-12-11T10:54:32.791108: step 267, loss 0.823162, acc 0.703125
2016-12-11T10:54:36.812189: step 268, loss 0.859976, acc 0.6875
2016-12-11T10:54:40.781555: step 269, loss 0.963939, acc 0.609375
2016-12-11T10:54:44.799032: step 270, loss 0.739876, acc 0.75
2016-12-11T10:54:48.820793: step 271, loss 1.02831, acc 0.640625
2016-12-11T10:54:52.766633: step 272, loss 1.11046, acc 0.625
2016-12-11T10:54:56.778779: step 273, loss 0.73581, acc 0.703125
2016-12-11T10:55:00.780174: step 274, loss 0.742842, acc 0.6875
2016-12-11T10:55:04.721426: step 275, loss 0.758223, acc 0.625
2016-12-11T10:55:08.659490: step 276, loss 0.696114, acc 0.625
2016-12-11T10:55:12.638014: step 277, loss 0.917029, acc 0.5625
2016-12-11T10:55:16.697875: step 278, loss 0.839863, acc 0.671875
2016-12-11T10:55:20.635525: step 279, loss 0.724436, acc 0.640625
2016-12-11T10:55:24.615299: step 280, loss 0.813658, acc 0.609375
2016-12-11T10:55:28.634096: step 281, loss 0.742663, acc 0.71875
2016-12-11T10:55:32.593743: step 282, loss 0.653646, acc 0.65625
2016-12-11T10:55:36.675783: step 283, loss 0.66214, acc 0.796875
2016-12-11T10:55:40.542807: step 284, loss 0.459048, acc 0.828125
2016-12-11T10:55:44.478811: step 285, loss 0.864364, acc 0.75
2016-12-11T10:55:48.496379: step 286, loss 0.742552, acc 0.75
2016-12-11T10:55:52.384082: step 287, loss 1.02975, acc 0.6875
2016-12-11T10:55:56.349802: step 288, loss 0.742121, acc 0.75
2016-12-11T10:56:00.290350: step 289, loss 0.886026, acc 0.6875
2016-12-11T10:56:04.327986: step 290, loss 0.75772, acc 0.640625
2016-12-11T10:56:08.240800: step 291, loss 0.671918, acc 0.640625
2016-12-11T10:56:12.121042: step 292, loss 0.864448, acc 0.609375
2016-12-11T10:56:16.029861: step 293, loss 0.792966, acc 0.59375
2016-12-11T10:56:19.983510: step 294, loss 0.778598, acc 0.609375
2016-12-11T10:56:23.894950: step 295, loss 1.03875, acc 0.546875
2016-12-11T10:56:27.891001: step 296, loss 0.769545, acc 0.671875
2016-12-11T10:56:31.832078: step 297, loss 0.636467, acc 0.75
2016-12-11T10:56:36.563636: step 298, loss 0.74529, acc 0.71875
2016-12-11T10:56:40.652520: step 299, loss 0.893234, acc 0.65625
2016-12-11T10:56:44.533626: step 300, loss 0.685752, acc 0.71875

Evaluation:
2016-12-11T11:00:01.313321: step 300, loss 0.587356, acc 0.748751

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481470256/checkpoints/model-300

2016-12-11T11:00:09.231157: step 301, loss 0.934292, acc 0.65625
2016-12-11T11:00:13.301615: step 302, loss 0.794268, acc 0.671875
2016-12-11T11:00:17.489704: step 303, loss 0.746579, acc 0.6875
2016-12-11T11:00:21.977951: step 304, loss 0.655498, acc 0.703125
2016-12-11T11:00:26.160993: step 305, loss 0.733421, acc 0.671875
2016-12-11T11:00:30.218629: step 306, loss 0.740153, acc 0.671875
2016-12-11T11:00:34.293128: step 307, loss 0.800784, acc 0.671875
2016-12-11T11:00:38.329900: step 308, loss 0.584024, acc 0.734375
2016-12-11T11:00:42.360594: step 309, loss 0.942413, acc 0.578125
2016-12-11T11:00:46.419653: step 310, loss 0.555272, acc 0.78125
2016-12-11T11:00:50.561460: step 311, loss 0.633153, acc 0.65625
2016-12-11T11:00:54.893749: step 312, loss 0.567653, acc 0.765625
2016-12-11T11:00:58.912147: step 313, loss 0.779145, acc 0.65625
2016-12-11T11:01:02.984917: step 314, loss 0.906332, acc 0.640625
2016-12-11T11:01:07.159579: step 315, loss 0.937281, acc 0.609375
2016-12-11T11:01:11.246212: step 316, loss 0.960534, acc 0.515625
2016-12-11T11:01:15.344548: step 317, loss 0.90803, acc 0.625
2016-12-11T11:01:19.327856: step 318, loss 0.923684, acc 0.609375
2016-12-11T11:01:23.349778: step 319, loss 0.673375, acc 0.625
2016-12-11T11:01:27.727113: step 320, loss 0.870174, acc 0.59375
2016-12-11T11:01:31.898307: step 321, loss 0.713388, acc 0.703125
2016-12-11T11:01:35.934202: step 322, loss 0.648338, acc 0.640625
2016-12-11T11:01:39.995712: step 323, loss 0.802695, acc 0.671875
2016-12-11T11:01:44.061078: step 324, loss 0.951776, acc 0.546875
2016-12-11T11:01:48.158438: step 325, loss 0.623102, acc 0.734375
2016-12-11T11:01:52.154328: step 326, loss 0.636257, acc 0.734375
2016-12-11T11:01:56.183766: step 327, loss 0.799849, acc 0.625
2016-12-11T11:02:00.525366: step 328, loss 0.767346, acc 0.671875
2016-12-11T11:02:05.009882: step 329, loss 0.652976, acc 0.703125
2016-12-11T11:02:09.487009: step 330, loss 0.829483, acc 0.640625
2016-12-11T11:02:13.724317: step 331, loss 0.676644, acc 0.703125
2016-12-11T11:02:18.179142: step 332, loss 0.777493, acc 0.703125
2016-12-11T11:02:22.303565: step 333, loss 0.648686, acc 0.671875
2016-12-11T11:02:26.576926: step 334, loss 0.591133, acc 0.71875
2016-12-11T11:02:31.307985: step 335, loss 0.838465, acc 0.609375
2016-12-11T11:02:35.842154: step 336, loss 0.626447, acc 0.71875
2016-12-11T11:02:40.058205: step 337, loss 0.623649, acc 0.6875
2016-12-11T11:02:44.083955: step 338, loss 0.805552, acc 0.671875
2016-12-11T11:02:48.071217: step 339, loss 0.784844, acc 0.6875
2016-12-11T11:02:52.076006: step 340, loss 0.565797, acc 0.703125
2016-12-11T11:02:56.206323: step 341, loss 0.589311, acc 0.703125
2016-12-11T11:03:00.350952: step 342, loss 0.596746, acc 0.734375
2016-12-11T11:03:05.853920: step 343, loss 0.640035, acc 0.71875
2016-12-11T11:03:10.560228: step 344, loss 0.896742, acc 0.546875
2016-12-11T11:03:14.969912: step 345, loss 0.726598, acc 0.671875
2016-12-11T11:03:18.895851: step 346, loss 0.743839, acc 0.65625
2016-12-11T11:03:22.938430: step 347, loss 0.795989, acc 0.71875
2016-12-11T11:03:26.860168: step 348, loss 0.688167, acc 0.6875
2016-12-11T11:03:30.947256: step 349, loss 0.712106, acc 0.65625
2016-12-11T11:03:34.966373: step 350, loss 0.67403, acc 0.6875
2016-12-11T11:03:39.352823: step 351, loss 0.861547, acc 0.625
2016-12-11T11:03:43.442555: step 352, loss 0.640311, acc 0.71875
2016-12-11T11:03:47.442091: step 353, loss 0.612694, acc 0.75
2016-12-11T11:03:51.489493: step 354, loss 0.655196, acc 0.6875
2016-12-11T11:03:55.509714: step 355, loss 0.657475, acc 0.6875
2016-12-11T11:03:59.583290: step 356, loss 0.756925, acc 0.75
2016-12-11T11:04:03.637296: step 357, loss 0.73919, acc 0.6875
2016-12-11T11:04:07.609201: step 358, loss 0.823615, acc 0.6875
2016-12-11T11:04:11.864771: step 359, loss 0.786175, acc 0.609375
2016-12-11T11:04:15.918650: step 360, loss 0.595103, acc 0.765625
2016-12-11T11:04:19.924841: step 361, loss 0.778492, acc 0.546875
2016-12-11T11:04:24.067995: step 362, loss 0.671283, acc 0.640625
2016-12-11T11:04:28.091274: step 363, loss 0.56983, acc 0.71875
2016-12-11T11:04:32.164458: step 364, loss 0.663494, acc 0.71875
2016-12-11T11:04:36.121299: step 365, loss 0.688139, acc 0.71875
2016-12-11T11:04:40.249026: step 366, loss 0.609672, acc 0.671875
2016-12-11T11:04:44.501898: step 367, loss 0.887297, acc 0.65625
2016-12-11T11:04:48.687403: step 368, loss 0.72816, acc 0.75
2016-12-11T11:04:52.718435: step 369, loss 0.483292, acc 0.8125
2016-12-11T11:04:56.765741: step 370, loss 0.83769, acc 0.640625
2016-12-11T11:05:00.855546: step 371, loss 0.580051, acc 0.78125
2016-12-11T11:05:04.948194: step 372, loss 0.553574, acc 0.6875
2016-12-11T11:05:08.979742: step 373, loss 0.60075, acc 0.75
2016-12-11T11:05:13.033957: step 374, loss 0.677109, acc 0.65625
2016-12-11T11:05:17.162404: step 375, loss 0.622801, acc 0.6875
2016-12-11T11:05:21.241129: step 376, loss 0.67596, acc 0.640625
2016-12-11T11:05:25.248749: step 377, loss 0.521852, acc 0.734375
2016-12-11T11:05:29.285993: step 378, loss 0.727921, acc 0.71875
2016-12-11T11:05:33.312936: step 379, loss 0.515328, acc 0.78125
2016-12-11T11:05:37.295240: step 380, loss 0.775088, acc 0.6875
2016-12-11T11:05:41.356368: step 381, loss 0.612942, acc 0.734375
2016-12-11T11:05:45.296441: step 382, loss 0.713126, acc 0.734375
2016-12-11T11:05:49.466221: step 383, loss 0.624565, acc 0.765625
2016-12-11T11:05:53.540212: step 384, loss 0.636313, acc 0.71875
2016-12-11T11:05:57.461168: step 385, loss 0.534272, acc 0.71875
2016-12-11T11:06:01.529199: step 386, loss 0.760898, acc 0.609375
2016-12-11T11:06:05.470485: step 387, loss 0.764813, acc 0.53125
2016-12-11T11:06:09.499486: step 388, loss 0.726557, acc 0.640625
2016-12-11T11:06:13.502610: step 389, loss 0.81271, acc 0.515625
2016-12-11T11:06:17.534213: step 390, loss 0.628608, acc 0.703125
2016-12-11T11:06:21.745090: step 391, loss 0.630212, acc 0.703125
2016-12-11T11:06:25.793698: step 392, loss 0.791177, acc 0.578125
2016-12-11T11:06:29.803071: step 393, loss 0.642645, acc 0.71875
2016-12-11T11:06:33.822674: step 394, loss 0.579405, acc 0.671875
2016-12-11T11:06:37.862937: step 395, loss 0.68695, acc 0.71875
2016-12-11T11:06:41.832891: step 396, loss 0.739135, acc 0.6875
2016-12-11T11:06:45.845412: step 397, loss 0.72267, acc 0.65625
2016-12-11T11:06:49.907080: step 398, loss 0.53667, acc 0.796875
2016-12-11T11:06:54.070481: step 399, loss 0.591135, acc 0.75
2016-12-11T11:06:58.037701: step 400, loss 0.713392, acc 0.671875

Evaluation:
2016-12-11T11:09:43.378199: step 400, loss 0.549966, acc 0.748751

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481470256/checkpoints/model-400

2016-12-11T11:09:50.971816: step 401, loss 0.668426, acc 0.671875
2016-12-11T11:09:55.056530: step 402, loss 0.593963, acc 0.71875
2016-12-11T11:09:58.975955: step 403, loss 0.740138, acc 0.671875
2016-12-11T11:10:03.036701: step 404, loss 0.701202, acc 0.703125
2016-12-11T11:10:07.695888: step 405, loss 0.554104, acc 0.796875
2016-12-11T11:10:12.108687: step 406, loss 0.60995, acc 0.734375
2016-12-11T11:10:16.311212: step 407, loss 0.66887, acc 0.671875
2016-12-11T11:10:20.303404: step 408, loss 0.595886, acc 0.75
2016-12-11T11:10:24.448069: step 409, loss 0.583537, acc 0.71875
2016-12-11T11:10:28.448289: step 410, loss 0.696438, acc 0.703125
2016-12-11T11:10:32.936281: step 411, loss 0.472742, acc 0.828125
2016-12-11T11:10:37.535417: step 412, loss 0.633917, acc 0.75
2016-12-11T11:10:42.604116: step 413, loss 0.516909, acc 0.765625
2016-12-11T11:10:47.050010: step 414, loss 0.764794, acc 0.609375
2016-12-11T11:10:51.842284: step 415, loss 0.457985, acc 0.796875
2016-12-11T11:10:56.200007: step 416, loss 0.604915, acc 0.71875
2016-12-11T11:11:00.501840: step 417, loss 0.50093, acc 0.796875
2016-12-11T11:11:05.702462: step 418, loss 0.456888, acc 0.8125
2016-12-11T11:11:09.691798: step 419, loss 0.613453, acc 0.78125
2016-12-11T11:11:13.797375: step 420, loss 0.717368, acc 0.6875
2016-12-11T11:11:18.147713: step 421, loss 0.814444, acc 0.609375
2016-12-11T11:11:22.828946: step 422, loss 0.597636, acc 0.71875
2016-12-11T11:11:26.919821: step 423, loss 0.638939, acc 0.65625
2016-12-11T11:11:30.902768: step 424, loss 0.697683, acc 0.59375
2016-12-11T11:11:34.966497: step 425, loss 0.775998, acc 0.546875
2016-12-11T11:11:39.195702: step 426, loss 0.636922, acc 0.609375
2016-12-11T11:11:43.159897: step 427, loss 0.645178, acc 0.703125
2016-12-11T11:11:47.119811: step 428, loss 0.739486, acc 0.65625
2016-12-11T11:11:51.162220: step 429, loss 0.609354, acc 0.78125
2016-12-11T11:11:55.140691: step 430, loss 0.63468, acc 0.6875
2016-12-11T11:11:59.181789: step 431, loss 0.60393, acc 0.6875
2016-12-11T11:12:03.189300: step 432, loss 0.548832, acc 0.75
2016-12-11T11:12:07.228995: step 433, loss 0.581028, acc 0.734375
2016-12-11T11:12:11.414229: step 434, loss 0.640615, acc 0.6875
2016-12-11T11:12:15.471728: step 435, loss 0.646721, acc 0.71875
2016-12-11T11:12:19.558476: step 436, loss 0.545782, acc 0.78125
2016-12-11T11:12:23.554820: step 437, loss 0.640276, acc 0.734375
2016-12-11T11:12:27.575644: step 438, loss 0.578414, acc 0.75
2016-12-11T11:12:31.594668: step 439, loss 0.809129, acc 0.59375
2016-12-11T11:12:35.576776: step 440, loss 0.605117, acc 0.671875
2016-12-11T11:12:39.665241: step 441, loss 0.676443, acc 0.640625
2016-12-11T11:12:43.783115: step 442, loss 0.559898, acc 0.703125
2016-12-11T11:12:47.857277: step 443, loss 0.663246, acc 0.703125
2016-12-11T11:12:51.906487: step 444, loss 0.604743, acc 0.65625
2016-12-11T11:12:55.872329: step 445, loss 0.722525, acc 0.625
2016-12-11T11:12:59.915161: step 446, loss 0.650154, acc 0.671875
2016-12-11T11:13:04.559366: step 447, loss 0.757978, acc 0.640625
2016-12-11T11:13:09.059758: step 448, loss 0.687487, acc 0.65625
2016-12-11T11:13:13.460507: step 449, loss 0.733754, acc 0.6875
2016-12-11T11:13:17.640589: step 450, loss 0.643812, acc 0.640625
2016-12-11T11:13:21.633477: step 451, loss 0.700261, acc 0.59375
2016-12-11T11:13:25.595086: step 452, loss 0.703192, acc 0.546875
2016-12-11T11:13:29.664615: step 453, loss 0.700855, acc 0.578125
2016-12-11T11:13:33.651074: step 454, loss 0.609582, acc 0.6875
2016-12-11T11:13:37.814671: step 455, loss 0.547801, acc 0.703125
2016-12-11T11:13:41.787832: step 456, loss 0.639322, acc 0.640625
2016-12-11T11:13:45.772183: step 457, loss 0.540289, acc 0.765625
2016-12-11T11:13:50.037208: step 458, loss 0.614271, acc 0.75
2016-12-11T11:13:54.032916: step 459, loss 0.614193, acc 0.734375
2016-12-11T11:13:57.987433: step 460, loss 0.596846, acc 0.734375
2016-12-11T11:14:01.937199: step 461, loss 0.580916, acc 0.71875
2016-12-11T11:14:05.977775: step 462, loss 0.724797, acc 0.703125
2016-12-11T11:14:09.920704: step 463, loss 0.560403, acc 0.734375
2016-12-11T11:14:13.881261: step 464, loss 0.537812, acc 0.75
2016-12-11T11:14:17.966028: step 465, loss 0.583587, acc 0.703125
2016-12-11T11:14:22.150218: step 466, loss 0.600187, acc 0.703125
2016-12-11T11:14:26.190604: step 467, loss 0.58819, acc 0.71875
2016-12-11T11:14:30.169296: step 468, loss 0.716413, acc 0.609375
2016-12-11T11:14:34.177333: step 469, loss 0.603759, acc 0.71875
2016-12-11T11:14:38.176862: step 470, loss 0.544279, acc 0.78125
2016-12-11T11:14:42.160101: step 471, loss 0.629746, acc 0.703125
2016-12-11T11:14:46.211624: step 472, loss 0.561053, acc 0.703125
2016-12-11T11:14:50.238165: step 473, loss 0.553439, acc 0.75
2016-12-11T11:14:54.431956: step 474, loss 0.642449, acc 0.640625
2016-12-11T11:14:58.482398: step 475, loss 0.672096, acc 0.703125
2016-12-11T11:15:02.413874: step 476, loss 0.674033, acc 0.671875
2016-12-11T11:15:06.429507: step 477, loss 0.681583, acc 0.65625
2016-12-11T11:15:10.414911: step 478, loss 0.574557, acc 0.71875
2016-12-11T11:15:14.456797: step 479, loss 0.663253, acc 0.625
2016-12-11T11:15:18.465106: step 480, loss 0.667742, acc 0.703125
2016-12-11T11:15:22.427883: step 481, loss 0.590386, acc 0.640625
2016-12-11T11:15:26.718988: step 482, loss 0.662224, acc 0.640625
2016-12-11T11:15:30.797330: step 483, loss 0.468337, acc 0.78125
2016-12-11T11:15:34.823540: step 484, loss 0.494043, acc 0.8125
2016-12-11T11:15:38.861325: step 485, loss 0.50595, acc 0.765625
2016-12-11T11:15:43.114706: step 486, loss 0.705241, acc 0.71875
2016-12-11T11:15:47.335396: step 487, loss 0.604658, acc 0.796875
2016-12-11T11:15:51.417980: step 488, loss 0.514634, acc 0.8125
2016-12-11T11:15:55.509245: step 489, loss 0.747689, acc 0.671875
2016-12-11T11:15:59.682660: step 490, loss 0.764492, acc 0.625
2016-12-11T11:16:03.681162: step 491, loss 0.595331, acc 0.734375
2016-12-11T11:16:07.771565: step 492, loss 0.55101, acc 0.75
2016-12-11T11:16:11.674660: step 493, loss 0.663313, acc 0.65625
2016-12-11T11:16:15.701508: step 494, loss 0.719748, acc 0.5625
2016-12-11T11:16:19.703811: step 495, loss 0.593478, acc 0.71875
2016-12-11T11:16:23.697874: step 496, loss 0.704768, acc 0.671875
2016-12-11T11:16:27.700849: step 497, loss 0.480271, acc 0.796875
2016-12-11T11:16:31.821258: step 498, loss 0.622488, acc 0.796875
2016-12-11T11:16:35.871296: step 499, loss 0.712512, acc 0.703125
2016-12-11T11:16:39.813805: step 500, loss 0.952491, acc 0.625

Evaluation:
2016-12-11T11:18:13.489919: step 500, loss 0.5834, acc 0.748751

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481470256/checkpoints/model-500

2016-12-11T11:18:20.722910: step 501, loss 0.616024, acc 0.78125
2016-12-11T11:18:24.662738: step 502, loss 0.689626, acc 0.71875
2016-12-11T11:18:28.623094: step 503, loss 0.593079, acc 0.765625
2016-12-11T11:18:32.658268: step 504, loss 0.7093, acc 0.5625
2016-12-11T11:18:36.678794: step 505, loss 0.664374, acc 0.609375
2016-12-11T11:18:40.573124: step 506, loss 0.751318, acc 0.5625
2016-12-11T11:18:44.583989: step 507, loss 0.620473, acc 0.625
2016-12-11T11:18:48.587415: step 508, loss 0.553146, acc 0.6875
2016-12-11T11:18:52.790767: step 509, loss 0.595963, acc 0.71875
2016-12-11T11:18:56.736104: step 510, loss 0.559909, acc 0.765625
2016-12-11T11:19:00.649275: step 511, loss 0.608374, acc 0.765625
2016-12-11T11:19:04.591458: step 512, loss 0.646334, acc 0.734375
2016-12-11T11:19:08.592013: step 513, loss 0.562327, acc 0.765625
2016-12-11T11:19:12.504385: step 514, loss 0.683401, acc 0.71875
2016-12-11T11:19:16.536566: step 515, loss 0.748152, acc 0.609375
2016-12-11T11:19:20.594838: step 516, loss 0.634392, acc 0.609375
2016-12-11T11:19:24.805771: step 517, loss 0.688007, acc 0.625
2016-12-11T11:19:28.813492: step 518, loss 0.682005, acc 0.640625
2016-12-11T11:19:32.801593: step 519, loss 0.609309, acc 0.671875
2016-12-11T11:19:36.831769: step 520, loss 0.621284, acc 0.6875
2016-12-11T11:19:40.824291: step 521, loss 0.591156, acc 0.71875
2016-12-11T11:19:44.815798: step 522, loss 0.563909, acc 0.75
2016-12-11T11:19:48.806892: step 523, loss 0.519712, acc 0.78125
2016-12-11T11:19:52.824103: step 524, loss 0.590672, acc 0.734375
2016-12-11T11:19:56.971963: step 525, loss 0.737745, acc 0.71875
2016-12-11T11:20:01.088445: step 526, loss 0.664002, acc 0.734375
2016-12-11T11:20:05.041830: step 527, loss 0.587462, acc 0.734375
2016-12-11T11:20:09.022650: step 528, loss 0.486887, acc 0.78125
2016-12-11T11:20:12.942259: step 529, loss 0.605405, acc 0.671875
2016-12-11T11:20:16.927327: step 530, loss 0.708338, acc 0.65625
2016-12-11T11:20:20.970407: step 531, loss 0.735721, acc 0.5625
2016-12-11T11:20:24.972655: step 532, loss 0.69735, acc 0.609375
2016-12-11T11:20:29.124544: step 533, loss 0.527418, acc 0.78125
2016-12-11T11:20:33.219653: step 534, loss 0.609463, acc 0.6875
2016-12-11T11:20:37.203839: step 535, loss 0.607445, acc 0.734375
2016-12-11T11:20:41.240081: step 536, loss 0.65254, acc 0.671875
2016-12-11T11:20:45.457953: step 537, loss 0.483553, acc 0.796875
2016-12-11T11:20:49.564399: step 538, loss 0.501291, acc 0.828125
2016-12-11T11:20:53.557969: step 539, loss 0.479789, acc 0.828125
2016-12-11T11:20:57.595005: step 540, loss 0.559474, acc 0.75
2016-12-11T11:21:01.764384: step 541, loss 0.573567, acc 0.75
2016-12-11T11:21:05.819164: step 542, loss 0.634088, acc 0.671875
2016-12-11T11:21:09.789026: step 543, loss 0.580888, acc 0.765625
2016-12-11T11:21:13.795064: step 544, loss 0.629036, acc 0.6875
2016-12-11T11:21:17.839688: step 545, loss 0.59745, acc 0.71875
2016-12-11T11:21:21.794933: step 546, loss 0.721993, acc 0.5625
2016-12-11T11:21:25.813535: step 547, loss 0.564625, acc 0.75
2016-12-11T11:21:29.836927: step 548, loss 0.604182, acc 0.71875
2016-12-11T11:21:33.984994: step 549, loss 0.588711, acc 0.75
2016-12-11T11:21:38.061361: step 550, loss 0.561188, acc 0.734375
2016-12-11T11:21:42.065336: step 551, loss 0.509657, acc 0.765625
2016-12-11T11:21:46.061096: step 552, loss 0.570443, acc 0.78125
2016-12-11T11:21:50.116146: step 553, loss 0.506067, acc 0.796875
2016-12-11T11:21:54.086432: step 554, loss 0.511973, acc 0.765625
2016-12-11T11:21:58.157757: step 555, loss 0.68599, acc 0.734375
2016-12-11T11:22:02.147138: step 556, loss 0.570692, acc 0.765625
2016-12-11T11:22:06.396739: step 557, loss 0.528933, acc 0.765625
2016-12-11T11:22:10.445361: step 558, loss 0.622212, acc 0.765625
2016-12-11T11:22:14.383347: step 559, loss 0.574393, acc 0.6875
2016-12-11T11:22:18.426217: step 560, loss 0.641439, acc 0.6875
2016-12-11T11:22:22.449745: step 561, loss 0.630592, acc 0.734375
2016-12-11T11:22:26.523975: step 562, loss 0.589584, acc 0.65625
2016-12-11T11:22:30.449430: step 563, loss 0.679474, acc 0.6875
2016-12-11T11:22:34.460315: step 564, loss 0.547185, acc 0.765625
2016-12-11T11:22:38.639632: step 565, loss 0.719814, acc 0.671875
2016-12-11T11:22:42.679951: step 566, loss 0.645155, acc 0.671875
2016-12-11T11:22:46.770514: step 567, loss 0.589625, acc 0.6875
2016-12-11T11:22:50.869916: step 568, loss 0.529216, acc 0.765625
2016-12-11T11:22:54.929127: step 569, loss 0.612936, acc 0.65625
2016-12-11T11:22:59.058843: step 570, loss 0.612935, acc 0.640625
2016-12-11T11:23:03.672702: step 571, loss 0.545945, acc 0.78125
2016-12-11T11:23:07.934706: step 572, loss 0.585502, acc 0.734375
2016-12-11T11:23:12.704137: step 573, loss 0.597957, acc 0.71875
2016-12-11T11:23:16.820109: step 574, loss 0.517398, acc 0.8125
2016-12-11T11:23:20.836627: step 575, loss 0.695537, acc 0.71875
2016-12-11T11:23:24.825863: step 576, loss 0.556323, acc 0.78125
2016-12-11T11:23:28.816685: step 577, loss 0.808778, acc 0.625
2016-12-11T11:23:32.803018: step 578, loss 0.697704, acc 0.671875
2016-12-11T11:23:36.821254: step 579, loss 0.573324, acc 0.71875
2016-12-11T11:23:40.767234: step 580, loss 0.575118, acc 0.734375
2016-12-11T11:23:45.082718: step 581, loss 0.658091, acc 0.609375
2016-12-11T11:23:49.096033: step 582, loss 0.601419, acc 0.6875
2016-12-11T11:23:53.067372: step 583, loss 0.563837, acc 0.703125
2016-12-11T11:23:57.092157: step 584, loss 0.6055, acc 0.71875
2016-12-11T11:24:01.139569: step 585, loss 0.687288, acc 0.625
2016-12-11T11:24:05.130425: step 586, loss 0.4601, acc 0.875
2016-12-11T11:24:09.170423: step 587, loss 0.581523, acc 0.71875
2016-12-11T11:24:13.135871: step 588, loss 0.592136, acc 0.703125
2016-12-11T11:24:17.367346: step 589, loss 0.646123, acc 0.71875
2016-12-11T11:24:21.365583: step 590, loss 0.570461, acc 0.6875
2016-12-11T11:24:25.337310: step 591, loss 0.727921, acc 0.640625
2016-12-11T11:24:29.332214: step 592, loss 0.781054, acc 0.625
2016-12-11T11:24:33.339450: step 593, loss 0.690578, acc 0.609375
2016-12-11T11:24:37.372432: step 594, loss 0.477763, acc 0.796875
2016-12-11T11:24:41.400651: step 595, loss 0.568931, acc 0.71875
2016-12-11T11:24:45.426861: step 596, loss 0.521683, acc 0.765625
2016-12-11T11:24:49.689072: step 597, loss 0.664762, acc 0.71875
2016-12-11T11:24:53.667737: step 598, loss 0.734291, acc 0.6875
2016-12-11T11:24:57.713414: step 599, loss 0.383897, acc 0.875
2016-12-11T11:25:01.667742: step 600, loss 0.650199, acc 0.6875

Evaluation:
2016-12-11T11:26:21.736541: step 600, loss 0.553436, acc 0.748751

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481470256/checkpoints/model-600

2016-12-11T11:26:29.041964: step 601, loss 0.601203, acc 0.78125
2016-12-11T11:26:32.997503: step 602, loss 0.58204, acc 0.6875
2016-12-11T11:26:37.001346: step 603, loss 0.604553, acc 0.65625
2016-12-11T11:26:41.027154: step 604, loss 0.610694, acc 0.671875
2016-12-11T11:26:45.062884: step 605, loss 0.62367, acc 0.671875
2016-12-11T11:26:49.051588: step 606, loss 0.580994, acc 0.703125
2016-12-11T11:26:53.042352: step 607, loss 0.566547, acc 0.734375
2016-12-11T11:26:56.980283: step 608, loss 0.578654, acc 0.703125
2016-12-11T11:27:00.935781: step 609, loss 0.638228, acc 0.671875
2016-12-11T11:27:04.927844: step 610, loss 0.524345, acc 0.765625
2016-12-11T11:27:08.849164: step 611, loss 0.642083, acc 0.703125
2016-12-11T11:27:12.984646: step 612, loss 0.687447, acc 0.65625
2016-12-11T11:27:17.095228: step 613, loss 0.653426, acc 0.671875
2016-12-11T11:27:21.011146: step 614, loss 0.583942, acc 0.640625
2016-12-11T11:27:25.067144: step 615, loss 0.584307, acc 0.75
2016-12-11T11:27:29.022397: step 616, loss 0.534783, acc 0.765625
2016-12-11T11:27:32.997998: step 617, loss 0.622985, acc 0.71875
2016-12-11T11:27:36.974997: step 618, loss 0.568415, acc 0.734375
2016-12-11T11:27:40.963757: step 619, loss 0.596552, acc 0.734375
2016-12-11T11:27:45.226197: step 620, loss 0.65303, acc 0.703125
2016-12-11T11:27:49.211360: step 621, loss 0.705796, acc 0.671875
2016-12-11T11:27:53.183309: step 622, loss 0.625808, acc 0.71875
2016-12-11T11:27:57.216151: step 623, loss 0.545458, acc 0.75
2016-12-11T11:28:01.330023: step 624, loss 0.615431, acc 0.640625
2016-12-11T11:28:05.746857: step 625, loss 0.589352, acc 0.6875
2016-12-11T11:28:10.238578: step 626, loss 0.627901, acc 0.6875
2016-12-11T11:28:14.395533: step 627, loss 0.645722, acc 0.65625
2016-12-11T11:28:18.514098: step 628, loss 0.532813, acc 0.734375
2016-12-11T11:28:22.406902: step 629, loss 0.44828, acc 0.84375
2016-12-11T11:28:26.380588: step 630, loss 0.514458, acc 0.765625
2016-12-11T11:28:30.357043: step 631, loss 0.567001, acc 0.765625
2016-12-11T11:28:34.471356: step 632, loss 0.560437, acc 0.765625
2016-12-11T11:28:38.476628: step 633, loss 0.623293, acc 0.765625
2016-12-11T11:28:42.383674: step 634, loss 0.522073, acc 0.75
2016-12-11T11:28:46.380091: step 635, loss 0.494699, acc 0.8125
2016-12-11T11:28:50.499442: step 636, loss 0.52739, acc 0.75
2016-12-11T11:28:54.433143: step 637, loss 0.573924, acc 0.71875
2016-12-11T11:28:58.368280: step 638, loss 0.564015, acc 0.71875
2016-12-11T11:29:02.421970: step 639, loss 0.560313, acc 0.6875
2016-12-11T11:29:06.362787: step 640, loss 0.553845, acc 0.78125
2016-12-11T11:29:10.365912: step 641, loss 0.574838, acc 0.734375
2016-12-11T11:29:14.409897: step 642, loss 0.609863, acc 0.6875
2016-12-11T11:29:18.426805: step 643, loss 0.619776, acc 0.71875
2016-12-11T11:29:22.521984: step 644, loss 0.491778, acc 0.78125
2016-12-11T11:29:26.473483: step 645, loss 0.61963, acc 0.75
2016-12-11T11:29:30.448273: step 646, loss 0.677333, acc 0.6875
2016-12-11T11:29:34.466271: step 647, loss 0.661292, acc 0.65625
2016-12-11T11:29:38.528441: step 648, loss 0.494845, acc 0.75
2016-12-11T11:29:42.464740: step 649, loss 0.5719, acc 0.75
2016-12-11T11:29:46.466135: step 650, loss 0.601416, acc 0.71875
2016-12-11T11:29:50.515573: step 651, loss 0.557906, acc 0.75
2016-12-11T11:29:54.681801: step 652, loss 0.662546, acc 0.671875
2016-12-11T11:29:58.704188: step 653, loss 0.487712, acc 0.75
2016-12-11T11:30:02.616188: step 654, loss 0.595873, acc 0.703125
2016-12-11T11:30:06.572424: step 655, loss 0.763651, acc 0.625
2016-12-11T11:30:10.661407: step 656, loss 0.676762, acc 0.625
2016-12-11T11:30:14.496931: step 657, loss 0.642037, acc 0.671875
2016-12-11T11:30:18.504970: step 658, loss 0.588539, acc 0.703125
2016-12-11T11:30:22.556741: step 659, loss 0.708166, acc 0.671875
2016-12-11T11:30:26.649154: step 660, loss 0.617445, acc 0.65625
2016-12-11T11:30:30.668508: step 661, loss 0.694633, acc 0.515625
2016-12-11T11:30:34.650049: step 662, loss 0.624077, acc 0.6875
2016-12-11T11:30:38.628832: step 663, loss 0.585184, acc 0.75
2016-12-11T11:30:42.632255: step 664, loss 0.538214, acc 0.765625
2016-12-11T11:30:46.607001: step 665, loss 0.460419, acc 0.828125
2016-12-11T11:30:50.665281: step 666, loss 0.834623, acc 0.578125
2016-12-11T11:30:54.850394: step 667, loss 0.681321, acc 0.703125
2016-12-11T11:30:58.887883: step 668, loss 0.515762, acc 0.8125
2016-12-11T11:31:02.882697: step 669, loss 0.562131, acc 0.734375
2016-12-11T11:31:06.831547: step 670, loss 0.601754, acc 0.703125
2016-12-11T11:31:10.794440: step 671, loss 0.612634, acc 0.71875
2016-12-11T11:31:14.721633: step 672, loss 0.610188, acc 0.6875
2016-12-11T11:31:18.677086: step 673, loss 0.662876, acc 0.640625
2016-12-11T11:31:22.658623: step 674, loss 0.67824, acc 0.640625
2016-12-11T11:31:26.634651: step 675, loss 0.53002, acc 0.765625
2016-12-11T11:31:30.746177: step 676, loss 0.611088, acc 0.6875
2016-12-11T11:31:34.739182: step 677, loss 0.609469, acc 0.734375
2016-12-11T11:31:38.678603: step 678, loss 0.594105, acc 0.734375
2016-12-11T11:31:42.720284: step 679, loss 0.499074, acc 0.78125
2016-12-11T11:31:46.636514: step 680, loss 0.511954, acc 0.78125
2016-12-11T11:31:50.609913: step 681, loss 0.588047, acc 0.703125
2016-12-11T11:31:54.613727: step 682, loss 0.678751, acc 0.65625
2016-12-11T11:31:58.566663: step 683, loss 0.564133, acc 0.703125
2016-12-11T11:32:02.706839: step 684, loss 0.51564, acc 0.75
2016-12-11T11:32:06.713414: step 685, loss 0.603424, acc 0.65625
2016-12-11T11:32:10.619833: step 686, loss 0.626874, acc 0.65625
2016-12-11T11:32:14.539134: step 687, loss 0.579297, acc 0.703125
2016-12-11T11:32:18.555506: step 688, loss 0.52885, acc 0.765625
2016-12-11T11:32:22.542822: step 689, loss 0.615655, acc 0.734375
2016-12-11T11:32:26.492045: step 690, loss 0.569864, acc 0.765625
2016-12-11T11:32:30.489250: step 691, loss 0.589386, acc 0.734375
2016-12-11T11:32:34.603356: step 692, loss 0.560858, acc 0.765625
2016-12-11T11:32:38.552901: step 693, loss 0.660243, acc 0.6875
2016-12-11T11:32:42.601829: step 694, loss 0.648938, acc 0.625
2016-12-11T11:32:46.605011: step 695, loss 0.590088, acc 0.71875
2016-12-11T11:32:50.564690: step 696, loss 0.586011, acc 0.671875
2016-12-11T11:32:54.580337: step 697, loss 0.524374, acc 0.765625
2016-12-11T11:32:58.593020: step 698, loss 0.57302, acc 0.71875
2016-12-11T11:33:03.134152: step 699, loss 0.593496, acc 0.703125
2016-12-11T11:33:07.626610: step 700, loss 0.78949, acc 0.625

Evaluation:
2016-12-11T11:34:19.857493: step 700, loss 0.574259, acc 0.748751

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481470256/checkpoints/model-700

2016-12-11T11:34:26.782902: step 701, loss 0.498509, acc 0.796875
2016-12-11T11:34:30.681075: step 702, loss 0.383098, acc 0.859375
2016-12-11T11:34:34.627937: step 703, loss 0.506016, acc 0.796875
2016-12-11T11:34:38.685623: step 704, loss 0.60331, acc 0.71875
2016-12-11T11:34:42.712155: step 705, loss 0.543209, acc 0.78125
2016-12-11T11:34:46.765778: step 706, loss 0.621722, acc 0.703125
2016-12-11T11:34:50.731528: step 707, loss 0.656933, acc 0.65625
2016-12-11T11:34:54.905834: step 708, loss 0.530462, acc 0.734375
2016-12-11T11:34:58.823647: step 709, loss 0.640908, acc 0.640625
2016-12-11T11:35:02.815001: step 710, loss 0.609932, acc 0.671875
2016-12-11T11:35:06.878341: step 711, loss 0.567612, acc 0.71875
2016-12-11T11:35:10.824754: step 712, loss 0.688668, acc 0.59375
2016-12-11T11:35:14.793448: step 713, loss 0.615189, acc 0.71875
2016-12-11T11:35:18.770366: step 714, loss 0.50389, acc 0.828125
2016-12-11T11:35:22.788693: step 715, loss 0.550037, acc 0.75
2016-12-11T11:35:26.900073: step 716, loss 0.607097, acc 0.734375
2016-12-11T11:35:30.943867: step 717, loss 0.515625, acc 0.78125
2016-12-11T11:35:34.891092: step 718, loss 0.811727, acc 0.59375
2016-12-11T11:35:38.910519: step 719, loss 0.55312, acc 0.71875
2016-12-11T11:35:42.929825: step 720, loss 0.52868, acc 0.734375
2016-12-11T11:35:46.854747: step 721, loss 0.583473, acc 0.703125
2016-12-11T11:35:50.832019: step 722, loss 0.587982, acc 0.609375
2016-12-11T11:35:54.764402: step 723, loss 0.597898, acc 0.640625
2016-12-11T11:35:58.902211: step 724, loss 0.396414, acc 0.90625
2016-12-11T11:36:02.851022: step 725, loss 0.735754, acc 0.625
2016-12-11T11:36:06.817049: step 726, loss 0.443741, acc 0.84375
2016-12-11T11:36:10.774168: step 727, loss 0.403943, acc 0.84375
2016-12-11T11:36:14.703858: step 728, loss 0.429009, acc 0.828125
2016-12-11T11:36:18.700818: step 729, loss 0.485804, acc 0.828125
2016-12-11T11:36:22.651169: step 730, loss 0.574133, acc 0.765625
2016-12-11T11:36:26.568524: step 731, loss 0.639457, acc 0.734375
2016-12-11T11:36:30.742527: step 732, loss 0.666406, acc 0.703125
2016-12-11T11:36:34.747616: step 733, loss 0.481725, acc 0.796875
2016-12-11T11:36:38.695186: step 734, loss 0.550807, acc 0.734375
2016-12-11T11:36:42.682845: step 735, loss 0.63242, acc 0.65625
2016-12-11T11:36:46.629093: step 736, loss 0.562527, acc 0.6875
2016-12-11T11:36:50.624488: step 737, loss 0.606715, acc 0.734375
2016-12-11T11:36:54.610105: step 738, loss 0.601078, acc 0.71875
2016-12-11T11:36:58.581020: step 739, loss 0.514234, acc 0.78125
2016-12-11T11:37:02.730165: step 740, loss 0.371708, acc 0.890625
2016-12-11T11:37:06.753418: step 741, loss 0.68861, acc 0.703125
2016-12-11T11:37:10.717290: step 742, loss 0.507374, acc 0.796875
2016-12-11T11:37:14.662915: step 743, loss 0.709597, acc 0.703125
2016-12-11T11:37:18.643939: step 744, loss 0.622378, acc 0.75
2016-12-11T11:37:22.646884: step 745, loss 0.510824, acc 0.78125
2016-12-11T11:37:26.719278: step 746, loss 0.554064, acc 0.703125
2016-12-11T11:37:30.685134: step 747, loss 0.562368, acc 0.734375
2016-12-11T11:37:34.764049: step 748, loss 0.511303, acc 0.78125
2016-12-11T11:37:38.887969: step 749, loss 0.659645, acc 0.65625
2016-12-11T11:37:42.950153: step 750, loss 0.500136, acc 0.765625
2016-12-11T11:37:47.003221: step 751, loss 0.44332, acc 0.8125
2016-12-11T11:37:51.000105: step 752, loss 0.724418, acc 0.640625
2016-12-11T11:37:54.930716: step 753, loss 0.633933, acc 0.734375
2016-12-11T11:37:58.872500: step 754, loss 0.52852, acc 0.75
2016-12-11T11:38:03.288627: step 755, loss 0.521603, acc 0.734375
2016-12-11T11:38:07.641069: step 756, loss 0.514098, acc 0.765625
2016-12-11T11:38:12.069953: step 757, loss 0.526522, acc 0.78125
2016-12-11T11:38:16.375395: step 758, loss 0.624848, acc 0.71875
2016-12-11T11:38:20.385866: step 759, loss 0.565331, acc 0.75
2016-12-11T11:38:24.348315: step 760, loss 0.490123, acc 0.78125
2016-12-11T11:38:28.342010: step 761, loss 0.539044, acc 0.734375
2016-12-11T11:38:32.313111: step 762, loss 0.668045, acc 0.671875
2016-12-11T11:38:36.265811: step 763, loss 0.708991, acc 0.625
2016-12-11T11:38:40.478271: step 764, loss 0.595205, acc 0.6875
2016-12-11T11:38:44.576862: step 765, loss 0.585957, acc 0.640625
2016-12-11T11:38:48.551753: step 766, loss 0.692672, acc 0.578125
2016-12-11T11:38:52.463384: step 767, loss 0.609766, acc 0.640625
2016-12-11T11:38:56.481245: step 768, loss 0.568913, acc 0.75
2016-12-11T11:39:00.491961: step 769, loss 0.454737, acc 0.84375
2016-12-11T11:39:04.556099: step 770, loss 0.715405, acc 0.640625
2016-12-11T11:39:08.550088: step 771, loss 0.667737, acc 0.71875
2016-12-11T11:39:12.692480: step 772, loss 0.356937, acc 0.890625
2016-12-11T11:39:16.628836: step 773, loss 0.566202, acc 0.75
2016-12-11T11:39:20.573615: step 774, loss 0.545903, acc 0.796875
2016-12-11T11:39:24.531530: step 775, loss 0.781547, acc 0.625
2016-12-11T11:39:28.508347: step 776, loss 0.577845, acc 0.703125
2016-12-11T11:39:32.452451: step 777, loss 0.551635, acc 0.703125
2016-12-11T11:39:36.423227: step 778, loss 0.620851, acc 0.671875
2016-12-11T11:39:40.414124: step 779, loss 0.613723, acc 0.6875
2016-12-11T11:39:44.436876: step 780, loss 0.592054, acc 0.6875
2016-12-11T11:39:48.437365: step 781, loss 0.618238, acc 0.71875
2016-12-11T11:39:52.424038: step 782, loss 0.586573, acc 0.71875
2016-12-11T11:39:56.389373: step 783, loss 0.57273, acc 0.734375
2016-12-11T11:40:00.345779: step 784, loss 0.503475, acc 0.796875
2016-12-11T11:40:04.280420: step 785, loss 0.759653, acc 0.640625
2016-12-11T11:40:08.240915: step 786, loss 0.585112, acc 0.75
2016-12-11T11:40:12.199040: step 787, loss 0.567368, acc 0.6875
2016-12-11T11:40:16.193374: step 788, loss 0.470658, acc 0.8125
2016-12-11T11:40:20.304413: step 789, loss 0.606146, acc 0.65625
2016-12-11T11:40:24.532485: step 790, loss 0.575429, acc 0.71875
2016-12-11T11:40:28.583994: step 791, loss 0.519844, acc 0.78125
2016-12-11T11:40:32.611219: step 792, loss 0.628478, acc 0.671875
2016-12-11T11:40:36.580235: step 793, loss 0.62575, acc 0.6875
2016-12-11T11:40:40.585842: step 794, loss 0.644984, acc 0.71875
2016-12-11T11:40:44.688450: step 795, loss 0.605003, acc 0.71875
2016-12-11T11:40:48.662003: step 796, loss 0.546604, acc 0.75
2016-12-11T11:40:52.755913: step 797, loss 0.658807, acc 0.640625
2016-12-11T11:40:57.125612: step 798, loss 0.657198, acc 0.65625
2016-12-11T11:41:01.783715: step 799, loss 0.617534, acc 0.671875
2016-12-11T11:41:06.344040: step 800, loss 0.573256, acc 0.65625

Evaluation:
2016-12-11T11:43:18.415099: step 800, loss 0.580828, acc 0.735903
