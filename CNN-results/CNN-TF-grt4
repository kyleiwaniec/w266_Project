python train.py --positive_data_file data/pos_data_train --negative_data_file data/neg_data_train --dev_sample_percentage .005

Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=64
CHECKPOINT_EVERY=100
DEV_SAMPLE_PERCENTAGE=0.005
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=128
EVALUATE_EVERY=100
FILTER_SIZES=3,4,5
L2_REG_LAMBDA=0.0
LOG_DEVICE_PLACEMENT=False
NEGATIVE_DATA_FILE=data/neg_data_train
NUM_EPOCHS=200
NUM_FILTERS=128
POSITIVE_DATA_FILE=data/pos_data_train

Loading data...
('l-positive_examples', 29298)
('l-negative_examples', 250996)
('type', 280294, 280294)
['1:', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '2:', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '3:', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '4:', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '5:', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '6:', '\n', '', '', '1x1=1', '', '', '', '', '', '', '', '', '', '', '', '', '', '2x1=2', '', '', '', '', '', '', '', '', '', '', '', '', '3x1=3', '', '', '', '', '', '', '', '', '', '', '', '', '4x1=4', '', '', '', '', '', '', '', '', '', '', '', '', '5x1=5', '', '', '', '', '', '', '', '', '', '', '', '', '6x1=6', '\n', '', '', '1x2=2', '', '', '', '', '', '', '', '', '', '', '', '', '', '2x2=4', '', '', '', '', '', '', '', '', '', '', '', '', '3x2=6', '', '', '', '', '', '', '', '', '', '', '', '', '4x2=8', '', '', '', '', '', '', '', '', '', '', '', '', '5x2=10', '', '', '', '', '', '', '', '', '', '', '6x2=12', '\n', '', '', '1x3=3', '', '', '', '', '', '', '', '', '', '', '', '', '', '2x3=6', '', '', '', '', '', '', '', '', '', '', '', '', '3x3=9', '', '', '', '', '', '', '', '', '', '', '', '', '4x3=12', '', '', '', '', '', '', '', '', '', '', '5x3=15', '', '', '', '', '', '', '', '', '', '', '6x3=18', '\n', '', '', '1x4=4', '', '', '', '', '', '', '', '', '', '', '', '', '', '2x4=8', '', '', '', '', '', '', '', '', '', '', '', '', '3x4=12', '', '', '', '', '', '', '', '', '', '', '4x4=16', '', '', '', '', '', '', '', '', '', '', '5x4=20', '', '', '', '', '', '', '', '', '', '', '6x4=24', '\n', '', '', '1x5=5', '', '', '', '', '', '', '', '', '', '', '', '', '', '2x5=10', '', '', '', '', '', '', '', '', '', '', '3x5=15', '', '', '', '', '', '', '', '', '', '', '4x5=20', '', '', '', '', '', '', '', '', '', '', '5x5=25', '', '', '', '', '', '', '', '', '', '', '6x5=30', '\n', '', '', '1x6=6', '', '', '', '', '', '', '', '', '', '', '', '', '', '2x6=12', '', '', '', '', '', '', '', '', '', '', '3x6=18', '', '', '', '', '', '', '', '', '', '', '4x6=24', '', '', '', '', '', '', '', '', '', '', '5x6=30', '', '', '', '', '', '', '', '', '', '', '6x6=36', '\n', '', '', '1x7=7', '', '', '', '', '', '', '', '', '', '', '', '', '', '2x7=14', '', '', '', '', '', '', '', '', '', '', '3x7=21', '', '', '', '', '', '', '', '', '', '', '4x7=28', '', '', '', '', '', '', '', '', '', '', '5x7=35', '', '', '', '', '', '', '', '', '', '', '6x7=42', '\n', '', '', '1x8=8', '', '', '', '', '', '', '', '', '', '', '', '', '', '2x8=16', '', '', '', '', '', '', '', '', '', '', '3x8=24', '', '', '', '', '', '', '', '', '', '', '4x8=32', '', '', '', '', '', '', '', '', '', '', '5x8=40', '', '', '', '', '', '', '', '', '', '', '6x8=48', '\n', '', '', '1x9=9', '', '', '', '', '', '', '', '', '', '', '', '', '', '2x9=18', '', '', '', '', '', '', '', '', '', '', '3x9=27', '', '', '', '', '', '', '', '', '', '', '4x9=36', '', '', '', '', '', '', '', '', '', '', '5x9=45', '', '', '', '', '', '', '', '', '', '', '6x9=54', '\n', '', '', '1x10=10', '', '', '', '', '', '', '', '', '2x10=20', '', '', '', '', '', '', '', '', '3x10=30', '', '', '', '', '', '', '', '', '4x10=40', '', '', '', '', '', '', '', '', '5x10=50', '', '', '', '', '', '', '', '', '6x10=60', '\n', '', '', '1x11=11', '', '', '', '', '', '', '', '', '2x11=22', '', '', '', '', '', '', '', '', '3x11=33', '', '', '', '', '', '', '', '', '4x11=44', '', '', '', '', '', '', '', '', '5x11=55', '', '', '', '', '', '', '', '', '6x11=66', '\n', '', '', '1x12=12', '', '', '', '', '', '', '', '', '2x12=24', '', '', '', '', '', '', '', '', '3x12=36', '', '', '', '', '', '', '', '', '4x12=48', '', '', '', '', '', '', '', '', '5x12=60', '', '', '', '', '', '', '', '', '6x12=72\n\n7:', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '8:', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '9:', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '10:', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '11:', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '12:', '\n', '', '7x1=7', '', '', '', '', '', '', '', '', '', '', '', '', '8x1=8', '', '', '', '', '', '', '', '', '', '', '', '', '9x1=9', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '10x1=10', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '11x1=11', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '12x1=12', '\n', '', '', '', '7x2=14', '', '', '', '', '', '', '', '', '', '8x2=16', '', '', '', '', '', '', '', '', '', '', '9x2=18', '', '', '', '', '', '', '', '', '', '', '', '', '', '10x2=20', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '11x2=22', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '12x2=24', '\n', '', '', '', '7x3=21', '', '', '', '', '', '', '', '', '', '8x3=24', '', '', '', '', '', '', '', '', '', '', '9x3=27', '', '', '', '', '', '', '', '', '', '', '', '', '', '10x3=30', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '11x3=33', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '12x3=36', '\n', '', '', '', '7x4=28', '', '', '', '', '', '', '', '', '', '8x4=32', '', '', '', '', '', '', '', '', '', '', '9x4=36', '', '', '', '', '', '', '', '', '', '', '', '', '', '10x4=40', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '11x4=44', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '12x4=48', '\n', '', '', '', '7x5=35', '', '', '', '', '', '', '', '', '', '8x5=40', '', '', '', '', '', '', '', '', '', '', '9x5=45', '', '', '', '', '', '', '', '', '', '', '', '', '', '10x5=50', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '11x5=55', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '12x5=60', '\n', '', '', '', '7x6=42', '', '', '', '', '', '', '', '', '', '8x6=48', '', '', '', '', '', '', '', '', '', '', '9x6=54', '', '', '', '', '', '', '', '', '', '', '', '', '', '10x6=60', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '11x6=66', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '12x6=72', '\n', '', '', '', '7x7=49', '', '', '', '', '', '', '', '', '', '8x7=56', '', '', '', '', '', '', '', '', '', '', '9x7=63', '', '', '', '', '', '', '', '', '', '', '', '', '', '10x7=70', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '11x7=77', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '12x7=84', '\n', '', '', '', '7x8=56', '', '', '', '', '', '', '', '', '', '8x8=64', '', '', '', '', '', '', '', '', '', '', '9x8=72', '', '', '', '', '', '', '', '', '', '', '', '', '', '10x8=80', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '11x8=88', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '12x8=96', '\n', '', '', '', '7x9=63', '', '', '', '', '', '', '', '', '', '8x9=72', '', '', '', '', '', '', '', '', '', '', '9x9=81', '', '', '', '', '', '', '', '', '', '', '', '', '', '10x9=90', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '11x9=99', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '12x9=108', '\n', '', '', '', '7x10=70', '', '', '', '', '', '', '', '8x10=80', '', '', '', '', '', '', '', '', '9x10=90', '', '', '', '', '', '', '', '', '', '', '10x10=100', '', '', '', '', '', '', '', '', '', '', '', '11x10=110', '', '', '', '', '', '', '', '', '', '', '12x10=120', '\n', '', '', '', '7x11=77', '', '', '', '', '', '', '', '8x11=88', '', '', '', '', '', '', '', '', '9x11=99', '', '', '', '', '', '', '', '', '', '', '10x11=110', '', '', '', '', '', '', '', '', '', '', '', '11x11=121', '', '', '', '', '', '', '', '', '', '', '12x11=132', '\n', '', '', '', '7x12=84', '', '', '', '', '', '', '', '8x12=96', '', '', '', '', '', '', '', '', '9x12=108', '', '', '', '', '', '', '', '', '10x12=120', '', '', '', '', '', '', '', '', '', '', '', '11x12=132', '', '', '', '', '', '', '', '', '', '', '12x12=144']
Build vocabulary...
('max_document_length...', 1713)
('x shape', (280294, 1713))
Split train/test set...
Vocabulary Size: 161959
Train/Dev split: 278893/1401
Writing to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481479730

2016-12-11T13:08:56.373839: step 1, loss 3.56049, acc 0.3125
2016-12-11T13:08:59.564084: step 2, loss 1.74757, acc 0.59375
2016-12-11T13:09:02.912132: step 3, loss 1.40823, acc 0.6875
2016-12-11T13:09:06.106362: step 4, loss 1.1774, acc 0.78125
2016-12-11T13:09:09.518345: step 5, loss 0.699378, acc 0.859375
2016-12-11T13:09:12.911782: step 6, loss 0.580136, acc 0.9375
2016-12-11T13:09:16.243707: step 7, loss 0.86892, acc 0.90625
2016-12-11T13:09:19.614209: step 8, loss 1.6529, acc 0.890625
2016-12-11T13:09:22.962967: step 9, loss 1.71367, acc 0.859375
2016-12-11T13:09:26.349808: step 10, loss 0.412605, acc 0.953125
2016-12-11T13:09:29.702825: step 11, loss 1.37237, acc 0.875
2016-12-11T13:09:32.898760: step 12, loss 1.16967, acc 0.921875
2016-12-11T13:09:36.148026: step 13, loss 1.11531, acc 0.953125
2016-12-11T13:09:39.319680: step 14, loss 0.571884, acc 0.9375
2016-12-11T13:09:42.613961: step 15, loss 1.35625, acc 0.875
2016-12-11T13:09:45.748405: step 16, loss 1.15916, acc 0.875
2016-12-11T13:09:48.967512: step 17, loss 0.980745, acc 0.875
2016-12-11T13:09:52.171573: step 18, loss 0.999325, acc 0.875
2016-12-11T13:09:55.384259: step 19, loss 2.01737, acc 0.8125
2016-12-11T13:09:58.712905: step 20, loss 0.947578, acc 0.8125
2016-12-11T13:10:01.938973: step 21, loss 1.1989, acc 0.796875
2016-12-11T13:10:05.152661: step 22, loss 1.40593, acc 0.828125
2016-12-11T13:10:08.278505: step 23, loss 1.33637, acc 0.765625
2016-12-11T13:10:11.399496: step 24, loss 1.12719, acc 0.765625
2016-12-11T13:10:14.462620: step 25, loss 0.747174, acc 0.796875
2016-12-11T13:10:17.537426: step 26, loss 1.01381, acc 0.8125
2016-12-11T13:10:20.648902: step 27, loss 0.940023, acc 0.75
2016-12-11T13:10:23.681463: step 28, loss 1.23305, acc 0.71875
2016-12-11T13:10:26.873301: step 29, loss 1.39935, acc 0.65625
2016-12-11T13:10:30.001484: step 30, loss 0.739379, acc 0.796875
2016-12-11T13:10:33.198472: step 31, loss 1.31628, acc 0.65625
2016-12-11T13:10:36.202073: step 32, loss 1.08186, acc 0.765625
2016-12-11T13:10:39.327719: step 33, loss 0.811899, acc 0.796875
2016-12-11T13:10:42.406530: step 34, loss 0.965139, acc 0.796875
2016-12-11T13:10:45.651947: step 35, loss 1.43941, acc 0.671875
2016-12-11T13:10:48.745135: step 36, loss 1.20098, acc 0.796875
2016-12-11T13:10:51.976781: step 37, loss 0.705504, acc 0.828125
2016-12-11T13:10:55.039485: step 38, loss 0.504708, acc 0.875
2016-12-11T13:10:58.158051: step 39, loss 0.827563, acc 0.84375
2016-12-11T13:11:01.194511: step 40, loss 0.791557, acc 0.90625
2016-12-11T13:11:04.440910: step 41, loss 1.04361, acc 0.796875
2016-12-11T13:11:07.598919: step 42, loss 0.234974, acc 0.953125
2016-12-11T13:11:10.616738: step 43, loss 0.88502, acc 0.859375
2016-12-11T13:11:13.687581: step 44, loss 1.20824, acc 0.859375
2016-12-11T13:11:16.674041: step 45, loss 1.2108, acc 0.859375
2016-12-11T13:11:19.731438: step 46, loss 0.953806, acc 0.875
2016-12-11T13:11:22.773032: step 47, loss 0.921867, acc 0.859375
2016-12-11T13:11:25.829974: step 48, loss 1.19529, acc 0.859375
2016-12-11T13:11:28.985658: step 49, loss 0.646672, acc 0.859375
2016-12-11T13:11:32.088797: step 50, loss 0.969403, acc 0.828125
2016-12-11T13:11:35.081874: step 51, loss 0.648027, acc 0.875
2016-12-11T13:11:38.326760: step 52, loss 1.15199, acc 0.8125
2016-12-11T13:11:41.371218: step 53, loss 0.859538, acc 0.890625
2016-12-11T13:11:44.407998: step 54, loss 1.02548, acc 0.828125
2016-12-11T13:11:47.522982: step 55, loss 1.37515, acc 0.796875
2016-12-11T13:11:50.538630: step 56, loss 1.40746, acc 0.71875
2016-12-11T13:11:53.546344: step 57, loss 0.933906, acc 0.78125
2016-12-11T13:11:56.666150: step 58, loss 1.19423, acc 0.703125
2016-12-11T13:11:59.694548: step 59, loss 0.666214, acc 0.875
2016-12-11T13:12:02.773096: step 60, loss 0.661437, acc 0.828125
2016-12-11T13:12:05.757822: step 61, loss 0.912094, acc 0.8125
2016-12-11T13:12:08.852197: step 62, loss 0.908754, acc 0.765625
2016-12-11T13:12:11.990326: step 63, loss 1.62404, acc 0.703125
2016-12-11T13:12:15.116355: step 64, loss 1.48614, acc 0.765625
2016-12-11T13:12:18.264245: step 65, loss 0.982205, acc 0.828125
2016-12-11T13:12:21.233052: step 66, loss 0.947398, acc 0.796875
2016-12-11T13:12:24.351187: step 67, loss 0.739752, acc 0.84375
2016-12-11T13:12:27.484052: step 68, loss 0.551024, acc 0.84375
2016-12-11T13:12:30.533413: step 69, loss 1.1908, acc 0.8125
2016-12-11T13:12:33.616014: step 70, loss 1.12211, acc 0.75
2016-12-11T13:12:36.660349: step 71, loss 0.752572, acc 0.828125
2016-12-11T13:12:39.711059: step 72, loss 0.986339, acc 0.8125
2016-12-11T13:12:42.918960: step 73, loss 1.36886, acc 0.78125
2016-12-11T13:12:46.007441: step 74, loss 2.09973, acc 0.71875
2016-12-11T13:12:49.092062: step 75, loss 1.0115, acc 0.78125
2016-12-11T13:12:52.093779: step 76, loss 0.737945, acc 0.875
2016-12-11T13:12:55.151632: step 77, loss 0.829913, acc 0.859375
2016-12-11T13:12:58.223954: step 78, loss 0.616705, acc 0.875
2016-12-11T13:13:01.566477: step 79, loss 0.489171, acc 0.875
2016-12-11T13:13:05.385722: step 80, loss 1.05513, acc 0.84375
2016-12-11T13:13:08.850508: step 81, loss 0.590425, acc 0.859375
2016-12-11T13:13:12.114249: step 82, loss 0.87658, acc 0.8125
2016-12-11T13:13:15.767693: step 83, loss 0.575237, acc 0.890625
2016-12-11T13:13:18.902786: step 84, loss 0.908993, acc 0.828125
2016-12-11T13:13:21.828617: step 85, loss 0.375328, acc 0.890625
2016-12-11T13:13:24.890083: step 86, loss 1.0042, acc 0.78125
2016-12-11T13:13:28.037895: step 87, loss 1.15222, acc 0.828125
2016-12-11T13:13:31.157282: step 88, loss 1.18797, acc 0.796875
2016-12-11T13:13:34.203537: step 89, loss 0.915769, acc 0.796875
2016-12-11T13:13:37.234385: step 90, loss 0.746234, acc 0.8125
2016-12-11T13:13:40.440029: step 91, loss 1.05676, acc 0.765625
2016-12-11T13:13:43.451851: step 92, loss 0.691348, acc 0.828125
2016-12-11T13:13:46.482897: step 93, loss 0.912168, acc 0.8125
2016-12-11T13:13:49.745389: step 94, loss 0.82251, acc 0.8125
2016-12-11T13:13:52.744853: step 95, loss 0.9415, acc 0.828125
2016-12-11T13:13:55.780982: step 96, loss 1.36456, acc 0.71875
2016-12-11T13:13:58.887547: step 97, loss 0.5171, acc 0.84375
2016-12-11T13:14:01.868375: step 98, loss 0.651329, acc 0.796875
2016-12-11T13:14:04.895084: step 99, loss 0.708817, acc 0.859375
2016-12-11T13:14:07.953351: step 100, loss 0.647776, acc 0.84375

Evaluation:
2016-12-11T13:14:48.396792: step 100, loss 0.639242, acc 0.896502

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481479730/checkpoints/model-100

2016-12-11T13:14:53.851724: step 101, loss 0.683273, acc 0.875
2016-12-11T13:14:56.996739: step 102, loss 0.732193, acc 0.84375
2016-12-11T13:15:00.061114: step 103, loss 0.974188, acc 0.796875
2016-12-11T13:15:03.225373: step 104, loss 1.19865, acc 0.8125
2016-12-11T13:15:06.415389: step 105, loss 0.600353, acc 0.828125
2016-12-11T13:15:09.567836: step 106, loss 1.35147, acc 0.828125
2016-12-11T13:15:12.689142: step 107, loss 0.888878, acc 0.859375
2016-12-11T13:15:15.675716: step 108, loss 0.588945, acc 0.828125
2016-12-11T13:15:18.703326: step 109, loss 0.776493, acc 0.8125
2016-12-11T13:15:21.731259: step 110, loss 0.579338, acc 0.890625
2016-12-11T13:15:24.854963: step 111, loss 0.930405, acc 0.796875
2016-12-11T13:15:27.860394: step 112, loss 1.07221, acc 0.796875
2016-12-11T13:15:31.008993: step 113, loss 0.600095, acc 0.765625
2016-12-11T13:15:34.163013: step 114, loss 0.973801, acc 0.78125
2016-12-11T13:15:37.164917: step 115, loss 1.12387, acc 0.84375
2016-12-11T13:15:40.161182: step 116, loss 0.61948, acc 0.828125
2016-12-11T13:15:43.190614: step 117, loss 0.42078, acc 0.859375
2016-12-11T13:15:46.257127: step 118, loss 0.843223, acc 0.78125
2016-12-11T13:15:49.266064: step 119, loss 0.984776, acc 0.75
2016-12-11T13:15:52.279221: step 120, loss 0.817185, acc 0.8125
2016-12-11T13:15:55.364996: step 121, loss 1.00936, acc 0.75
2016-12-11T13:15:58.407149: step 122, loss 1.06157, acc 0.828125
2016-12-11T13:16:01.512294: step 123, loss 1.09964, acc 0.78125
2016-12-11T13:16:04.593623: step 124, loss 0.600787, acc 0.84375
2016-12-11T13:16:07.685387: step 125, loss 1.03893, acc 0.8125
2016-12-11T13:16:10.701331: step 126, loss 0.171445, acc 0.953125
2016-12-11T13:16:13.795991: step 127, loss 1.19066, acc 0.84375
2016-12-11T13:16:16.907281: step 128, loss 0.194675, acc 0.921875
2016-12-11T13:16:19.914533: step 129, loss 0.483367, acc 0.859375
2016-12-11T13:16:22.936959: step 130, loss 0.764974, acc 0.84375
2016-12-11T13:16:26.060178: step 131, loss 0.668355, acc 0.890625
2016-12-11T13:16:29.086995: step 132, loss 0.614131, acc 0.828125
2016-12-11T13:16:32.102580: step 133, loss 0.736504, acc 0.90625
2016-12-11T13:16:35.402323: step 134, loss 1.24669, acc 0.828125
2016-12-11T13:16:38.508295: step 135, loss 1.06805, acc 0.84375
2016-12-11T13:16:41.481875: step 136, loss 0.366963, acc 0.875
2016-12-11T13:16:44.590606: step 137, loss 1.24745, acc 0.765625
2016-12-11T13:16:47.644904: step 138, loss 0.847647, acc 0.8125
2016-12-11T13:16:50.779859: step 139, loss 0.911892, acc 0.765625
2016-12-11T13:16:53.851881: step 140, loss 0.737624, acc 0.890625
2016-12-11T13:16:56.843323: step 141, loss 1.38356, acc 0.734375
2016-12-11T13:16:59.989051: step 142, loss 0.95981, acc 0.78125
2016-12-11T13:17:02.969905: step 143, loss 1.32627, acc 0.734375
2016-12-11T13:17:06.190604: step 144, loss 0.75992, acc 0.8125
2016-12-11T13:17:09.336016: step 145, loss 0.823595, acc 0.765625
2016-12-11T13:17:12.370539: step 146, loss 0.934743, acc 0.796875
2016-12-11T13:17:15.361472: step 147, loss 0.957257, acc 0.71875
2016-12-11T13:17:18.385661: step 148, loss 0.550466, acc 0.859375
2016-12-11T13:17:21.357022: step 149, loss 0.91608, acc 0.78125
2016-12-11T13:17:24.431274: step 150, loss 1.02759, acc 0.765625
2016-12-11T13:17:27.501634: step 151, loss 0.816593, acc 0.828125
2016-12-11T13:17:30.552851: step 152, loss 0.910448, acc 0.84375
2016-12-11T13:17:33.591959: step 153, loss 0.469732, acc 0.9375
2016-12-11T13:17:36.707670: step 154, loss 0.510801, acc 0.90625
2016-12-11T13:17:39.945206: step 155, loss 0.598067, acc 0.84375
2016-12-11T13:17:42.998725: step 156, loss 1.24131, acc 0.765625
2016-12-11T13:17:46.095703: step 157, loss 1.13502, acc 0.65625
2016-12-11T13:17:49.095541: step 158, loss 0.68023, acc 0.828125
2016-12-11T13:17:52.160601: step 159, loss 1.15705, acc 0.796875
2016-12-11T13:17:55.217105: step 160, loss 1.35756, acc 0.75
2016-12-11T13:17:58.272579: step 161, loss 1.09609, acc 0.765625
2016-12-11T13:18:01.651292: step 162, loss 1.08687, acc 0.75
2016-12-11T13:18:04.998169: step 163, loss 1.21065, acc 0.78125
2016-12-11T13:18:08.419315: step 164, loss 1.01567, acc 0.78125
2016-12-11T13:18:12.053278: step 165, loss 0.834472, acc 0.78125
2016-12-11T13:18:15.155971: step 166, loss 1.39742, acc 0.625
2016-12-11T13:18:18.288796: step 167, loss 0.911001, acc 0.8125
2016-12-11T13:18:21.232153: step 168, loss 0.676026, acc 0.796875
2016-12-11T13:18:24.352511: step 169, loss 0.486653, acc 0.828125
2016-12-11T13:18:27.362153: step 170, loss 0.775353, acc 0.828125
2016-12-11T13:18:30.418403: step 171, loss 0.958343, acc 0.796875
2016-12-11T13:18:33.470548: step 172, loss 0.674315, acc 0.859375
2016-12-11T13:18:36.532053: step 173, loss 0.639471, acc 0.859375
2016-12-11T13:18:39.597299: step 174, loss 1.08515, acc 0.859375
2016-12-11T13:18:42.535472: step 175, loss 0.842662, acc 0.828125
2016-12-11T13:18:45.766118: step 176, loss 0.31407, acc 0.90625
2016-12-11T13:18:48.899996: step 177, loss 0.734605, acc 0.84375
2016-12-11T13:18:51.845544: step 178, loss 0.912758, acc 0.859375
2016-12-11T13:18:54.939858: step 179, loss 0.583798, acc 0.828125
2016-12-11T13:18:57.998881: step 180, loss 1.23412, acc 0.8125
2016-12-11T13:19:01.000456: step 181, loss 0.736484, acc 0.890625
2016-12-11T13:19:04.054750: step 182, loss 0.900708, acc 0.84375
2016-12-11T13:19:07.183292: step 183, loss 0.734078, acc 0.796875
2016-12-11T13:19:10.165708: step 184, loss 0.801342, acc 0.765625
2016-12-11T13:19:13.195951: step 185, loss 1.14565, acc 0.734375
2016-12-11T13:19:16.400198: step 186, loss 0.68618, acc 0.859375
2016-12-11T13:19:19.519795: step 187, loss 1.08594, acc 0.71875
2016-12-11T13:19:22.571494: step 188, loss 0.83465, acc 0.828125
2016-12-11T13:19:25.644459: step 189, loss 0.709351, acc 0.859375
2016-12-11T13:19:28.656144: step 190, loss 0.719343, acc 0.8125
2016-12-11T13:19:31.698357: step 191, loss 0.871748, acc 0.8125
2016-12-11T13:19:34.799823: step 192, loss 0.950783, acc 0.8125
2016-12-11T13:19:37.836867: step 193, loss 0.629203, acc 0.84375
2016-12-11T13:19:40.856469: step 194, loss 0.527687, acc 0.90625
2016-12-11T13:19:43.938658: step 195, loss 0.609615, acc 0.875
2016-12-11T13:19:46.993283: step 196, loss 1.25055, acc 0.78125
2016-12-11T13:19:50.252560: step 197, loss 0.588458, acc 0.875
2016-12-11T13:19:53.357345: step 198, loss 0.701502, acc 0.78125
2016-12-11T13:19:56.465825: step 199, loss 0.60599, acc 0.828125
2016-12-11T13:19:59.467513: step 200, loss 0.182825, acc 0.953125

Evaluation:
2016-12-11T13:20:30.469494: step 200, loss 0.470893, acc 0.896502

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481479730/checkpoints/model-200

2016-12-11T13:20:35.584237: step 201, loss 0.671367, acc 0.765625
2016-12-11T13:20:38.525900: step 202, loss 0.562798, acc 0.859375
2016-12-11T13:20:41.483078: step 203, loss 1.45854, acc 0.71875
2016-12-11T13:20:44.581138: step 204, loss 0.736907, acc 0.828125
2016-12-11T13:20:47.702176: step 205, loss 0.522773, acc 0.8125
2016-12-11T13:20:50.774335: step 206, loss 0.882272, acc 0.765625
2016-12-11T13:20:53.913387: step 207, loss 1.1259, acc 0.765625
2016-12-11T13:20:57.049598: step 208, loss 0.978435, acc 0.75
2016-12-11T13:21:00.374286: step 209, loss 0.795853, acc 0.78125
2016-12-11T13:21:03.606918: step 210, loss 0.510202, acc 0.875
2016-12-11T13:21:06.708028: step 211, loss 1.02513, acc 0.71875
2016-12-11T13:21:09.917333: step 212, loss 0.793205, acc 0.6875
2016-12-11T13:21:12.968343: step 213, loss 0.922978, acc 0.734375
2016-12-11T13:21:16.044211: step 214, loss 0.662154, acc 0.765625
2016-12-11T13:21:19.113071: step 215, loss 0.765919, acc 0.765625
2016-12-11T13:21:22.190220: step 216, loss 0.474775, acc 0.84375
2016-12-11T13:21:25.259095: step 217, loss 0.540256, acc 0.84375
2016-12-11T13:21:28.292239: step 218, loss 0.775177, acc 0.859375
2016-12-11T13:21:31.481170: step 219, loss 0.37425, acc 0.90625
2016-12-11T13:21:34.643500: step 220, loss 0.59651, acc 0.859375
2016-12-11T13:21:37.774494: step 221, loss 0.582009, acc 0.90625
2016-12-11T13:21:40.755012: step 222, loss 0.765638, acc 0.890625
2016-12-11T13:21:43.758651: step 223, loss 0.533026, acc 0.921875
2016-12-11T13:21:46.770871: step 224, loss 0.87226, acc 0.875
2016-12-11T13:21:49.864289: step 225, loss 0.927778, acc 0.8125
2016-12-11T13:21:52.847899: step 226, loss 0.570548, acc 0.890625
2016-12-11T13:21:55.925455: step 227, loss 0.269351, acc 0.90625
2016-12-11T13:21:59.046299: step 228, loss 0.651606, acc 0.875
2016-12-11T13:22:02.168128: step 229, loss 0.658856, acc 0.828125
2016-12-11T13:22:05.382519: step 230, loss 1.03372, acc 0.75
2016-12-11T13:22:08.476790: step 231, loss 0.48281, acc 0.859375
2016-12-11T13:22:11.441174: step 232, loss 0.552825, acc 0.828125
2016-12-11T13:22:14.596808: step 233, loss 1.02973, acc 0.828125
2016-12-11T13:22:17.633210: step 234, loss 0.393702, acc 0.890625
2016-12-11T13:22:20.646732: step 235, loss 0.799273, acc 0.84375
2016-12-11T13:22:23.786689: step 236, loss 1.20117, acc 0.71875
2016-12-11T13:22:26.965594: step 237, loss 0.437484, acc 0.890625
2016-12-11T13:22:30.063994: step 238, loss 0.619456, acc 0.796875
2016-12-11T13:22:33.085325: step 239, loss 0.269921, acc 0.90625
2016-12-11T13:22:36.411997: step 240, loss 0.503189, acc 0.84375
2016-12-11T13:22:39.483199: step 241, loss 0.729751, acc 0.796875
2016-12-11T13:22:42.546582: step 242, loss 0.614825, acc 0.84375
2016-12-11T13:22:45.625014: step 243, loss 0.62166, acc 0.84375
2016-12-11T13:22:48.691805: step 244, loss 0.689303, acc 0.84375
2016-12-11T13:22:51.705162: step 245, loss 0.589417, acc 0.875
2016-12-11T13:22:54.762563: step 246, loss 0.603898, acc 0.828125
2016-12-11T13:22:57.772856: step 247, loss 0.289808, acc 0.921875
2016-12-11T13:23:00.966336: step 248, loss 0.667827, acc 0.8125
2016-12-11T13:23:04.484341: step 249, loss 0.83091, acc 0.78125
2016-12-11T13:23:08.095071: step 250, loss 0.612935, acc 0.84375
2016-12-11T13:23:11.588666: step 251, loss 0.573935, acc 0.8125
2016-12-11T13:23:14.727518: step 252, loss 0.852653, acc 0.84375
2016-12-11T13:23:17.797509: step 253, loss 0.459946, acc 0.859375
2016-12-11T13:23:20.836644: step 254, loss 0.643255, acc 0.8125
2016-12-11T13:23:23.899598: step 255, loss 0.673759, acc 0.8125
2016-12-11T13:23:26.972454: step 256, loss 0.50057, acc 0.828125
2016-12-11T13:23:30.295357: step 257, loss 0.442311, acc 0.890625
2016-12-11T13:23:33.277967: step 258, loss 0.886717, acc 0.828125
2016-12-11T13:23:36.278156: step 259, loss 0.669707, acc 0.84375
2016-12-11T13:23:39.327025: step 260, loss 0.634808, acc 0.828125
2016-12-11T13:23:42.528131: step 261, loss 1.15167, acc 0.78125
2016-12-11T13:23:45.873669: step 262, loss 0.664467, acc 0.828125
2016-12-11T13:23:49.020154: step 263, loss 0.442914, acc 0.828125
2016-12-11T13:23:52.058377: step 264, loss 0.681826, acc 0.828125
2016-12-11T13:23:55.136155: step 265, loss 0.555885, acc 0.84375
2016-12-11T13:23:58.238736: step 266, loss 0.625532, acc 0.84375
2016-12-11T13:24:01.287665: step 267, loss 0.504975, acc 0.875
2016-12-11T13:24:04.347363: step 268, loss 0.506834, acc 0.921875
2016-12-11T13:24:07.522805: step 269, loss 0.378923, acc 0.875
2016-12-11T13:24:10.719143: step 270, loss 1.22398, acc 0.703125
2016-12-11T13:24:14.100851: step 271, loss 0.757406, acc 0.765625
2016-12-11T13:24:17.316324: step 272, loss 0.238104, acc 0.90625
2016-12-11T13:24:20.433739: step 273, loss 0.549537, acc 0.875
2016-12-11T13:24:23.425118: step 274, loss 0.565009, acc 0.8125
2016-12-11T13:24:26.486123: step 275, loss 0.536521, acc 0.890625
2016-12-11T13:24:29.636938: step 276, loss 0.436822, acc 0.875
2016-12-11T13:24:33.119666: step 277, loss 0.975028, acc 0.796875
2016-12-11T13:24:36.454074: step 278, loss 0.864199, acc 0.765625
2016-12-11T13:24:39.665792: step 279, loss 0.716383, acc 0.859375
2016-12-11T13:24:42.915003: step 280, loss 0.714762, acc 0.828125
2016-12-11T13:24:46.388546: step 281, loss 0.730174, acc 0.8125
2016-12-11T13:24:49.562830: step 282, loss 0.747362, acc 0.828125
2016-12-11T13:24:52.655756: step 283, loss 0.665203, acc 0.75
2016-12-11T13:24:55.661035: step 284, loss 0.726487, acc 0.796875
2016-12-11T13:24:58.740893: step 285, loss 1.04862, acc 0.65625
2016-12-11T13:25:01.804755: step 286, loss 0.5605, acc 0.84375
2016-12-11T13:25:04.865928: step 287, loss 0.495606, acc 0.796875
2016-12-11T13:25:08.076793: step 288, loss 0.619996, acc 0.84375
2016-12-11T13:25:11.467593: step 289, loss 0.71006, acc 0.828125
2016-12-11T13:25:14.874218: step 290, loss 0.556663, acc 0.84375
2016-12-11T13:25:18.378965: step 291, loss 0.689816, acc 0.890625
2016-12-11T13:25:21.737430: step 292, loss 0.568127, acc 0.890625
2016-12-11T13:25:25.140493: step 293, loss 0.455316, acc 0.859375
2016-12-11T13:25:28.420555: step 294, loss 1.16162, acc 0.75
2016-12-11T13:25:31.731712: step 295, loss 0.221943, acc 0.9375
2016-12-11T13:25:35.064985: step 296, loss 0.686354, acc 0.90625
2016-12-11T13:25:38.344984: step 297, loss 0.571052, acc 0.8125
2016-12-11T13:25:41.601949: step 298, loss 0.935622, acc 0.765625
2016-12-11T13:25:44.994339: step 299, loss 0.902251, acc 0.8125
2016-12-11T13:25:48.407959: step 300, loss 0.632664, acc 0.8125

Evaluation:
2016-12-11T13:26:28.420055: step 300, loss 0.362982, acc 0.896502

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481479730/checkpoints/model-300

2016-12-11T13:26:34.282637: step 301, loss 0.553859, acc 0.828125
2016-12-11T13:26:37.975025: step 302, loss 0.685683, acc 0.828125
2016-12-11T13:26:42.414993: step 303, loss 0.644833, acc 0.75
2016-12-11T13:26:46.414699: step 304, loss 0.573135, acc 0.84375
2016-12-11T13:26:50.206800: step 305, loss 0.47602, acc 0.859375
2016-12-11T13:26:53.655410: step 306, loss 0.904005, acc 0.78125
2016-12-11T13:26:57.496395: step 307, loss 0.68444, acc 0.8125
2016-12-11T13:27:01.585309: step 308, loss 0.354916, acc 0.875
2016-12-11T13:27:04.923405: step 309, loss 0.490976, acc 0.828125
2016-12-11T13:27:08.140537: step 310, loss 0.518011, acc 0.828125
2016-12-11T13:27:11.413887: step 311, loss 0.450727, acc 0.859375
2016-12-11T13:27:14.606517: step 312, loss 0.618665, acc 0.875
2016-12-11T13:27:17.629915: step 313, loss 0.58567, acc 0.84375
2016-12-11T13:27:20.921206: step 314, loss 0.276891, acc 0.890625
2016-12-11T13:27:24.298272: step 315, loss 0.506078, acc 0.90625
2016-12-11T13:27:27.460817: step 316, loss 0.361592, acc 0.90625
2016-12-11T13:27:30.563279: step 317, loss 0.314451, acc 0.921875
2016-12-11T13:27:33.798086: step 318, loss 0.660815, acc 0.8125
2016-12-11T13:27:37.010761: step 319, loss 1.12755, acc 0.78125
2016-12-11T13:27:40.147892: step 320, loss 0.925591, acc 0.859375
2016-12-11T13:27:43.293346: step 321, loss 0.588394, acc 0.890625
2016-12-11T13:27:46.616914: step 322, loss 0.441237, acc 0.875
2016-12-11T13:27:49.730389: step 323, loss 0.49709, acc 0.828125
2016-12-11T13:27:52.827353: step 324, loss 0.497405, acc 0.84375
2016-12-11T13:27:55.925270: step 325, loss 0.847661, acc 0.78125
2016-12-11T13:27:59.032003: step 326, loss 0.381806, acc 0.796875
2016-12-11T13:28:02.804859: step 327, loss 0.741419, acc 0.84375
2016-12-11T13:28:07.041081: step 328, loss 0.594409, acc 0.828125
2016-12-11T13:28:10.811121: step 329, loss 0.571463, acc 0.875
2016-12-11T13:28:14.219468: step 330, loss 0.477752, acc 0.84375
2016-12-11T13:28:17.433176: step 331, loss 0.445721, acc 0.90625
2016-12-11T13:28:20.662436: step 332, loss 0.34215, acc 0.90625
2016-12-11T13:28:23.749659: step 333, loss 0.255592, acc 0.90625
2016-12-11T13:28:26.732039: step 334, loss 0.488504, acc 0.875
2016-12-11T13:28:29.767239: step 335, loss 0.740821, acc 0.828125
2016-12-11T13:28:32.793750: step 336, loss 0.625629, acc 0.828125
2016-12-11T13:28:35.928104: step 337, loss 0.925362, acc 0.828125