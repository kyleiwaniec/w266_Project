python train.py --positive_data_file data/pos_data_train --negative_data_file data/neg_data_train --dev_sample_percentage .005

Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=64
CHECKPOINT_EVERY=100
DEV_SAMPLE_PERCENTAGE=0.005
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=128
EVALUATE_EVERY=100
FILTER_SIZES=3,4,5
L2_REG_LAMBDA=0.0
LOG_DEVICE_PLACEMENT=False
NEGATIVE_DATA_FILE=data/neg_data_train
NUM_EPOCHS=200
NUM_FILTERS=128
POSITIVE_DATA_FILE=data/pos_data_train

Loading data...
('l-positive_examples', 248241)
('l-negative_examples', 163887)
('type', 412128, 412128)
['1)', 'first', '', 'change', 'your', 'answer', 'to', 'a', 'fraction.\n2)then', 'your', 'numerator', 'drops', 'into', 'a', 'division', 'box.', 'ex.', '1\n', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '-', '', '', '', '', '', '', '', '', '', '_______\n', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '2', '', '', '', '', '', '', '2|', '1\n3)you', 'cannot', 'do', 'this', 'problem', 'without', 'a', 'decimal\n\n', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '__', '__________\n', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '2|', '', '', '1.0', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '\n\n', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '_0.5_', '__________\n', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '2|', '', '', '1.0', '\n', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '0\n', '', '', '', '4)you', "divide(don't", 'forget', 'the', 'decimal', 'and', 'keep)', '', '', '', '', '', '', '', '', '', '--------']
Build vocabulary...
('max_document_length...', 702)
('x shape', (412128, 702))
Split train/test set...
Vocabulary Size: 173722
Train/Dev split: 410068/2060
Writing to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1482016882

2016-12-17T18:21:26.376884: step 1, loss 2.27317, acc 0.5625
2016-12-17T18:21:27.924260: step 2, loss 1.60443, acc 0.578125
2016-12-17T18:21:29.447526: step 3, loss 2.62627, acc 0.484375
2016-12-17T18:21:30.937060: step 4, loss 1.60036, acc 0.515625
2016-12-17T18:21:32.442689: step 5, loss 2.29002, acc 0.46875
2016-12-17T18:21:33.986363: step 6, loss 1.5911, acc 0.59375
2016-12-17T18:21:35.516360: step 7, loss 2.18075, acc 0.515625
2016-12-17T18:21:37.140752: step 8, loss 2.04131, acc 0.578125
2016-12-17T18:21:38.696947: step 9, loss 1.77851, acc 0.546875
2016-12-17T18:21:40.250754: step 10, loss 2.05105, acc 0.578125
2016-12-17T18:21:41.800058: step 11, loss 2.1668, acc 0.5625
2016-12-17T18:21:43.358459: step 12, loss 1.66165, acc 0.578125
2016-12-17T18:21:44.945310: step 13, loss 2.15544, acc 0.546875
2016-12-17T18:21:46.538539: step 14, loss 1.64921, acc 0.578125
2016-12-17T18:21:48.129225: step 15, loss 1.49306, acc 0.578125
2016-12-17T18:21:49.714452: step 16, loss 2.01219, acc 0.515625
2016-12-17T18:21:51.336876: step 17, loss 1.8848, acc 0.546875
2016-12-17T18:21:53.008906: step 18, loss 1.99096, acc 0.5625
2016-12-17T18:21:54.757183: step 19, loss 1.92166, acc 0.53125
2016-12-17T18:21:56.331960: step 20, loss 1.77006, acc 0.421875
2016-12-17T18:21:57.953119: step 21, loss 1.33238, acc 0.59375
2016-12-17T18:21:59.574582: step 22, loss 1.64672, acc 0.59375
2016-12-17T18:22:01.182425: step 23, loss 2.00909, acc 0.46875
2016-12-17T18:22:02.770016: step 24, loss 1.52306, acc 0.609375
2016-12-17T18:22:04.343797: step 25, loss 2.04337, acc 0.484375
2016-12-17T18:22:05.981407: step 26, loss 1.82758, acc 0.546875
2016-12-17T18:22:07.540995: step 27, loss 1.96442, acc 0.4375
2016-12-17T18:22:09.205609: step 28, loss 1.65725, acc 0.59375
2016-12-17T18:22:10.845911: step 29, loss 1.44369, acc 0.546875
2016-12-17T18:22:12.414565: step 30, loss 1.91478, acc 0.53125
2016-12-17T18:22:13.998222: step 31, loss 2.07916, acc 0.53125
2016-12-17T18:22:15.539453: step 32, loss 2.00353, acc 0.578125
2016-12-17T18:22:17.115769: step 33, loss 2.31756, acc 0.515625
2016-12-17T18:22:18.712475: step 34, loss 2.02351, acc 0.515625
2016-12-17T18:22:20.261366: step 35, loss 2.017, acc 0.453125
2016-12-17T18:22:21.804325: step 36, loss 2.04739, acc 0.59375
2016-12-17T18:22:23.362788: step 37, loss 2.07297, acc 0.5
2016-12-17T18:22:24.915435: step 38, loss 1.93633, acc 0.453125
2016-12-17T18:22:26.485251: step 39, loss 1.99364, acc 0.53125
2016-12-17T18:22:28.085851: step 40, loss 1.81072, acc 0.46875
2016-12-17T18:22:29.667746: step 41, loss 1.96237, acc 0.53125
2016-12-17T18:22:31.293557: step 42, loss 2.05307, acc 0.453125
2016-12-17T18:22:32.961450: step 43, loss 2.29209, acc 0.5625
2016-12-17T18:22:34.686739: step 44, loss 1.20658, acc 0.578125
2016-12-17T18:22:36.398144: step 45, loss 1.54188, acc 0.609375
2016-12-17T18:22:38.066721: step 46, loss 1.6831, acc 0.59375
2016-12-17T18:22:39.658974: step 47, loss 2.08385, acc 0.484375
2016-12-17T18:22:41.314421: step 48, loss 1.8688, acc 0.609375
2016-12-17T18:22:42.891993: step 49, loss 2.52791, acc 0.59375
2016-12-17T18:22:44.420394: step 50, loss 1.78733, acc 0.5625
2016-12-17T18:22:45.983443: step 51, loss 1.03842, acc 0.65625
2016-12-17T18:22:47.533369: step 52, loss 1.72099, acc 0.484375
2016-12-17T18:22:49.107675: step 53, loss 1.2731, acc 0.609375
2016-12-17T18:22:50.727810: step 54, loss 1.90155, acc 0.53125
2016-12-17T18:22:52.316895: step 55, loss 1.26348, acc 0.609375
2016-12-17T18:22:53.896472: step 56, loss 1.94811, acc 0.5
2016-12-17T18:22:55.455872: step 57, loss 2.06325, acc 0.484375
2016-12-17T18:22:56.972323: step 58, loss 2.15465, acc 0.4375
2016-12-17T18:22:58.519153: step 59, loss 1.79101, acc 0.453125
2016-12-17T18:23:00.144284: step 60, loss 1.38886, acc 0.5625
2016-12-17T18:23:01.843900: step 61, loss 1.49826, acc 0.515625
2016-12-17T18:23:03.507560: step 62, loss 1.51549, acc 0.640625
2016-12-17T18:23:05.220579: step 63, loss 1.67883, acc 0.453125
2016-12-17T18:23:06.927399: step 64, loss 1.61566, acc 0.59375
2016-12-17T18:23:08.521000: step 65, loss 1.9057, acc 0.5
2016-12-17T18:23:10.070013: step 66, loss 1.69258, acc 0.625
2016-12-17T18:23:11.628862: step 67, loss 1.72366, acc 0.5625
2016-12-17T18:23:13.241128: step 68, loss 1.54576, acc 0.546875
2016-12-17T18:23:14.849468: step 69, loss 2.10649, acc 0.5
2016-12-17T18:23:16.427868: step 70, loss 1.39223, acc 0.59375
2016-12-17T18:23:18.029400: step 71, loss 1.55401, acc 0.53125
2016-12-17T18:23:19.683802: step 72, loss 1.15239, acc 0.59375
2016-12-17T18:23:21.216368: step 73, loss 1.94894, acc 0.46875
2016-12-17T18:23:22.833893: step 74, loss 1.78002, acc 0.5625
2016-12-17T18:23:24.431710: step 75, loss 1.78662, acc 0.53125
2016-12-17T18:23:26.028969: step 76, loss 1.16344, acc 0.640625
2016-12-17T18:23:27.562888: step 77, loss 1.57025, acc 0.5
2016-12-17T18:23:29.098403: step 78, loss 1.46872, acc 0.546875
2016-12-17T18:23:30.636946: step 79, loss 1.61172, acc 0.53125
2016-12-17T18:23:32.199281: step 80, loss 1.217, acc 0.59375
2016-12-17T18:23:33.734060: step 81, loss 1.06026, acc 0.6875
2016-12-17T18:23:35.280601: step 82, loss 1.27001, acc 0.59375
2016-12-17T18:23:36.798377: step 83, loss 1.27912, acc 0.546875
2016-12-17T18:23:38.340446: step 84, loss 1.34939, acc 0.609375
2016-12-17T18:23:39.885581: step 85, loss 2.08125, acc 0.484375
2016-12-17T18:23:41.425011: step 86, loss 1.82432, acc 0.46875
2016-12-17T18:23:42.964336: step 87, loss 1.65122, acc 0.46875
2016-12-17T18:23:44.557469: step 88, loss 1.41247, acc 0.59375
2016-12-17T18:23:46.146899: step 89, loss 1.67786, acc 0.59375
2016-12-17T18:23:47.673622: step 90, loss 1.58566, acc 0.59375
2016-12-17T18:23:49.206941: step 91, loss 1.28945, acc 0.59375
2016-12-17T18:23:50.756235: step 92, loss 1.35917, acc 0.609375
2016-12-17T18:23:52.315483: step 93, loss 1.03637, acc 0.625
2016-12-17T18:23:53.849766: step 94, loss 1.45058, acc 0.65625
2016-12-17T18:23:55.424281: step 95, loss 1.19622, acc 0.640625
2016-12-17T18:23:56.960017: step 96, loss 0.857293, acc 0.6875
2016-12-17T18:23:58.493356: step 97, loss 0.968794, acc 0.703125
2016-12-17T18:24:00.032962: step 98, loss 2.09133, acc 0.484375
2016-12-17T18:24:01.583511: step 99, loss 1.27222, acc 0.515625
2016-12-17T18:24:03.134186: step 100, loss 1.63241, acc 0.546875

Evaluation:
2016-12-17T18:24:22.492675: step 100, loss 0.765381, acc 0.596116

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1482016882/checkpoints/model-100

2016-12-17T18:24:26.599431: step 101, loss 1.92613, acc 0.390625
2016-12-17T18:24:28.118823: step 102, loss 1.11425, acc 0.546875
2016-12-17T18:24:29.644766: step 103, loss 1.5296, acc 0.515625
2016-12-17T18:24:31.185787: step 104, loss 1.29844, acc 0.53125
2016-12-17T18:24:32.745348: step 105, loss 1.38633, acc 0.5625
2016-12-17T18:24:34.269962: step 106, loss 1.32377, acc 0.640625
2016-12-17T18:24:35.773221: step 107, loss 1.05981, acc 0.640625
2016-12-17T18:24:37.293071: step 108, loss 1.33937, acc 0.625
2016-12-17T18:24:38.851391: step 109, loss 1.57893, acc 0.578125
2016-12-17T18:24:40.378726: step 110, loss 1.83991, acc 0.515625
2016-12-17T18:24:41.886900: step 111, loss 1.28564, acc 0.53125
2016-12-17T18:24:43.397105: step 112, loss 1.30979, acc 0.609375
2016-12-17T18:24:44.918176: step 113, loss 1.41903, acc 0.484375
2016-12-17T18:24:46.444013: step 114, loss 1.46683, acc 0.46875
2016-12-17T18:24:47.999065: step 115, loss 1.57549, acc 0.53125
2016-12-17T18:24:49.543950: step 116, loss 1.17777, acc 0.578125
2016-12-17T18:24:51.098138: step 117, loss 1.30626, acc 0.53125
2016-12-17T18:24:52.691732: step 118, loss 1.23977, acc 0.640625
2016-12-17T18:24:54.264194: step 119, loss 1.50541, acc 0.578125
2016-12-17T18:24:55.757213: step 120, loss 1.32071, acc 0.578125
2016-12-17T18:24:57.266682: step 121, loss 1.91784, acc 0.40625
2016-12-17T18:24:58.749988: step 122, loss 1.08727, acc 0.5625
2016-12-17T18:25:00.277976: step 123, loss 1.48837, acc 0.453125
2016-12-17T18:25:01.817783: step 124, loss 0.992965, acc 0.625
2016-12-17T18:25:03.315936: step 125, loss 1.51362, acc 0.515625
2016-12-17T18:25:04.847861: step 126, loss 1.52205, acc 0.515625
2016-12-17T18:25:06.372567: step 127, loss 1.79709, acc 0.4375
2016-12-17T18:25:07.894894: step 128, loss 1.43829, acc 0.5
2016-12-17T18:25:09.431405: step 129, loss 1.19902, acc 0.515625
2016-12-17T18:25:10.943178: step 130, loss 1.31993, acc 0.578125
2016-12-17T18:25:12.454045: step 131, loss 1.371, acc 0.578125
2016-12-17T18:25:13.952879: step 132, loss 1.05882, acc 0.625
2016-12-17T18:25:15.498369: step 133, loss 1.28923, acc 0.515625
2016-12-17T18:25:17.014352: step 134, loss 1.6261, acc 0.546875
2016-12-17T18:25:18.543158: step 135, loss 1.45145, acc 0.59375
2016-12-17T18:25:20.015919: step 136, loss 1.05268, acc 0.640625
2016-12-17T18:25:21.547825: step 137, loss 1.44691, acc 0.46875
2016-12-17T18:25:23.080367: step 138, loss 1.00231, acc 0.6875
2016-12-17T18:25:24.633294: step 139, loss 1.34842, acc 0.625
2016-12-17T18:25:26.134205: step 140, loss 1.47836, acc 0.515625
2016-12-17T18:25:27.654310: step 141, loss 1.11634, acc 0.6875
2016-12-17T18:25:29.207313: step 142, loss 1.32632, acc 0.625
2016-12-17T18:25:30.686135: step 143, loss 1.48041, acc 0.515625
2016-12-17T18:25:32.214098: step 144, loss 1.37931, acc 0.53125
2016-12-17T18:25:33.718303: step 145, loss 1.17116, acc 0.515625
2016-12-17T18:25:35.381637: step 146, loss 1.57281, acc 0.5
2016-12-17T18:25:36.982109: step 147, loss 1.40147, acc 0.5625
2016-12-17T18:25:38.524530: step 148, loss 1.43085, acc 0.46875
2016-12-17T18:25:40.022493: step 149, loss 0.975146, acc 0.625
2016-12-17T18:25:41.559543: step 150, loss 1.15451, acc 0.59375
2016-12-17T18:25:43.064850: step 151, loss 1.21647, acc 0.5
2016-12-17T18:25:44.616205: step 152, loss 0.971359, acc 0.625
2016-12-17T18:25:46.167378: step 153, loss 1.35033, acc 0.515625
2016-12-17T18:25:47.665565: step 154, loss 1.26931, acc 0.5625
2016-12-17T18:25:49.159516: step 155, loss 1.24241, acc 0.578125
2016-12-17T18:25:50.711001: step 156, loss 1.49385, acc 0.515625
2016-12-17T18:25:52.203230: step 157, loss 1.39128, acc 0.53125
2016-12-17T18:25:53.741110: step 158, loss 1.54851, acc 0.5
2016-12-17T18:25:55.254479: step 159, loss 1.34518, acc 0.421875
2016-12-17T18:25:56.834432: step 160, loss 1.24109, acc 0.515625
2016-12-17T18:25:58.347574: step 161, loss 0.936723, acc 0.53125
2016-12-17T18:25:59.873623: step 162, loss 1.71091, acc 0.4375
2016-12-17T18:26:01.388864: step 163, loss 1.44484, acc 0.484375
2016-12-17T18:26:02.917726: step 164, loss 1.01691, acc 0.578125
2016-12-17T18:26:04.415026: step 165, loss 1.36196, acc 0.484375
2016-12-17T18:26:05.918064: step 166, loss 1.03009, acc 0.609375
2016-12-17T18:26:07.427877: step 167, loss 1.39349, acc 0.5625
2016-12-17T18:26:08.920673: step 168, loss 1.36431, acc 0.453125
2016-12-17T18:26:10.420534: step 169, loss 1.1698, acc 0.546875
2016-12-17T18:26:11.964485: step 170, loss 1.15944, acc 0.625
2016-12-17T18:26:13.486746: step 171, loss 1.31965, acc 0.46875
2016-12-17T18:26:14.987209: step 172, loss 1.17103, acc 0.578125
2016-12-17T18:26:16.495545: step 173, loss 1.13397, acc 0.515625
2016-12-17T18:26:17.987957: step 174, loss 1.18769, acc 0.546875
2016-12-17T18:26:19.506117: step 175, loss 1.35435, acc 0.515625
2016-12-17T18:26:21.026091: step 176, loss 1.24395, acc 0.609375
2016-12-17T18:26:22.501057: step 177, loss 1.26288, acc 0.546875
2016-12-17T18:26:24.016475: step 178, loss 0.937002, acc 0.671875
2016-12-17T18:26:25.548850: step 179, loss 1.56106, acc 0.453125
2016-12-17T18:26:27.065150: step 180, loss 1.27801, acc 0.546875
2016-12-17T18:26:28.638263: step 181, loss 1.08743, acc 0.5625
2016-12-17T18:26:30.184166: step 182, loss 1.16683, acc 0.609375
2016-12-17T18:26:31.726377: step 183, loss 1.33394, acc 0.609375
2016-12-17T18:26:33.210832: step 184, loss 1.07536, acc 0.5625
2016-12-17T18:26:34.693545: step 185, loss 0.790727, acc 0.625
2016-12-17T18:26:36.207611: step 186, loss 0.903342, acc 0.546875
2016-12-17T18:26:37.712783: step 187, loss 0.848142, acc 0.671875
2016-12-17T18:26:39.228355: step 188, loss 1.20686, acc 0.578125
2016-12-17T18:26:40.763543: step 189, loss 1.1334, acc 0.453125
2016-12-17T18:26:42.265398: step 190, loss 1.01904, acc 0.546875
2016-12-17T18:26:43.790660: step 191, loss 1.28112, acc 0.515625
2016-12-17T18:26:45.303344: step 192, loss 1.52293, acc 0.515625
2016-12-17T18:26:46.807784: step 193, loss 1.36026, acc 0.484375
2016-12-17T18:26:48.309178: step 194, loss 1.2072, acc 0.5
2016-12-17T18:26:49.826933: step 195, loss 1.07483, acc 0.453125
2016-12-17T18:26:51.325837: step 196, loss 1.23277, acc 0.453125
2016-12-17T18:26:52.823945: step 197, loss 1.10999, acc 0.546875
2016-12-17T18:26:54.311745: step 198, loss 0.992881, acc 0.53125
2016-12-17T18:26:55.829069: step 199, loss 1.29639, acc 0.484375
2016-12-17T18:26:57.339900: step 200, loss 1.63922, acc 0.515625

Evaluation:
2016-12-17T18:27:13.262924: step 200, loss 0.797025, acc 0.62767

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1482016882/checkpoints/model-200

2016-12-17T18:27:16.912424: step 201, loss 1.25689, acc 0.53125
2016-12-17T18:27:18.462810: step 202, loss 0.958951, acc 0.59375
2016-12-17T18:27:20.021827: step 203, loss 1.40329, acc 0.46875
2016-12-17T18:27:21.565672: step 204, loss 1.2685, acc 0.578125
2016-12-17T18:27:23.098742: step 205, loss 1.13675, acc 0.484375
2016-12-17T18:27:24.632124: step 206, loss 1.11989, acc 0.515625
2016-12-17T18:27:26.166304: step 207, loss 1.45677, acc 0.5
2016-12-17T18:27:27.719807: step 208, loss 1.12369, acc 0.53125
2016-12-17T18:27:29.291823: step 209, loss 0.995416, acc 0.515625
2016-12-17T18:27:30.832079: step 210, loss 1.16953, acc 0.609375
2016-12-17T18:27:32.393941: step 211, loss 1.13801, acc 0.59375
2016-12-17T18:27:33.991209: step 212, loss 0.902782, acc 0.5625
2016-12-17T18:27:35.521576: step 213, loss 0.856716, acc 0.59375
2016-12-17T18:27:37.074762: step 214, loss 1.05485, acc 0.578125
2016-12-17T18:27:38.611079: step 215, loss 1.26704, acc 0.53125
2016-12-17T18:27:40.167663: step 216, loss 0.957412, acc 0.546875
2016-12-17T18:27:41.735666: step 217, loss 1.13356, acc 0.59375
2016-12-17T18:27:43.291729: step 218, loss 0.893263, acc 0.609375
2016-12-17T18:27:44.815302: step 219, loss 1.33112, acc 0.4375
2016-12-17T18:27:46.346276: step 220, loss 1.07452, acc 0.53125
2016-12-17T18:27:47.857241: step 221, loss 0.891939, acc 0.6875
2016-12-17T18:27:49.390467: step 222, loss 0.978929, acc 0.578125
2016-12-17T18:27:50.912247: step 223, loss 1.19862, acc 0.609375
2016-12-17T18:27:52.434051: step 224, loss 0.784756, acc 0.65625
2016-12-17T18:27:53.967173: step 225, loss 1.1068, acc 0.5
2016-12-17T18:27:55.503415: step 226, loss 0.777366, acc 0.671875
2016-12-17T18:27:56.996902: step 227, loss 1.01497, acc 0.640625
2016-12-17T18:27:58.554164: step 228, loss 1.10837, acc 0.59375
2016-12-17T18:28:00.112450: step 229, loss 1.14651, acc 0.4375
2016-12-17T18:28:01.666197: step 230, loss 0.907558, acc 0.578125
2016-12-17T18:28:03.204064: step 231, loss 1.02725, acc 0.546875
2016-12-17T18:28:04.729593: step 232, loss 1.26213, acc 0.5
2016-12-17T18:28:06.337631: step 233, loss 0.875164, acc 0.609375
2016-12-17T18:28:07.901967: step 234, loss 1.21932, acc 0.546875
2016-12-17T18:28:09.442220: step 235, loss 1.06048, acc 0.578125
2016-12-17T18:28:10.983728: step 236, loss 1.20805, acc 0.53125
2016-12-17T18:28:12.491893: step 237, loss 1.16345, acc 0.4375
2016-12-17T18:28:14.055298: step 238, loss 0.908467, acc 0.59375
2016-12-17T18:28:15.568148: step 239, loss 1.18924, acc 0.453125
2016-12-17T18:28:17.082296: step 240, loss 0.821258, acc 0.59375
2016-12-17T18:28:18.635095: step 241, loss 1.06813, acc 0.5
2016-12-17T18:28:20.139435: step 242, loss 1.1484, acc 0.46875
2016-12-17T18:28:21.673296: step 243, loss 0.971471, acc 0.59375
2016-12-17T18:28:23.196255: step 244, loss 1.0648, acc 0.5
2016-12-17T18:28:24.710385: step 245, loss 0.802134, acc 0.625
2016-12-17T18:28:26.192112: step 246, loss 1.13472, acc 0.5625
2016-12-17T18:28:27.721342: step 247, loss 0.982671, acc 0.640625
2016-12-17T18:28:29.261518: step 248, loss 1.05195, acc 0.46875
2016-12-17T18:28:30.758731: step 249, loss 0.98149, acc 0.625
2016-12-17T18:28:32.263195: step 250, loss 0.983625, acc 0.53125
2016-12-17T18:28:33.796996: step 251, loss 1.29055, acc 0.5625
2016-12-17T18:28:35.296538: step 252, loss 0.891581, acc 0.625
2016-12-17T18:28:36.812751: step 253, loss 0.695035, acc 0.625
2016-12-17T18:28:38.468545: step 254, loss 0.836952, acc 0.578125
2016-12-17T18:28:40.024111: step 255, loss 1.03365, acc 0.578125
2016-12-17T18:28:41.536885: step 256, loss 1.24262, acc 0.453125
2016-12-17T18:28:43.126061: step 257, loss 1.19612, acc 0.4375
2016-12-17T18:28:44.678778: step 258, loss 0.896869, acc 0.5625
2016-12-17T18:28:46.236666: step 259, loss 1.20801, acc 0.515625
2016-12-17T18:28:47.728318: step 260, loss 1.1252, acc 0.5
2016-12-17T18:28:49.262250: step 261, loss 1.06575, acc 0.5
2016-12-17T18:28:50.759724: step 262, loss 1.1222, acc 0.546875
2016-12-17T18:28:52.293613: step 263, loss 0.873955, acc 0.609375
2016-12-17T18:28:53.844547: step 264, loss 0.896448, acc 0.59375
2016-12-17T18:28:55.381813: step 265, loss 0.9063, acc 0.53125
2016-12-17T18:28:56.927329: step 266, loss 1.00685, acc 0.5
2016-12-17T18:28:58.451524: step 267, loss 0.7204, acc 0.59375
2016-12-17T18:28:59.955319: step 268, loss 0.821178, acc 0.625
2016-12-17T18:29:01.509270: step 269, loss 0.930528, acc 0.46875
2016-12-17T18:29:03.040712: step 270, loss 0.90745, acc 0.5
2016-12-17T18:29:04.562959: step 271, loss 1.00423, acc 0.578125
2016-12-17T18:29:06.083817: step 272, loss 0.722092, acc 0.640625
2016-12-17T18:29:07.623972: step 273, loss 0.927383, acc 0.484375
2016-12-17T18:29:09.157435: step 274, loss 0.856033, acc 0.625
2016-12-17T18:29:10.774267: step 275, loss 0.900031, acc 0.5625
2016-12-17T18:29:12.296936: step 276, loss 1.00468, acc 0.609375
2016-12-17T18:29:13.821417: step 277, loss 0.894021, acc 0.6875
2016-12-17T18:29:15.388737: step 278, loss 0.848052, acc 0.640625
2016-12-17T18:29:16.884752: step 279, loss 1.03367, acc 0.546875
2016-12-17T18:29:18.445156: step 280, loss 0.950063, acc 0.546875
2016-12-17T18:29:19.955165: step 281, loss 0.815999, acc 0.65625
2016-12-17T18:29:21.509646: step 282, loss 0.960199, acc 0.59375
2016-12-17T18:29:23.033330: step 283, loss 0.920198, acc 0.53125
2016-12-17T18:29:24.539628: step 284, loss 0.713058, acc 0.640625
2016-12-17T18:29:26.047211: step 285, loss 0.802841, acc 0.53125
2016-12-17T18:29:27.591501: step 286, loss 0.850335, acc 0.5625
2016-12-17T18:29:29.119468: step 287, loss 0.906064, acc 0.546875
2016-12-17T18:29:30.628796: step 288, loss 1.16021, acc 0.5625
2016-12-17T18:29:32.192264: step 289, loss 1.15695, acc 0.453125
2016-12-17T18:29:33.712354: step 290, loss 1.01613, acc 0.515625
2016-12-17T18:29:35.223091: step 291, loss 0.838944, acc 0.640625
2016-12-17T18:29:36.714547: step 292, loss 0.879168, acc 0.578125
2016-12-17T18:29:38.243038: step 293, loss 0.854893, acc 0.59375
2016-12-17T18:29:39.756915: step 294, loss 0.774624, acc 0.625
2016-12-17T18:29:41.308268: step 295, loss 0.808351, acc 0.609375
2016-12-17T18:29:42.883485: step 296, loss 0.820973, acc 0.578125
2016-12-17T18:29:44.440499: step 297, loss 0.977078, acc 0.578125
2016-12-17T18:29:45.976990: step 298, loss 0.757789, acc 0.671875
2016-12-17T18:29:47.533007: step 299, loss 0.862844, acc 0.578125
2016-12-17T18:29:49.070676: step 300, loss 0.906648, acc 0.5625

Evaluation:
2016-12-17T18:30:04.296030: step 300, loss 0.664457, acc 0.618932

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1482016882/checkpoints/model-300

2016-12-17T18:30:07.806398: step 301, loss 0.887575, acc 0.5625
2016-12-17T18:30:09.296882: step 302, loss 0.74323, acc 0.609375
2016-12-17T18:30:10.823775: step 303, loss 0.772291, acc 0.59375
2016-12-17T18:30:12.385536: step 304, loss 0.798936, acc 0.5
2016-12-17T18:30:13.967201: step 305, loss 0.692262, acc 0.703125
2016-12-17T18:30:15.508879: step 306, loss 0.923756, acc 0.609375
2016-12-17T18:30:17.029230: step 307, loss 0.918346, acc 0.625
2016-12-17T18:30:18.555817: step 308, loss 1.16012, acc 0.5
2016-12-17T18:30:20.099290: step 309, loss 1.071, acc 0.484375
2016-12-17T18:30:21.632105: step 310, loss 0.871181, acc 0.640625
2016-12-17T18:30:23.161828: step 311, loss 0.877579, acc 0.53125
2016-12-17T18:30:24.694399: step 312, loss 0.64514, acc 0.609375
2016-12-17T18:30:26.227096: step 313, loss 0.898387, acc 0.46875
2016-12-17T18:30:27.747648: step 314, loss 0.696889, acc 0.65625
2016-12-17T18:30:29.279161: step 315, loss 1.04429, acc 0.5
2016-12-17T18:30:30.821351: step 316, loss 0.800805, acc 0.609375
2016-12-17T18:30:32.353429: step 317, loss 0.697399, acc 0.6875
2016-12-17T18:30:33.857296: step 318, loss 0.868199, acc 0.5
2016-12-17T18:30:35.400361: step 319, loss 0.798669, acc 0.578125
2016-12-17T18:30:36.925919: step 320, loss 0.79028, acc 0.671875
2016-12-17T18:30:38.415899: step 321, loss 1.07316, acc 0.515625
2016-12-17T18:30:39.981047: step 322, loss 0.910594, acc 0.5625
2016-12-17T18:30:41.502866: step 323, loss 0.822813, acc 0.625
2016-12-17T18:30:43.010601: step 324, loss 0.809037, acc 0.53125
2016-12-17T18:30:44.511039: step 325, loss 0.930976, acc 0.46875
2016-12-17T18:30:46.089635: step 326, loss 0.785907, acc 0.609375
2016-12-17T18:30:47.650940: step 327, loss 0.874859, acc 0.53125
2016-12-17T18:30:49.210127: step 328, loss 0.911751, acc 0.515625
2016-12-17T18:30:50.743707: step 329, loss 0.721369, acc 0.640625
2016-12-17T18:30:52.259548: step 330, loss 0.775319, acc 0.609375
2016-12-17T18:30:53.789867: step 331, loss 0.589388, acc 0.6875
2016-12-17T18:30:55.340408: step 332, loss 0.800528, acc 0.625
2016-12-17T18:30:56.866696: step 333, loss 0.818942, acc 0.5625
2016-12-17T18:30:58.368654: step 334, loss 0.96015, acc 0.453125
2016-12-17T18:30:59.943093: step 335, loss 0.969272, acc 0.5
2016-12-17T18:31:01.496488: step 336, loss 0.750762, acc 0.6875
2016-12-17T18:31:03.055852: step 337, loss 0.862132, acc 0.5625
2016-12-17T18:31:04.553073: step 338, loss 0.866518, acc 0.578125
2016-12-17T18:31:06.107447: step 339, loss 0.79161, acc 0.578125
2016-12-17T18:31:07.641088: step 340, loss 0.988969, acc 0.421875
2016-12-17T18:31:09.176515: step 341, loss 0.895242, acc 0.40625
2016-12-17T18:31:10.721016: step 342, loss 0.838768, acc 0.53125
2016-12-17T18:31:12.243867: step 343, loss 0.692391, acc 0.609375
2016-12-17T18:31:13.791556: step 344, loss 0.677185, acc 0.609375
2016-12-17T18:31:15.291503: step 345, loss 0.764922, acc 0.5625
2016-12-17T18:31:16.793724: step 346, loss 0.860895, acc 0.578125
2016-12-17T18:31:18.410093: step 347, loss 0.913366, acc 0.578125
2016-12-17T18:31:19.960527: step 348, loss 0.833883, acc 0.5625
2016-12-17T18:31:21.512069: step 349, loss 0.762982, acc 0.5625
2016-12-17T18:31:23.058692: step 350, loss 0.712069, acc 0.609375
2016-12-17T18:31:24.565469: step 351, loss 0.765156, acc 0.59375
2016-12-17T18:31:26.105458: step 352, loss 0.814421, acc 0.53125
2016-12-17T18:31:27.633919: step 353, loss 0.82661, acc 0.609375
2016-12-17T18:31:29.160658: step 354, loss 0.745372, acc 0.53125
2016-12-17T18:31:30.679607: step 355, loss 0.717774, acc 0.59375
2016-12-17T18:31:32.195266: step 356, loss 0.763219, acc 0.59375
2016-12-17T18:31:33.694599: step 357, loss 0.561919, acc 0.75
2016-12-17T18:31:35.232010: step 358, loss 0.802987, acc 0.640625
2016-12-17T18:31:36.791533: step 359, loss 0.917017, acc 0.578125
2016-12-17T18:31:38.320820: step 360, loss 0.776281, acc 0.640625
2016-12-17T18:31:39.824250: step 361, loss 0.973211, acc 0.53125
2016-12-17T18:31:41.357114: step 362, loss 0.811992, acc 0.59375
2016-12-17T18:31:42.884892: step 363, loss 0.721747, acc 0.59375
2016-12-17T18:31:44.420488: step 364, loss 0.743845, acc 0.546875
2016-12-17T18:31:45.949233: step 365, loss 0.853201, acc 0.484375
2016-12-17T18:31:47.459923: step 366, loss 0.760242, acc 0.5
2016-12-17T18:31:48.939400: step 367, loss 0.763067, acc 0.5625
2016-12-17T18:31:50.570258: step 368, loss 0.704759, acc 0.625
2016-12-17T18:31:52.074796: step 369, loss 0.790486, acc 0.546875
2016-12-17T18:31:53.607721: step 370, loss 0.687346, acc 0.578125
2016-12-17T18:31:55.146381: step 371, loss 0.723735, acc 0.640625
2016-12-17T18:31:56.706094: step 372, loss 0.902489, acc 0.546875
2016-12-17T18:31:58.238902: step 373, loss 0.950911, acc 0.53125
2016-12-17T18:31:59.764467: step 374, loss 0.767968, acc 0.546875
2016-12-17T18:32:01.368243: step 375, loss 0.981443, acc 0.5
2016-12-17T18:32:02.901831: step 376, loss 0.734688, acc 0.5625
2016-12-17T18:32:04.468361: step 377, loss 0.835593, acc 0.5
2016-12-17T18:32:06.042407: step 378, loss 0.828185, acc 0.53125
2016-12-17T18:32:07.621234: step 379, loss 0.858194, acc 0.5
2016-12-17T18:32:09.218523: step 380, loss 0.827706, acc 0.53125
2016-12-17T18:32:10.790553: step 381, loss 0.67777, acc 0.578125
2016-12-17T18:32:12.392064: step 382, loss 0.724955, acc 0.609375
2016-12-17T18:32:13.937926: step 383, loss 0.856533, acc 0.546875
2016-12-17T18:32:15.488353: step 384, loss 0.758198, acc 0.640625
2016-12-17T18:32:17.061125: step 385, loss 0.599835, acc 0.703125
2016-12-17T18:32:18.625227: step 386, loss 0.793762, acc 0.5625
2016-12-17T18:32:20.192569: step 387, loss 0.742622, acc 0.65625
2016-12-17T18:32:21.823381: step 388, loss 0.846429, acc 0.578125
2016-12-17T18:32:23.409352: step 389, loss 0.766019, acc 0.578125
2016-12-17T18:32:24.951246: step 390, loss 0.844785, acc 0.484375
2016-12-17T18:32:26.499085: step 391, loss 0.752135, acc 0.546875
2016-12-17T18:32:28.043613: step 392, loss 0.82928, acc 0.484375
2016-12-17T18:32:29.568065: step 393, loss 0.874899, acc 0.53125
2016-12-17T18:32:31.081603: step 394, loss 0.890772, acc 0.453125
2016-12-17T18:32:32.586191: step 395, loss 0.696945, acc 0.625
2016-12-17T18:32:34.126821: step 396, loss 0.78767, acc 0.53125
2016-12-17T18:32:35.657309: step 397, loss 0.83635, acc 0.53125
2016-12-17T18:32:37.178051: step 398, loss 0.824935, acc 0.546875
2016-12-17T18:32:38.725347: step 399, loss 0.541348, acc 0.765625
2016-12-17T18:32:40.282195: step 400, loss 0.819497, acc 0.546875

Evaluation:
2016-12-17T18:32:54.777000: step 400, loss 0.666312, acc 0.629126

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1482016882/checkpoints/model-400

2016-12-17T18:32:58.402865: step 401, loss 0.752607, acc 0.609375
2016-12-17T18:32:59.963631: step 402, loss 0.865097, acc 0.5
2016-12-17T18:33:01.440924: step 403, loss 0.672241, acc 0.59375
2016-12-17T18:33:02.978887: step 404, loss 0.760281, acc 0.53125
2016-12-17T18:33:04.475530: step 405, loss 0.770222, acc 0.59375
2016-12-17T18:33:05.973893: step 406, loss 0.689896, acc 0.609375
2016-12-17T18:33:07.492029: step 407, loss 0.843406, acc 0.484375
2016-12-17T18:33:09.011000: step 408, loss 0.746737, acc 0.578125
2016-12-17T18:33:10.536610: step 409, loss 0.628318, acc 0.71875
2016-12-17T18:33:12.016764: step 410, loss 0.64679, acc 0.6875
2016-12-17T18:33:13.533886: step 411, loss 0.800635, acc 0.53125
2016-12-17T18:33:15.065416: step 412, loss 0.797743, acc 0.5625
2016-12-17T18:33:16.625803: step 413, loss 1.04732, acc 0.53125
2016-12-17T18:33:18.145714: step 414, loss 0.885742, acc 0.578125
2016-12-17T18:33:19.663376: step 415, loss 0.797651, acc 0.59375
2016-12-17T18:33:21.201108: step 416, loss 0.794135, acc 0.546875
2016-12-17T18:33:22.723380: step 417, loss 0.73802, acc 0.53125
2016-12-17T18:33:24.256143: step 418, loss 0.796911, acc 0.53125
2016-12-17T18:33:25.750953: step 419, loss 0.771635, acc 0.5625
2016-12-17T18:33:27.299690: step 420, loss 0.781319, acc 0.546875
2016-12-17T18:33:28.837130: step 421, loss 0.843428, acc 0.4375
2016-12-17T18:33:30.305235: step 422, loss 0.680597, acc 0.5625
2016-12-17T18:33:31.826213: step 423, loss 0.748322, acc 0.546875
2016-12-17T18:33:33.314881: step 424, loss 0.696788, acc 0.609375
2016-12-17T18:33:34.815460: step 425, loss 0.956427, acc 0.5
2016-12-17T18:33:36.342604: step 426, loss 0.764042, acc 0.625
2016-12-17T18:33:37.851865: step 427, loss 0.855048, acc 0.578125
2016-12-17T18:33:39.367151: step 428, loss 0.760866, acc 0.578125
2016-12-17T18:33:40.896489: step 429, loss 0.762676, acc 0.59375
2016-12-17T18:33:42.421567: step 430, loss 0.754699, acc 0.578125
2016-12-17T18:33:43.949747: step 431, loss 0.691505, acc 0.59375
2016-12-17T18:33:45.425986: step 432, loss 0.681196, acc 0.546875
2016-12-17T18:33:46.946087: step 433, loss 0.7334, acc 0.546875
2016-12-17T18:33:48.447101: step 434, loss 0.679668, acc 0.609375
2016-12-17T18:33:49.914828: step 435, loss 0.694431, acc 0.5625
2016-12-17T18:33:51.419071: step 436, loss 0.818448, acc 0.546875
2016-12-17T18:33:52.922079: step 437, loss 0.623973, acc 0.6875
2016-12-17T18:33:54.414986: step 438, loss 0.774906, acc 0.578125
2016-12-17T18:33:55.896686: step 439, loss 0.873788, acc 0.59375
2016-12-17T18:33:57.395446: step 440, loss 0.66944, acc 0.71875
2016-12-17T18:33:58.929917: step 441, loss 0.745045, acc 0.578125
2016-12-17T18:34:00.489362: step 442, loss 0.727182, acc 0.59375
2016-12-17T18:34:02.039179: step 443, loss 0.76756, acc 0.578125
2016-12-17T18:34:03.542105: step 444, loss 0.726722, acc 0.5
2016-12-17T18:34:05.072638: step 445, loss 0.824295, acc 0.421875
2016-12-17T18:34:06.611774: step 446, loss 0.654172, acc 0.546875
2016-12-17T18:34:08.123516: step 447, loss 0.677303, acc 0.6875
2016-12-17T18:34:09.635488: step 448, loss 0.627109, acc 0.671875
2016-12-17T18:34:11.165400: step 449, loss 0.649917, acc 0.578125
2016-12-17T18:34:12.706252: step 450, loss 0.755915, acc 0.59375
2016-12-17T18:34:14.225437: step 451, loss 0.666679, acc 0.703125
2016-12-17T18:34:15.738800: step 452, loss 0.74095, acc 0.59375
2016-12-17T18:34:17.252516: step 453, loss 0.782371, acc 0.53125
2016-12-17T18:34:18.778854: step 454, loss 0.689698, acc 0.609375
2016-12-17T18:34:20.241918: step 455, loss 0.720667, acc 0.546875
2016-12-17T18:34:21.793018: step 456, loss 0.751639, acc 0.484375
2016-12-17T18:34:23.341798: step 457, loss 0.831169, acc 0.46875
2016-12-17T18:34:24.874228: step 458, loss 0.6944, acc 0.59375
2016-12-17T18:34:26.419333: step 459, loss 0.673566, acc 0.6875
2016-12-17T18:34:27.950691: step 460, loss 0.806159, acc 0.59375
2016-12-17T18:34:29.466025: step 461, loss 0.76226, acc 0.59375
2016-12-17T18:34:31.026267: step 462, loss 0.720306, acc 0.59375
2016-12-17T18:34:32.583842: step 463, loss 0.644151, acc 0.640625
2016-12-17T18:34:34.145240: step 464, loss 0.657826, acc 0.625
2016-12-17T18:34:35.639476: step 465, loss 0.640628, acc 0.625
2016-12-17T18:34:37.137730: step 466, loss 0.693233, acc 0.546875
2016-12-17T18:34:38.656220: step 467, loss 0.623588, acc 0.640625
2016-12-17T18:34:40.205239: step 468, loss 0.71646, acc 0.578125
2016-12-17T18:34:41.759892: step 469, loss 0.710828, acc 0.546875
2016-12-17T18:34:43.307129: step 470, loss 0.687394, acc 0.625
2016-12-17T18:34:44.812635: step 471, loss 0.696931, acc 0.59375
2016-12-17T18:34:46.339386: step 472, loss 0.607128, acc 0.6875
2016-12-17T18:34:47.869443: step 473, loss 0.734043, acc 0.546875
2016-12-17T18:34:49.378715: step 474, loss 0.583079, acc 0.703125
2016-12-17T18:34:50.937102: step 475, loss 0.726867, acc 0.59375
2016-12-17T18:34:52.467335: step 476, loss 0.761428, acc 0.53125
2016-12-17T18:34:53.980529: step 477, loss 0.68538, acc 0.546875
2016-12-17T18:34:55.499090: step 478, loss 0.618315, acc 0.65625
2016-12-17T18:34:57.033676: step 479, loss 0.761517, acc 0.484375
2016-12-17T18:34:58.551791: step 480, loss 0.653016, acc 0.6875
2016-12-17T18:35:00.082443: step 481, loss 0.661656, acc 0.625
2016-12-17T18:35:01.591248: step 482, loss 0.740476, acc 0.609375
2016-12-17T18:35:03.173707: step 483, loss 0.603438, acc 0.671875
2016-12-17T18:35:04.688475: step 484, loss 0.620405, acc 0.625
2016-12-17T18:35:06.229177: step 485, loss 0.748744, acc 0.484375
2016-12-17T18:35:07.751763: step 486, loss 0.641635, acc 0.640625
2016-12-17T18:35:09.275004: step 487, loss 0.61421, acc 0.625
2016-12-17T18:35:10.825065: step 488, loss 0.65708, acc 0.625
2016-12-17T18:35:12.354201: step 489, loss 0.639386, acc 0.625
2016-12-17T18:35:13.831380: step 490, loss 0.666495, acc 0.609375
2016-12-17T18:35:15.341359: step 491, loss 0.743176, acc 0.53125
2016-12-17T18:35:16.826576: step 492, loss 0.707835, acc 0.5625
2016-12-17T18:35:18.372934: step 493, loss 0.747647, acc 0.5625
2016-12-17T18:35:19.858362: step 494, loss 0.837473, acc 0.40625
2016-12-17T18:35:21.402064: step 495, loss 0.786892, acc 0.515625
2016-12-17T18:35:22.917381: step 496, loss 0.621415, acc 0.671875
2016-12-17T18:35:24.424158: step 497, loss 0.717416, acc 0.484375
2016-12-17T18:35:25.943714: step 498, loss 0.62905, acc 0.65625
2016-12-17T18:35:27.457519: step 499, loss 0.717232, acc 0.546875
2016-12-17T18:35:28.965145: step 500, loss 0.7019, acc 0.578125

Evaluation:
2016-12-17T18:35:43.622688: step 500, loss 0.646562, acc 0.63835

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1482016882/checkpoints/model-500

2016-12-17T18:35:47.404268: step 501, loss 0.600254, acc 0.6875
2016-12-17T18:35:48.926258: step 502, loss 0.691975, acc 0.578125
2016-12-17T18:35:50.422710: step 503, loss 0.82951, acc 0.5625
2016-12-17T18:35:51.952908: step 504, loss 0.599933, acc 0.6875
2016-12-17T18:35:53.465268: step 505, loss 0.696587, acc 0.59375
2016-12-17T18:35:54.981029: step 506, loss 0.63545, acc 0.71875
2016-12-17T18:35:56.463926: step 507, loss 0.724047, acc 0.609375
2016-12-17T18:35:57.964545: step 508, loss 0.639736, acc 0.65625
2016-12-17T18:35:59.529199: step 509, loss 0.625128, acc 0.59375
2016-12-17T18:36:01.037538: step 510, loss 0.704702, acc 0.578125
2016-12-17T18:36:02.559643: step 511, loss 0.739838, acc 0.59375
2016-12-17T18:36:04.097242: step 512, loss 0.774302, acc 0.5625
2016-12-17T18:36:05.612655: step 513, loss 0.614969, acc 0.6875
2016-12-17T18:36:07.113471: step 514, loss 0.747427, acc 0.5625
2016-12-17T18:36:08.646163: step 515, loss 0.692298, acc 0.640625
2016-12-17T18:36:10.229268: step 516, loss 0.66428, acc 0.578125
2016-12-17T18:36:11.748818: step 517, loss 0.675993, acc 0.59375
2016-12-17T18:36:13.278683: step 518, loss 0.6734, acc 0.65625
2016-12-17T18:36:14.818652: step 519, loss 0.707042, acc 0.5
2016-12-17T18:36:16.320641: step 520, loss 0.644601, acc 0.65625
2016-12-17T18:36:17.839074: step 521, loss 0.790184, acc 0.46875
2016-12-17T18:36:19.362717: step 522, loss 0.744407, acc 0.609375
2016-12-17T18:36:20.870448: step 523, loss 0.645585, acc 0.65625
2016-12-17T18:36:22.382480: step 524, loss 0.675158, acc 0.5625
2016-12-17T18:36:23.882079: step 525, loss 0.747468, acc 0.5
2016-12-17T18:36:25.392077: step 526, loss 0.605111, acc 0.671875
2016-12-17T18:36:26.941603: step 527, loss 0.660352, acc 0.609375
2016-12-17T18:36:28.452559: step 528, loss 0.711251, acc 0.53125
2016-12-17T18:36:29.969416: step 529, loss 0.760493, acc 0.53125
2016-12-17T18:36:31.513744: step 530, loss 0.728347, acc 0.46875
2016-12-17T18:36:33.045312: step 531, loss 0.733135, acc 0.546875
2016-12-17T18:36:34.560265: step 532, loss 0.79083, acc 0.4375
2016-12-17T18:36:36.069925: step 533, loss 0.683706, acc 0.625
2016-12-17T18:36:37.549469: step 534, loss 0.735896, acc 0.5
2016-12-17T18:36:39.071951: step 535, loss 0.676794, acc 0.578125
2016-12-17T18:36:40.585092: step 536, loss 0.665391, acc 0.5625
2016-12-17T18:36:42.189596: step 537, loss 0.655595, acc 0.609375
2016-12-17T18:36:43.722649: step 538, loss 0.659736, acc 0.59375
2016-12-17T18:36:45.252552: step 539, loss 0.687328, acc 0.59375
2016-12-17T18:36:46.793731: step 540, loss 0.651435, acc 0.640625
2016-12-17T18:36:48.291704: step 541, loss 0.646095, acc 0.625
2016-12-17T18:36:49.811334: step 542, loss 0.591912, acc 0.71875
2016-12-17T18:36:51.346564: step 543, loss 0.743376, acc 0.578125
2016-12-17T18:36:52.876948: step 544, loss 0.605987, acc 0.6875
2016-12-17T18:36:54.416674: step 545, loss 0.649737, acc 0.640625
2016-12-17T18:36:55.944962: step 546, loss 0.626336, acc 0.609375
2016-12-17T18:36:57.482334: step 547, loss 0.619887, acc 0.625
2016-12-17T18:36:58.974600: step 548, loss 0.619963, acc 0.6875
2016-12-17T18:37:00.528188: step 549, loss 0.70472, acc 0.5625
2016-12-17T18:37:02.095083: step 550, loss 0.603398, acc 0.65625
2016-12-17T18:37:03.685108: step 551, loss 0.707802, acc 0.546875
2016-12-17T18:37:05.233149: step 552, loss 0.817549, acc 0.5
2016-12-17T18:37:06.805571: step 553, loss 0.779827, acc 0.53125
2016-12-17T18:37:08.417719: step 554, loss 0.665805, acc 0.609375
2016-12-17T18:37:09.973713: step 555, loss 0.749837, acc 0.515625
2016-12-17T18:37:11.547510: step 556, loss 0.728492, acc 0.421875
2016-12-17T18:37:13.194493: step 557, loss 0.694332, acc 0.59375
2016-12-17T18:37:14.833065: step 558, loss 0.650502, acc 0.65625
2016-12-17T18:37:16.475276: step 559, loss 0.709865, acc 0.578125
2016-12-17T18:37:18.039380: step 560, loss 0.740037, acc 0.59375
2016-12-17T18:37:19.633225: step 561, loss 0.612237, acc 0.671875
2016-12-17T18:37:21.231565: step 562, loss 0.630129, acc 0.75
2016-12-17T18:37:22.794034: step 563, loss 0.642363, acc 0.734375
2016-12-17T18:37:24.306466: step 564, loss 0.65284, acc 0.625
2016-12-17T18:37:25.850038: step 565, loss 0.654501, acc 0.671875
2016-12-17T18:37:27.403689: step 566, loss 0.718961, acc 0.5625
2016-12-17T18:37:28.898390: step 567, loss 0.650335, acc 0.640625
2016-12-17T18:37:30.423042: step 568, loss 0.816238, acc 0.515625
2016-12-17T18:37:31.954756: step 569, loss 0.667439, acc 0.5625
2016-12-17T18:37:33.483889: step 570, loss 0.754975, acc 0.484375
2016-12-17T18:37:34.991734: step 571, loss 0.717928, acc 0.546875
2016-12-17T18:37:36.538031: step 572, loss 0.691894, acc 0.5625
2016-12-17T18:37:38.074104: step 573, loss 0.775215, acc 0.484375
2016-12-17T18:37:39.564585: step 574, loss 0.710474, acc 0.59375
2016-12-17T18:37:41.094571: step 575, loss 0.620027, acc 0.625
2016-12-17T18:37:42.615326: step 576, loss 0.77367, acc 0.546875
2016-12-17T18:37:44.140573: step 577, loss 0.622246, acc 0.6875
2016-12-17T18:37:45.802907: step 578, loss 0.779868, acc 0.578125
2016-12-17T18:37:47.344843: step 579, loss 0.82718, acc 0.515625
2016-12-17T18:37:48.878013: step 580, loss 0.711401, acc 0.5625
2016-12-17T18:37:50.443354: step 581, loss 0.692123, acc 0.578125
2016-12-17T18:37:51.960940: step 582, loss 0.65406, acc 0.671875
2016-12-17T18:37:53.496770: step 583, loss 0.733902, acc 0.546875
2016-12-17T18:37:55.047160: step 584, loss 0.656779, acc 0.59375
2016-12-17T18:37:56.596354: step 585, loss 0.662034, acc 0.609375
2016-12-17T18:37:58.139802: step 586, loss 0.648088, acc 0.609375
2016-12-17T18:37:59.661381: step 587, loss 0.656496, acc 0.640625
2016-12-17T18:38:01.184933: step 588, loss 0.796353, acc 0.5625
2016-12-17T18:38:02.678868: step 589, loss 0.661984, acc 0.59375
2016-12-17T18:38:04.220266: step 590, loss 0.680113, acc 0.578125
2016-12-17T18:38:05.721799: step 591, loss 0.741146, acc 0.59375
2016-12-17T18:38:07.249617: step 592, loss 0.631527, acc 0.640625
2016-12-17T18:38:08.790537: step 593, loss 0.687846, acc 0.515625
2016-12-17T18:38:10.310013: step 594, loss 0.676633, acc 0.5625
2016-12-17T18:38:11.844426: step 595, loss 0.72212, acc 0.515625
2016-12-17T18:38:13.391890: step 596, loss 0.673962, acc 0.625
2016-12-17T18:38:14.919316: step 597, loss 0.697241, acc 0.59375
2016-12-17T18:38:16.437612: step 598, loss 0.715616, acc 0.515625
2016-12-17T18:38:18.084984: step 599, loss 0.637569, acc 0.609375
2016-12-17T18:38:19.618500: step 600, loss 0.656383, acc 0.640625

Evaluation:
2016-12-17T18:38:33.278203: step 600, loss 0.644997, acc 0.631553

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1482016882/checkpoints/model-600

2016-12-17T18:38:36.834872: step 601, loss 0.598979, acc 0.671875
2016-12-17T18:38:38.327275: step 602, loss 0.655338, acc 0.703125
2016-12-17T18:38:39.832812: step 603, loss 0.584142, acc 0.6875
2016-12-17T18:38:41.360057: step 604, loss 0.751674, acc 0.515625
2016-12-17T18:38:42.887430: step 605, loss 0.625988, acc 0.734375
2016-12-17T18:38:44.434022: step 606, loss 0.708871, acc 0.546875
2016-12-17T18:38:45.905338: step 607, loss 0.603296, acc 0.65625
2016-12-17T18:38:47.433034: step 608, loss 0.744085, acc 0.546875
2016-12-17T18:38:49.004281: step 609, loss 0.631238, acc 0.671875
2016-12-17T18:38:50.583766: step 610, loss 0.718696, acc 0.484375
2016-12-17T18:38:52.116673: step 611, loss 0.68062, acc 0.609375
2016-12-17T18:38:53.640438: step 612, loss 0.601095, acc 0.71875
2016-12-17T18:38:55.164163: step 613, loss 0.669064, acc 0.609375
2016-12-17T18:38:56.681379: step 614, loss 0.587908, acc 0.703125
2016-12-17T18:38:58.196736: step 615, loss 0.923002, acc 0.4375
2016-12-17T18:38:59.736939: step 616, loss 0.68196, acc 0.625
2016-12-17T18:39:01.264213: step 617, loss 0.695807, acc 0.59375
2016-12-17T18:39:02.778073: step 618, loss 0.650536, acc 0.640625
2016-12-17T18:39:04.276299: step 619, loss 0.625411, acc 0.609375
2016-12-17T18:39:05.758075: step 620, loss 0.720041, acc 0.53125
2016-12-17T18:39:07.299183: step 621, loss 0.650476, acc 0.625
2016-12-17T18:39:08.866302: step 622, loss 0.608051, acc 0.6875
2016-12-17T18:39:10.356621: step 623, loss 0.547677, acc 0.71875
2016-12-17T18:39:11.861772: step 624, loss 0.696667, acc 0.609375
2016-12-17T18:39:13.394874: step 625, loss 0.70603, acc 0.59375
2016-12-17T18:39:14.888206: step 626, loss 0.584678, acc 0.703125
2016-12-17T18:39:16.393039: step 627, loss 0.624173, acc 0.625
2016-12-17T18:39:17.911380: step 628, loss 0.609793, acc 0.65625
2016-12-17T18:39:19.412389: step 629, loss 0.725806, acc 0.609375
2016-12-17T18:39:20.926018: step 630, loss 0.652241, acc 0.609375
2016-12-17T18:39:22.525005: step 631, loss 0.660831, acc 0.59375
2016-12-17T18:39:24.043854: step 632, loss 0.753283, acc 0.453125
2016-12-17T18:39:25.553121: step 633, loss 0.756547, acc 0.46875
2016-12-17T18:39:27.071062: step 634, loss 0.703783, acc 0.546875
2016-12-17T18:39:28.584355: step 635, loss 0.739929, acc 0.515625
2016-12-17T18:39:30.091639: step 636, loss 0.650563, acc 0.59375
2016-12-17T18:39:31.604974: step 637, loss 0.640442, acc 0.578125
2016-12-17T18:39:33.133817: step 638, loss 0.634904, acc 0.671875
2016-12-17T18:39:34.624137: step 639, loss 0.639908, acc 0.65625
2016-12-17T18:39:36.157157: step 640, loss 0.798477, acc 0.515625
2016-12-17T18:39:37.685813: step 641, loss 0.651163, acc 0.671875
2016-12-17T18:39:39.254630: step 642, loss 0.647579, acc 0.640625
2016-12-17T18:39:40.761711: step 643, loss 0.686338, acc 0.5625
2016-12-17T18:39:42.274897: step 644, loss 0.657951, acc 0.59375
2016-12-17T18:39:43.804420: step 645, loss 0.677702, acc 0.59375
2016-12-17T18:39:45.314737: step 646, loss 0.640146, acc 0.65625
2016-12-17T18:39:46.851193: step 647, loss 0.654959, acc 0.609375
2016-12-17T18:39:48.386811: step 648, loss 0.609755, acc 0.703125
2016-12-17T18:39:49.920421: step 649, loss 0.733035, acc 0.59375
2016-12-17T18:39:51.457155: step 650, loss 0.656306, acc 0.640625
2016-12-17T18:39:52.963208: step 651, loss 0.736293, acc 0.609375
2016-12-17T18:39:54.547777: step 652, loss 0.737813, acc 0.5625
2016-12-17T18:39:56.089508: step 653, loss 0.685384, acc 0.578125
2016-12-17T18:39:57.628124: step 654, loss 0.670424, acc 0.59375
2016-12-17T18:39:59.135830: step 655, loss 0.739388, acc 0.515625
2016-12-17T18:40:00.648850: step 656, loss 0.671871, acc 0.609375
2016-12-17T18:40:02.190901: step 657, loss 0.719951, acc 0.53125
2016-12-17T18:40:03.682663: step 658, loss 0.667692, acc 0.625
2016-12-17T18:40:05.199350: step 659, loss 0.664973, acc 0.625
2016-12-17T18:40:06.730346: step 660, loss 0.601444, acc 0.6875
2016-12-17T18:40:08.270135: step 661, loss 0.637282, acc 0.71875
2016-12-17T18:40:09.770446: step 662, loss 0.714666, acc 0.59375
2016-12-17T18:40:11.267221: step 663, loss 0.695851, acc 0.59375
2016-12-17T18:40:12.758316: step 664, loss 0.630309, acc 0.6875
2016-12-17T18:40:14.290273: step 665, loss 0.676591, acc 0.671875
2016-12-17T18:40:15.834522: step 666, loss 0.637823, acc 0.625
2016-12-17T18:40:17.370121: step 667, loss 0.682203, acc 0.578125
2016-12-17T18:40:18.930130: step 668, loss 0.60795, acc 0.671875
2016-12-17T18:40:20.420274: step 669, loss 0.657817, acc 0.6875
2016-12-17T18:40:21.988084: step 670, loss 0.650652, acc 0.75
2016-12-17T18:40:23.524279: step 671, loss 0.679452, acc 0.578125
2016-12-17T18:40:25.051164: step 672, loss 0.604633, acc 0.65625
2016-12-17T18:40:26.697817: step 673, loss 0.646326, acc 0.65625
2016-12-17T18:40:28.223736: step 674, loss 0.708765, acc 0.53125
2016-12-17T18:40:29.737569: step 675, loss 0.650146, acc 0.671875
2016-12-17T18:40:31.276188: step 676, loss 0.622398, acc 0.640625
2016-12-17T18:40:32.833108: step 677, loss 0.610037, acc 0.703125
2016-12-17T18:40:34.354265: step 678, loss 0.722427, acc 0.546875
2016-12-17T18:40:35.875466: step 679, loss 0.734435, acc 0.5
2016-12-17T18:40:37.395062: step 680, loss 0.687504, acc 0.578125
2016-12-17T18:40:38.927333: step 681, loss 0.641538, acc 0.625
2016-12-17T18:40:40.447852: step 682, loss 0.708417, acc 0.546875
2016-12-17T18:40:41.974530: step 683, loss 0.816803, acc 0.4375
2016-12-17T18:40:43.538941: step 684, loss 0.684968, acc 0.640625
2016-12-17T18:40:45.067066: step 685, loss 0.707069, acc 0.5
2016-12-17T18:40:46.554365: step 686, loss 0.687911, acc 0.59375
2016-12-17T18:40:48.074680: step 687, loss 0.652972, acc 0.640625
2016-12-17T18:40:49.596826: step 688, loss 0.72008, acc 0.5
2016-12-17T18:40:51.180436: step 689, loss 0.65471, acc 0.609375
2016-12-17T18:40:52.729481: step 690, loss 0.637181, acc 0.65625
2016-12-17T18:40:54.226239: step 691, loss 0.657045, acc 0.609375
2016-12-17T18:40:55.738678: step 692, loss 0.674358, acc 0.578125
2016-12-17T18:40:57.282406: step 693, loss 0.675795, acc 0.578125
2016-12-17T18:40:58.911431: step 694, loss 0.651973, acc 0.6875
2016-12-17T18:41:00.480347: step 695, loss 0.650932, acc 0.640625
2016-12-17T18:41:02.026669: step 696, loss 0.696846, acc 0.6875
2016-12-17T18:41:03.579662: step 697, loss 0.621141, acc 0.609375
2016-12-17T18:41:05.119467: step 698, loss 0.755216, acc 0.59375
2016-12-17T18:41:06.651238: step 699, loss 0.652905, acc 0.625
2016-12-17T18:41:08.172962: step 700, loss 0.697621, acc 0.5625

Evaluation:
2016-12-17T18:41:21.895087: step 700, loss 0.642125, acc 0.635922

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1482016882/checkpoints/model-700

2016-12-17T18:41:25.405795: step 701, loss 0.672554, acc 0.546875
2016-12-17T18:41:26.947919: step 702, loss 0.709611, acc 0.5
2016-12-17T18:41:28.483185: step 703, loss 0.682627, acc 0.609375
2016-12-17T18:41:30.097503: step 704, loss 0.710475, acc 0.546875
2016-12-17T18:41:31.694097: step 705, loss 0.706389, acc 0.484375
2016-12-17T18:41:33.202152: step 706, loss 0.683277, acc 0.53125
2016-12-17T18:41:34.733087: step 707, loss 0.685877, acc 0.546875
2016-12-17T18:41:36.280501: step 708, loss 0.60463, acc 0.734375
2016-12-17T18:41:37.825146: step 709, loss 0.814609, acc 0.5
2016-12-17T18:41:39.333333: step 710, loss 0.678081, acc 0.640625
2016-12-17T18:41:40.827322: step 711, loss 0.771731, acc 0.53125
2016-12-17T18:41:42.361951: step 712, loss 0.644053, acc 0.640625
2016-12-17T18:41:43.856032: step 713, loss 0.814264, acc 0.390625
2016-12-17T18:41:45.334284: step 714, loss 0.653225, acc 0.640625
2016-12-17T18:41:46.852389: step 715, loss 0.702133, acc 0.5625
2016-12-17T18:41:48.389850: step 716, loss 0.799112, acc 0.359375
2016-12-17T18:41:49.880848: step 717, loss 0.732312, acc 0.4375
2016-12-17T18:41:51.373517: step 718, loss 0.738437, acc 0.5
2016-12-17T18:41:52.935323: step 719, loss 0.691697, acc 0.609375
2016-12-17T18:41:54.491511: step 720, loss 0.741276, acc 0.546875
2016-12-17T18:41:55.987113: step 721, loss 0.713859, acc 0.609375
2016-12-17T18:41:57.509195: step 722, loss 0.763316, acc 0.5625
2016-12-17T18:41:59.017595: step 723, loss 0.659163, acc 0.609375
2016-12-17T18:42:00.553175: step 724, loss 0.700716, acc 0.640625
2016-12-17T18:42:02.134860: step 725, loss 0.687851, acc 0.65625
2016-12-17T18:42:03.794297: step 726, loss 0.731781, acc 0.421875
2016-12-17T18:42:05.365880: step 727, loss 0.626454, acc 0.65625
2016-12-17T18:42:06.943221: step 728, loss 0.701639, acc 0.546875
2016-12-17T18:42:08.536880: step 729, loss 0.658232, acc 0.578125
2016-12-17T18:42:10.090367: step 730, loss 0.734242, acc 0.5
2016-12-17T18:42:11.666105: step 731, loss 0.647343, acc 0.640625
2016-12-17T18:42:13.231693: step 732, loss 0.638101, acc 0.65625
2016-12-17T18:42:14.819125: step 733, loss 0.743333, acc 0.578125
2016-12-17T18:42:16.395936: step 734, loss 0.75269, acc 0.578125
2016-12-17T18:42:17.934150: step 735, loss 0.649764, acc 0.640625
2016-12-17T18:42:19.461235: step 736, loss 0.696919, acc 0.625
2016-12-17T18:42:21.012439: step 737, loss 0.674675, acc 0.640625
2016-12-17T18:42:22.571405: step 738, loss 0.608035, acc 0.703125
2016-12-17T18:42:24.162791: step 739, loss 0.659317, acc 0.65625
2016-12-17T18:42:25.675638: step 740, loss 0.717732, acc 0.5
2016-12-17T18:42:27.197062: step 741, loss 0.709259, acc 0.59375
2016-12-17T18:42:28.696596: step 742, loss 0.681489, acc 0.515625
2016-12-17T18:42:30.212080: step 743, loss 0.659833, acc 0.640625
2016-12-17T18:42:31.726140: step 744, loss 0.704588, acc 0.625
2016-12-17T18:42:33.209431: step 745, loss 0.612066, acc 0.6875
2016-12-17T18:42:34.754130: step 746, loss 0.709209, acc 0.625
2016-12-17T18:42:36.344052: step 747, loss 0.656473, acc 0.625
2016-12-17T18:42:37.897736: step 748, loss 0.582338, acc 0.734375
2016-12-17T18:42:39.423277: step 749, loss 0.693718, acc 0.609375
2016-12-17T18:42:40.925574: step 750, loss 0.728999, acc 0.5625
2016-12-17T18:42:42.421598: step 751, loss 0.649731, acc 0.640625
2016-12-17T18:42:43.958795: step 752, loss 0.661331, acc 0.59375
2016-12-17T18:42:45.441708: step 753, loss 0.714231, acc 0.5
2016-12-17T18:42:46.966457: step 754, loss 0.640354, acc 0.609375
2016-12-17T18:42:48.509702: step 755, loss 0.645717, acc 0.625
2016-12-17T18:42:50.019239: step 756, loss 0.609087, acc 0.6875
2016-12-17T18:42:51.561817: step 757, loss 0.617717, acc 0.625
2016-12-17T18:42:53.077644: step 758, loss 0.717029, acc 0.578125
2016-12-17T18:42:54.604396: step 759, loss 0.632355, acc 0.625
2016-12-17T18:42:56.145034: step 760, loss 0.623208, acc 0.703125
2016-12-17T18:42:57.633199: step 761, loss 0.706814, acc 0.640625
2016-12-17T18:42:59.163230: step 762, loss 0.652706, acc 0.703125
2016-12-17T18:43:00.722021: step 763, loss 0.758305, acc 0.515625
2016-12-17T18:43:02.254569: step 764, loss 0.628126, acc 0.640625
2016-12-17T18:43:03.764074: step 765, loss 0.668257, acc 0.5
2016-12-17T18:43:05.265065: step 766, loss 0.662625, acc 0.53125
2016-12-17T18:43:06.823372: step 767, loss 0.703904, acc 0.546875
2016-12-17T18:43:08.391069: step 768, loss 0.726633, acc 0.5
2016-12-17T18:43:09.955871: step 769, loss 0.697756, acc 0.546875
2016-12-17T18:43:11.532196: step 770, loss 0.744881, acc 0.46875
2016-12-17T18:43:13.062035: step 771, loss 0.726564, acc 0.546875
2016-12-17T18:43:14.580841: step 772, loss 0.723467, acc 0.578125
2016-12-17T18:43:16.122620: step 773, loss 0.721474, acc 0.53125
2016-12-17T18:43:17.666720: step 774, loss 0.68801, acc 0.5625
2016-12-17T18:43:19.203222: step 775, loss 0.62892, acc 0.65625
2016-12-17T18:43:20.718489: step 776, loss 0.619022, acc 0.671875
2016-12-17T18:43:22.281681: step 777, loss 0.653513, acc 0.625
2016-12-17T18:43:23.819220: step 778, loss 0.663009, acc 0.5625
2016-12-17T18:43:25.349747: step 779, loss 0.684174, acc 0.578125
2016-12-17T18:43:26.896475: step 780, loss 0.645163, acc 0.640625
2016-12-17T18:43:28.415384: step 781, loss 0.715062, acc 0.578125
2016-12-17T18:43:29.961620: step 782, loss 0.672784, acc 0.59375
2016-12-17T18:43:31.532760: step 783, loss 0.766124, acc 0.5625
2016-12-17T18:43:33.044639: step 784, loss 0.682789, acc 0.609375
2016-12-17T18:43:34.567342: step 785, loss 0.720168, acc 0.53125
2016-12-17T18:43:36.073155: step 786, loss 0.66164, acc 0.609375
2016-12-17T18:43:37.574050: step 787, loss 0.703308, acc 0.53125
2016-12-17T18:43:39.167935: step 788, loss 0.668244, acc 0.578125
2016-12-17T18:43:40.735731: step 789, loss 0.684987, acc 0.515625
2016-12-17T18:43:42.300733: step 790, loss 0.779486, acc 0.5
2016-12-17T18:43:43.847603: step 791, loss 0.536388, acc 0.765625
2016-12-17T18:43:45.376465: step 792, loss 0.74323, acc 0.640625
2016-12-17T18:43:46.917335: step 793, loss 0.636266, acc 0.65625
2016-12-17T18:43:48.461417: step 794, loss 0.731083, acc 0.5625
2016-12-17T18:43:49.959734: step 795, loss 0.79877, acc 0.46875
2016-12-17T18:43:51.523722: step 796, loss 0.717458, acc 0.546875
2016-12-17T18:43:53.035822: step 797, loss 0.716866, acc 0.453125
2016-12-17T18:43:54.559414: step 798, loss 0.677461, acc 0.625
2016-12-17T18:43:56.115930: step 799, loss 0.719617, acc 0.5
2016-12-17T18:43:57.626837: step 800, loss 0.677277, acc 0.609375

Evaluation:
2016-12-17T18:44:10.881945: step 800, loss 0.700897, acc 0.485922

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1482016882/checkpoints/model-800

2016-12-17T18:44:14.393134: step 801, loss 0.694434, acc 0.484375
2016-12-17T18:44:15.913740: step 802, loss 0.602578, acc 0.703125
2016-12-17T18:44:17.443947: step 803, loss 0.694576, acc 0.640625
2016-12-17T18:44:18.971367: step 804, loss 0.608581, acc 0.6875
2016-12-17T18:44:20.462855: step 805, loss 0.926596, acc 0.484375
2016-12-17T18:44:22.005214: step 806, loss 0.732576, acc 0.625
2016-12-17T18:44:23.519183: step 807, loss 0.675458, acc 0.671875
2016-12-17T18:44:25.041919: step 808, loss 0.624586, acc 0.640625
2016-12-17T18:44:26.573322: step 809, loss 0.610274, acc 0.671875
2016-12-17T18:44:28.091305: step 810, loss 0.658429, acc 0.609375
2016-12-17T18:44:29.618226: step 811, loss 0.663811, acc 0.625
2016-12-17T18:44:31.148028: step 812, loss 0.645742, acc 0.65625
2016-12-17T18:44:32.667830: step 813, loss 0.633294, acc 0.59375
2016-12-17T18:44:34.179466: step 814, loss 0.66347, acc 0.65625
2016-12-17T18:44:35.706290: step 815, loss 0.707906, acc 0.53125
2016-12-17T18:44:37.223512: step 816, loss 0.61991, acc 0.671875
2016-12-17T18:44:38.733904: step 817, loss 0.643979, acc 0.609375
2016-12-17T18:44:40.266710: step 818, loss 0.638819, acc 0.578125
2016-12-17T18:44:41.761592: step 819, loss 0.608183, acc 0.671875
2016-12-17T18:44:43.298385: step 820, loss 0.602898, acc 0.734375
2016-12-17T18:44:44.913593: step 821, loss 0.702163, acc 0.59375
2016-12-17T18:44:46.437630: step 822, loss 0.656037, acc 0.65625
2016-12-17T18:44:47.977547: step 823, loss 0.678482, acc 0.625
2016-12-17T18:44:49.510544: step 824, loss 0.717319, acc 0.5
2016-12-17T18:44:51.024884: step 825, loss 0.650481, acc 0.5625
2016-12-17T18:44:52.535135: step 826, loss 0.655717, acc 0.515625
2016-12-17T18:44:54.048695: step 827, loss 0.675389, acc 0.625
2016-12-17T18:44:55.556150: step 828, loss 0.658049, acc 0.578125
2016-12-17T18:44:57.077300: step 829, loss 0.648394, acc 0.609375
2016-12-17T18:44:58.606562: step 830, loss 0.683195, acc 0.5625
2016-12-17T18:45:00.130677: step 831, loss 0.634173, acc 0.609375
2016-12-17T18:45:01.628204: step 832, loss 0.726326, acc 0.609375
2016-12-17T18:45:03.136403: step 833, loss 0.624227, acc 0.65625
2016-12-17T18:45:04.638669: step 834, loss 0.692637, acc 0.609375
2016-12-17T18:45:06.152203: step 835, loss 0.690647, acc 0.5625
2016-12-17T18:45:07.656323: step 836, loss 0.682806, acc 0.578125
2016-12-17T18:45:09.187268: step 837, loss 0.635825, acc 0.65625
2016-12-17T18:45:10.723481: step 838, loss 0.620226, acc 0.671875
2016-12-17T18:45:12.221231: step 839, loss 0.761805, acc 0.53125
2016-12-17T18:45:13.729324: step 840, loss 0.656336, acc 0.671875
2016-12-17T18:45:15.250309: step 841, loss 0.618545, acc 0.625
2016-12-17T18:45:16.943752: step 842, loss 0.668752, acc 0.578125
2016-12-17T18:45:18.456186: step 843, loss 0.634965, acc 0.625
2016-12-17T18:45:20.006134: step 844, loss 0.70717, acc 0.578125
2016-12-17T18:45:21.553568: step 845, loss 0.624371, acc 0.609375
2016-12-17T18:45:23.081012: step 846, loss 0.666025, acc 0.578125
2016-12-17T18:45:24.612292: step 847, loss 0.700147, acc 0.59375
2016-12-17T18:45:26.135910: step 848, loss 0.642554, acc 0.625
2016-12-17T18:45:27.681575: step 849, loss 0.711568, acc 0.546875
2016-12-17T18:45:29.205544: step 850, loss 0.689963, acc 0.671875
2016-12-17T18:45:30.713109: step 851, loss 0.602104, acc 0.609375
2016-12-17T18:45:32.262648: step 852, loss 0.657597, acc 0.578125
2016-12-17T18:45:33.807295: step 853, loss 0.646141, acc 0.671875
2016-12-17T18:45:35.326021: step 854, loss 0.683197, acc 0.640625
2016-12-17T18:45:36.852762: step 855, loss 0.698156, acc 0.578125
2016-12-17T18:45:38.361124: step 856, loss 0.759715, acc 0.5625
2016-12-17T18:45:39.893214: step 857, loss 0.684451, acc 0.5625
2016-12-17T18:45:41.449554: step 858, loss 0.715779, acc 0.546875
2016-12-17T18:45:42.932493: step 859, loss 0.647039, acc 0.609375
2016-12-17T18:45:44.440928: step 860, loss 0.614113, acc 0.65625
2016-12-17T18:45:45.958176: step 861, loss 0.706722, acc 0.546875
2016-12-17T18:45:47.530476: step 862, loss 0.606979, acc 0.640625
2016-12-17T18:45:49.134891: step 863, loss 0.637076, acc 0.625
2016-12-17T18:45:50.676918: step 864, loss 0.574263, acc 0.640625
2016-12-17T18:45:52.209651: step 865, loss 0.625835, acc 0.640625
2016-12-17T18:45:53.724680: step 866, loss 0.69377, acc 0.578125
2016-12-17T18:45:55.271539: step 867, loss 0.71489, acc 0.640625
2016-12-17T18:45:56.774419: step 868, loss 0.684976, acc 0.625
2016-12-17T18:45:58.289533: step 869, loss 0.618352, acc 0.671875
2016-12-17T18:45:59.840453: step 870, loss 0.650916, acc 0.609375
2016-12-17T18:46:01.406036: step 871, loss 0.653563, acc 0.609375
2016-12-17T18:46:02.917026: step 872, loss 0.648128, acc 0.65625
2016-12-17T18:46:04.451961: step 873, loss 0.658771, acc 0.609375
2016-12-17T18:46:05.959367: step 874, loss 0.69074, acc 0.578125
2016-12-17T18:46:07.486759: step 875, loss 0.702951, acc 0.578125
2016-12-17T18:46:08.985494: step 876, loss 0.660924, acc 0.625
2016-12-17T18:46:10.489422: step 877, loss 0.795182, acc 0.453125
2016-12-17T18:46:12.023305: step 878, loss 0.695074, acc 0.546875
2016-12-17T18:46:13.506926: step 879, loss 0.660106, acc 0.609375
2016-12-17T18:46:15.034197: step 880, loss 0.6237, acc 0.609375
2016-12-17T18:46:16.560021: step 881, loss 0.636618, acc 0.609375
2016-12-17T18:46:18.041391: step 882, loss 0.743596, acc 0.484375
2016-12-17T18:46:19.576448: step 883, loss 0.664299, acc 0.546875
2016-12-17T18:46:21.203326: step 884, loss 0.687636, acc 0.578125
2016-12-17T18:46:22.722327: step 885, loss 0.628346, acc 0.703125
2016-12-17T18:46:24.274443: step 886, loss 0.656433, acc 0.640625
2016-12-17T18:46:25.820713: step 887, loss 0.597521, acc 0.671875
2016-12-17T18:46:27.369081: step 888, loss 0.701711, acc 0.59375
2016-12-17T18:46:28.924951: step 889, loss 0.666284, acc 0.59375
2016-12-17T18:46:30.456314: step 890, loss 0.58314, acc 0.75
2016-12-17T18:46:31.962593: step 891, loss 0.615972, acc 0.65625
2016-12-17T18:46:33.513462: step 892, loss 0.616476, acc 0.640625
2016-12-17T18:46:35.033714: step 893, loss 0.696144, acc 0.59375
2016-12-17T18:46:36.576787: step 894, loss 0.759502, acc 0.46875
2016-12-17T18:46:38.105393: step 895, loss 0.708187, acc 0.609375
2016-12-17T18:46:39.621675: step 896, loss 0.677487, acc 0.609375
2016-12-17T18:46:41.159328: step 897, loss 0.676962, acc 0.546875
2016-12-17T18:46:42.683465: step 898, loss 0.65415, acc 0.625
2016-12-17T18:46:44.178960: step 899, loss 0.677301, acc 0.578125
2016-12-17T18:46:45.679777: step 900, loss 0.655327, acc 0.625

Evaluation:
2016-12-17T18:47:00.644039: step 900, loss 0.641703, acc 0.638835

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1482016882/checkpoints/model-900

2016-12-17T18:47:04.244947: step 901, loss 0.630472, acc 0.65625
2016-12-17T18:47:05.785200: step 902, loss 0.690114, acc 0.609375
2016-12-17T18:47:07.317135: step 903, loss 0.669604, acc 0.546875
2016-12-17T18:47:08.863968: step 904, loss 0.639198, acc 0.6875
2016-12-17T18:47:10.365157: step 905, loss 0.641382, acc 0.6875
2016-12-17T18:47:11.914000: step 906, loss 0.716946, acc 0.578125
2016-12-17T18:47:13.434014: step 907, loss 0.657467, acc 0.578125
2016-12-17T18:47:15.021398: step 908, loss 0.663948, acc 0.625
2016-12-17T18:47:16.594393: step 909, loss 0.667639, acc 0.65625
2016-12-17T18:47:18.207998: step 910, loss 0.623859, acc 0.65625
2016-12-17T18:47:19.759898: step 911, loss 0.736396, acc 0.5
2016-12-17T18:47:21.337734: step 912, loss 0.716, acc 0.546875
2016-12-17T18:47:22.869690: step 913, loss 0.642239, acc 0.625
2016-12-17T18:47:24.410554: step 914, loss 0.651083, acc 0.65625
2016-12-17T18:47:25.933336: step 915, loss 0.666535, acc 0.5625
2016-12-17T18:47:27.525185: step 916, loss 0.689895, acc 0.515625
2016-12-17T18:47:29.103064: step 917, loss 0.65163, acc 0.609375
2016-12-17T18:47:30.636558: step 918, loss 0.657678, acc 0.59375
2016-12-17T18:47:32.174191: step 919, loss 0.695895, acc 0.625
2016-12-17T18:47:33.698027: step 920, loss 0.556776, acc 0.703125
2016-12-17T18:47:35.251424: step 921, loss 0.762463, acc 0.59375
2016-12-17T18:47:36.761743: step 922, loss 0.721251, acc 0.640625
2016-12-17T18:47:38.272666: step 923, loss 0.674987, acc 0.671875
2016-12-17T18:47:39.758453: step 924, loss 0.708994, acc 0.5625
2016-12-17T18:47:41.291903: step 925, loss 0.616819, acc 0.640625
2016-12-17T18:47:42.795669: step 926, loss 0.721382, acc 0.515625
2016-12-17T18:47:44.310261: step 927, loss 0.710008, acc 0.484375
2016-12-17T18:47:45.850638: step 928, loss 0.73392, acc 0.421875
2016-12-17T18:47:47.372454: step 929, loss 0.704813, acc 0.53125
2016-12-17T18:47:48.899398: step 930, loss 0.648681, acc 0.609375
2016-12-17T18:47:50.415188: step 931, loss 0.543094, acc 0.765625
2016-12-17T18:47:51.936704: step 932, loss 0.632048, acc 0.65625
2016-12-17T18:47:53.417301: step 933, loss 0.689448, acc 0.625
2016-12-17T18:47:54.937159: step 934, loss 0.6677, acc 0.625
2016-12-17T18:47:56.509541: step 935, loss 0.702676, acc 0.609375
2016-12-17T18:47:58.089203: step 936, loss 0.750872, acc 0.609375
2016-12-17T18:47:59.769537: step 937, loss 0.612799, acc 0.671875
2016-12-17T18:48:01.467406: step 938, loss 0.629874, acc 0.671875
2016-12-17T18:48:03.178806: step 939, loss 0.627904, acc 0.625
2016-12-17T18:48:04.838368: step 940, loss 0.671931, acc 0.609375
2016-12-17T18:48:06.412593: step 941, loss 0.723657, acc 0.5625
2016-12-17T18:48:07.997362: step 942, loss 0.631595, acc 0.65625
2016-12-17T18:48:09.604490: step 943, loss 0.664296, acc 0.625
2016-12-17T18:48:11.217266: step 944, loss 0.63761, acc 0.625
2016-12-17T18:48:12.772564: step 945, loss 0.677971, acc 0.546875
2016-12-17T18:48:14.373054: step 946, loss 0.629044, acc 0.625
2016-12-17T18:48:15.989867: step 947, loss 0.712825, acc 0.640625
2016-12-17T18:48:17.583181: step 948, loss 0.64708, acc 0.625
2016-12-17T18:48:19.222392: step 949, loss 0.668832, acc 0.625
2016-12-17T18:48:20.775590: step 950, loss 0.643387, acc 0.609375
2016-12-17T18:48:22.318199: step 951, loss 0.65077, acc 0.578125
2016-12-17T18:48:23.855140: step 952, loss 0.626299, acc 0.609375
2016-12-17T18:48:25.565556: step 953, loss 0.729156, acc 0.53125
2016-12-17T18:48:27.133684: step 954, loss 0.653094, acc 0.59375
2016-12-17T18:48:28.785491: step 955, loss 0.689589, acc 0.546875
2016-12-17T18:48:30.410563: step 956, loss 0.685376, acc 0.5625
2016-12-17T18:48:32.224796: step 957, loss 0.638905, acc 0.640625
2016-12-17T18:48:33.929994: step 958, loss 0.672154, acc 0.59375
2016-12-17T18:48:35.667256: step 959, loss 0.60752, acc 0.6875
2016-12-17T18:48:37.346392: step 960, loss 0.651839, acc 0.5625
2016-12-17T18:48:39.021217: step 961, loss 0.680697, acc 0.5625
2016-12-17T18:48:40.820779: step 962, loss 0.696678, acc 0.578125
2016-12-17T18:48:42.606696: step 963, loss 0.675783, acc 0.671875
2016-12-17T18:48:44.381493: step 964, loss 0.700924, acc 0.625
2016-12-17T18:48:46.048521: step 965, loss 0.73521, acc 0.4375
2016-12-17T18:48:47.615734: step 966, loss 0.675403, acc 0.5625
2016-12-17T18:48:49.232437: step 967, loss 0.667702, acc 0.578125
2016-12-17T18:48:50.884638: step 968, loss 0.696072, acc 0.546875
2016-12-17T18:48:52.499035: step 969, loss 0.633779, acc 0.71875
2016-12-17T18:48:54.157895: step 970, loss 0.659686, acc 0.546875
2016-12-17T18:48:55.818605: step 971, loss 0.578489, acc 0.703125
2016-12-17T18:48:57.829505: step 972, loss 0.67878, acc 0.671875
2016-12-17T18:48:59.504849: step 973, loss 0.608639, acc 0.703125
2016-12-17T18:49:01.119889: step 974, loss 0.683342, acc 0.625
2016-12-17T18:49:02.669842: step 975, loss 0.706157, acc 0.5625
2016-12-17T18:49:04.450238: step 976, loss 0.696352, acc 0.59375
2016-12-17T18:49:06.008230: step 977, loss 0.672795, acc 0.65625
2016-12-17T18:49:07.688072: step 978, loss 0.653048, acc 0.578125
2016-12-17T18:49:09.276521: step 979, loss 0.649324, acc 0.578125
2016-12-17T18:49:10.824516: step 980, loss 0.667143, acc 0.625
2016-12-17T18:49:12.340643: step 981, loss 0.703184, acc 0.53125
2016-12-17T18:49:13.983730: step 982, loss 0.646024, acc 0.65625
2016-12-17T18:49:15.672841: step 983, loss 0.666326, acc 0.59375
2016-12-17T18:49:17.384442: step 984, loss 0.619446, acc 0.640625
2016-12-17T18:49:18.982351: step 985, loss 0.762612, acc 0.515625
2016-12-17T18:49:20.563908: step 986, loss 0.601481, acc 0.65625
2016-12-17T18:49:22.238675: step 987, loss 0.647523, acc 0.703125
2016-12-17T18:49:23.855467: step 988, loss 0.624611, acc 0.671875
2016-12-17T18:49:25.440872: step 989, loss 0.69469, acc 0.53125
2016-12-17T18:49:27.008616: step 990, loss 0.632118, acc 0.65625
2016-12-17T18:49:28.632294: step 991, loss 0.640107, acc 0.578125
2016-12-17T18:49:30.182854: step 992, loss 0.681733, acc 0.65625
2016-12-17T18:49:31.739778: step 993, loss 0.650857, acc 0.578125
2016-12-17T18:49:33.263802: step 994, loss 0.762414, acc 0.484375
2016-12-17T18:49:34.805084: step 995, loss 0.701641, acc 0.5625
2016-12-17T18:49:36.514042: step 996, loss 0.628104, acc 0.640625
2016-12-17T18:49:38.070571: step 997, loss 0.690205, acc 0.546875
2016-12-17T18:49:39.681913: step 998, loss 0.725957, acc 0.5
2016-12-17T18:49:41.240672: step 999, loss 0.644599, acc 0.609375
2016-12-17T18:49:42.753448: step 1000, loss 0.673254, acc 0.65625

Evaluation:
2016-12-17T18:49:59.070996: step 1000, loss 0.644739, acc 0.637379

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1482016882/checkpoints/model-1000

2016-12-17T18:50:02.737063: step 1001, loss 0.633332, acc 0.671875
2016-12-17T18:50:04.380887: step 1002, loss 0.673564, acc 0.609375
2016-12-17T18:50:06.017295: step 1003, loss 0.680323, acc 0.546875
2016-12-17T18:50:07.640464: step 1004, loss 0.635341, acc 0.609375
2016-12-17T18:50:09.315862: step 1005, loss 0.693835, acc 0.578125
2016-12-17T18:50:10.894867: step 1006, loss 0.629174, acc 0.671875
2016-12-17T18:50:12.483648: step 1007, loss 0.583844, acc 0.703125
2016-12-17T18:50:14.022318: step 1008, loss 0.715046, acc 0.5625
2016-12-17T18:50:15.559793: step 1009, loss 0.639913, acc 0.625
2016-12-17T18:50:17.120889: step 1010, loss 0.659181, acc 0.671875
2016-12-17T18:50:18.663229: step 1011, loss 0.679536, acc 0.640625
2016-12-17T18:50:20.252829: step 1012, loss 0.567829, acc 0.6875
2016-12-17T18:50:21.858813: step 1013, loss 0.730814, acc 0.5625
2016-12-17T18:50:23.422628: step 1014, loss 0.707763, acc 0.578125
2016-12-17T18:50:25.002035: step 1015, loss 0.647528, acc 0.546875
2016-12-17T18:50:26.589143: step 1016, loss 0.631593, acc 0.65625
2016-12-17T18:50:28.171646: step 1017, loss 0.690933, acc 0.5625
2016-12-17T18:50:29.760095: step 1018, loss 0.634262, acc 0.671875
2016-12-17T18:50:31.354750: step 1019, loss 0.630516, acc 0.625
2016-12-17T18:50:32.888287: step 1020, loss 0.752802, acc 0.515625
2016-12-17T18:50:34.428713: step 1021, loss 0.733503, acc 0.5
2016-12-17T18:50:35.951636: step 1022, loss 0.680662, acc 0.515625
2016-12-17T18:50:37.516884: step 1023, loss 0.610321, acc 0.703125
2016-12-17T18:50:39.166169: step 1024, loss 0.662871, acc 0.625
2016-12-17T18:50:40.788599: step 1025, loss 0.702845, acc 0.609375
2016-12-17T18:50:42.442763: step 1026, loss 0.672843, acc 0.609375
2016-12-17T18:50:44.063886: step 1027, loss 0.753635, acc 0.5625
2016-12-17T18:50:45.713240: step 1028, loss 0.719218, acc 0.5625
2016-12-17T18:50:47.336121: step 1029, loss 0.643462, acc 0.609375
2016-12-17T18:50:48.899539: step 1030, loss 0.683927, acc 0.578125
2016-12-17T18:50:50.535732: step 1031, loss 0.623358, acc 0.671875
2016-12-17T18:50:52.169178: step 1032, loss 0.65203, acc 0.578125
2016-12-17T18:50:53.742335: step 1033, loss 0.640167, acc 0.625
2016-12-17T18:50:55.432540: step 1034, loss 0.678414, acc 0.578125
2016-12-17T18:50:57.292627: step 1035, loss 0.686441, acc 0.609375
2016-12-17T18:50:58.952945: step 1036, loss 0.679374, acc 0.609375
2016-12-17T18:51:00.510343: step 1037, loss 0.668193, acc 0.65625
2016-12-17T18:51:02.094894: step 1038, loss 0.673017, acc 0.609375
2016-12-17T18:51:03.721187: step 1039, loss 0.679901, acc 0.609375
2016-12-17T18:51:05.349418: step 1040, loss 0.720724, acc 0.546875
2016-12-17T18:51:06.883380: step 1041, loss 0.696528, acc 0.640625
2016-12-17T18:51:08.447119: step 1042, loss 0.629992, acc 0.625
2016-12-17T18:51:10.073462: step 1043, loss 0.650178, acc 0.609375
2016-12-17T18:51:11.651980: step 1044, loss 0.685636, acc 0.625
2016-12-17T18:51:13.395131: step 1045, loss 0.686431, acc 0.5
2016-12-17T18:51:15.057713: step 1046, loss 0.726021, acc 0.453125
2016-12-17T18:51:16.729713: step 1047, loss 0.681725, acc 0.609375
2016-12-17T18:51:18.583314: step 1048, loss 0.681572, acc 0.59375
2016-12-17T18:51:20.218209: step 1049, loss 0.669071, acc 0.5625
2016-12-17T18:51:21.827082: step 1050, loss 0.665819, acc 0.625
2016-12-17T18:51:23.398318: step 1051, loss 0.648492, acc 0.625
2016-12-17T18:51:24.935594: step 1052, loss 0.776199, acc 0.53125
2016-12-17T18:51:26.480803: step 1053, loss 0.747679, acc 0.546875
2016-12-17T18:51:28.037320: step 1054, loss 0.638968, acc 0.671875
2016-12-17T18:51:29.767069: step 1055, loss 0.71171, acc 0.53125
2016-12-17T18:51:31.566244: step 1056, loss 0.631017, acc 0.65625
2016-12-17T18:51:33.387825: step 1057, loss 0.740034, acc 0.53125
2016-12-17T18:51:35.277558: step 1058, loss 0.731584, acc 0.453125
2016-12-17T18:51:37.136943: step 1059, loss 0.659936, acc 0.625
2016-12-17T18:51:38.859081: step 1060, loss 0.689668, acc 0.515625
2016-12-17T18:51:40.524359: step 1061, loss 0.650399, acc 0.625
2016-12-17T18:51:42.102150: step 1062, loss 0.661615, acc 0.578125
2016-12-17T18:51:43.808347: step 1063, loss 0.620744, acc 0.671875
2016-12-17T18:51:45.776039: step 1064, loss 0.652173, acc 0.625
2016-12-17T18:51:47.683098: step 1065, loss 0.7646, acc 0.578125
2016-12-17T18:51:49.439212: step 1066, loss 0.718528, acc 0.625
2016-12-17T18:51:51.081831: step 1067, loss 0.743674, acc 0.5625
2016-12-17T18:51:52.676139: step 1068, loss 0.799636, acc 0.515625
2016-12-17T18:51:54.252802: step 1069, loss 0.626666, acc 0.59375
2016-12-17T18:51:55.920600: step 1070, loss 0.676629, acc 0.59375
2016-12-17T18:51:57.444345: step 1071, loss 0.67169, acc 0.609375
2016-12-17T18:51:59.166136: step 1072, loss 0.677309, acc 0.640625
2016-12-17T18:52:00.721614: step 1073, loss 0.621909, acc 0.671875
2016-12-17T18:52:02.253236: step 1074, loss 0.652824, acc 0.609375
2016-12-17T18:52:03.932587: step 1075, loss 0.821331, acc 0.546875
2016-12-17T18:52:05.507259: step 1076, loss 0.706596, acc 0.640625
2016-12-17T18:52:07.180185: step 1077, loss 0.751157, acc 0.59375
2016-12-17T18:52:08.796777: step 1078, loss 0.667869, acc 0.625
2016-12-17T18:52:10.359711: step 1079, loss 0.644133, acc 0.609375
2016-12-17T18:52:11.983526: step 1080, loss 0.663232, acc 0.59375
2016-12-17T18:52:13.593253: step 1081, loss 0.676927, acc 0.5625
2016-12-17T18:52:15.148123: step 1082, loss 0.647773, acc 0.609375
2016-12-17T18:52:16.668511: step 1083, loss 0.654381, acc 0.5625
2016-12-17T18:52:18.326067: step 1084, loss 0.74886, acc 0.53125
2016-12-17T18:52:19.860285: step 1085, loss 0.640868, acc 0.671875
2016-12-17T18:52:21.424540: step 1086, loss 0.717666, acc 0.59375
2016-12-17T18:52:22.983789: step 1087, loss 0.65891, acc 0.640625
2016-12-17T18:52:24.554624: step 1088, loss 0.646905, acc 0.671875
2016-12-17T18:52:26.167761: step 1089, loss 0.650642, acc 0.640625
2016-12-17T18:52:27.729094: step 1090, loss 0.736924, acc 0.5625
2016-12-17T18:52:29.334831: step 1091, loss 0.655268, acc 0.59375
2016-12-17T18:52:30.903249: step 1092, loss 0.650028, acc 0.59375
2016-12-17T18:52:32.457314: step 1093, loss 0.605123, acc 0.71875
2016-12-17T18:52:34.061873: step 1094, loss 0.781955, acc 0.453125
2016-12-17T18:52:35.623124: step 1095, loss 0.600584, acc 0.75
2016-12-17T18:52:37.170462: step 1096, loss 0.700636, acc 0.546875
2016-12-17T18:52:38.732722: step 1097, loss 0.64857, acc 0.609375
2016-12-17T18:52:40.264655: step 1098, loss 0.648589, acc 0.578125
2016-12-17T18:52:41.834283: step 1099, loss 0.650999, acc 0.609375
2016-12-17T18:52:43.388829: step 1100, loss 0.573025, acc 0.6875

Evaluation:
2016-12-17T18:52:58.975254: step 1100, loss 0.649691, acc 0.631068

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1482016882/checkpoints/model-1100

2016-12-17T18:53:02.817273: step 1101, loss 0.63672, acc 0.671875
2016-12-17T18:53:04.466462: step 1102, loss 0.643015, acc 0.703125
2016-12-17T18:53:06.106314: step 1103, loss 0.703424, acc 0.59375
2016-12-17T18:53:07.834578: step 1104, loss 0.710214, acc 0.5625
2016-12-17T18:53:09.490522: step 1105, loss 0.726134, acc 0.609375
2016-12-17T18:53:11.273646: step 1106, loss 0.671671, acc 0.625
2016-12-17T18:53:12.901024: step 1107, loss 0.723291, acc 0.5625
2016-12-17T18:53:14.525721: step 1108, loss 0.707967, acc 0.546875
2016-12-17T18:53:16.343658: step 1109, loss 0.712329, acc 0.484375
2016-12-17T18:53:18.042256: step 1110, loss 0.733247, acc 0.484375
2016-12-17T18:53:19.690010: step 1111, loss 0.694157, acc 0.53125
2016-12-17T18:53:21.402531: step 1112, loss 0.663334, acc 0.6875
2016-12-17T18:53:23.025591: step 1113, loss 0.588902, acc 0.65625
2016-12-17T18:53:24.681503: step 1114, loss 0.679272, acc 0.578125
2016-12-17T18:53:26.360712: step 1115, loss 0.742233, acc 0.609375
2016-12-17T18:53:27.990131: step 1116, loss 0.598557, acc 0.671875
2016-12-17T18:53:29.649772: step 1117, loss 0.594344, acc 0.6875
2016-12-17T18:53:31.281776: step 1118, loss 0.684806, acc 0.640625
2016-12-17T18:53:32.901554: step 1119, loss 0.571053, acc 0.703125
2016-12-17T18:53:34.542612: step 1120, loss 0.624936, acc 0.65625
2016-12-17T18:53:36.119192: step 1121, loss 0.608108, acc 0.625
2016-12-17T18:53:37.692363: step 1122, loss 0.677862, acc 0.484375
2016-12-17T18:53:39.253200: step 1123, loss 0.619338, acc 0.625
2016-12-17T18:53:40.822988: step 1124, loss 0.627636, acc 0.65625
2016-12-17T18:53:42.743840: step 1125, loss 0.674265, acc 0.5
2016-12-17T18:53:44.357456: step 1126, loss 0.671451, acc 0.578125
2016-12-17T18:53:45.950289: step 1127, loss 0.657813, acc 0.609375
2016-12-17T18:53:47.625198: step 1128, loss 0.714879, acc 0.640625
2016-12-17T18:53:49.347247: step 1129, loss 0.600558, acc 0.734375
2016-12-17T18:53:51.027687: step 1130, loss 0.641812, acc 0.578125
2016-12-17T18:53:52.630002: step 1131, loss 0.676036, acc 0.625
2016-12-17T18:53:54.189257: step 1132, loss 0.684553, acc 0.609375
2016-12-17T18:53:55.775119: step 1133, loss 0.650737, acc 0.6875
2016-12-17T18:53:57.577788: step 1134, loss 0.695308, acc 0.59375
2016-12-17T18:53:59.219540: step 1135, loss 0.666507, acc 0.578125
2016-12-17T18:54:00.815084: step 1136, loss 0.683536, acc 0.546875
2016-12-17T18:54:02.392594: step 1137, loss 0.662673, acc 0.578125
2016-12-17T18:54:03.967675: step 1138, loss 0.636057, acc 0.65625
2016-12-17T18:54:05.552640: step 1139, loss 0.643375, acc 0.65625
2016-12-17T18:54:07.070353: step 1140, loss 0.602394, acc 0.703125
2016-12-17T18:54:08.610664: step 1141, loss 0.646466, acc 0.640625
2016-12-17T18:54:10.193983: step 1142, loss 0.801458, acc 0.515625
2016-12-17T18:54:11.745432: step 1143, loss 0.628151, acc 0.703125
2016-12-17T18:54:13.282549: step 1144, loss 0.690083, acc 0.625
2016-12-17T18:54:14.809157: step 1145, loss 0.606268, acc 0.625
2016-12-17T18:54:16.375019: step 1146, loss 0.667716, acc 0.65625
2016-12-17T18:54:17.914566: step 1147, loss 0.624471, acc 0.625
2016-12-17T18:54:19.438149: step 1148, loss 0.669384, acc 0.609375
2016-12-17T18:54:21.019646: step 1149, loss 0.675312, acc 0.5625
2016-12-17T18:54:22.531665: step 1150, loss 0.622141, acc 0.609375
2016-12-17T18:54:24.069988: step 1151, loss 0.672147, acc 0.640625
2016-12-17T18:54:25.638157: step 1152, loss 0.72517, acc 0.515625
2016-12-17T18:54:27.191288: step 1153, loss 0.695359, acc 0.546875
2016-12-17T18:54:28.823811: step 1154, loss 0.719802, acc 0.546875
2016-12-17T18:54:30.558758: step 1155, loss 0.689772, acc 0.578125
2016-12-17T18:54:32.239609: step 1156, loss 0.624122, acc 0.671875
2016-12-17T18:54:33.955198: step 1157, loss 0.659011, acc 0.515625
2016-12-17T18:54:35.682700: step 1158, loss 0.6691, acc 0.578125
2016-12-17T18:54:37.417691: step 1159, loss 0.636744, acc 0.65625
2016-12-17T18:54:39.087624: step 1160, loss 0.683569, acc 0.546875
2016-12-17T18:54:40.723081: step 1161, loss 0.67026, acc 0.625
2016-12-17T18:54:42.701396: step 1162, loss 0.699888, acc 0.609375
2016-12-17T18:54:44.360168: step 1163, loss 0.598764, acc 0.703125
2016-12-17T18:54:46.023711: step 1164, loss 0.647927, acc 0.671875
2016-12-17T18:54:47.630948: step 1165, loss 0.654578, acc 0.625
2016-12-17T18:54:49.182680: step 1166, loss 0.663524, acc 0.609375
2016-12-17T18:54:50.736790: step 1167, loss 0.74774, acc 0.546875
2016-12-17T18:54:52.276536: step 1168, loss 0.685663, acc 0.609375
2016-12-17T18:54:53.792643: step 1169, loss 0.646206, acc 0.5625
2016-12-17T18:54:55.311239: step 1170, loss 0.677197, acc 0.578125
2016-12-17T18:54:56.890929: step 1171, loss 0.691316, acc 0.625
2016-12-17T18:54:58.410196: step 1172, loss 0.709266, acc 0.546875
2016-12-17T18:54:59.986395: step 1173, loss 0.622924, acc 0.703125
2016-12-17T18:55:01.574290: step 1174, loss 0.671371, acc 0.5625
2016-12-17T18:55:03.151723: step 1175, loss 0.705567, acc 0.5625
2016-12-17T18:55:04.812171: step 1176, loss 0.62346, acc 0.703125
2016-12-17T18:55:06.482076: step 1177, loss 0.574539, acc 0.71875
2016-12-17T18:55:08.340508: step 1178, loss 0.648438, acc 0.640625
2016-12-17T18:55:10.141390: step 1179, loss 0.727139, acc 0.5
2016-12-17T18:55:11.874491: step 1180, loss 0.661672, acc 0.5625
2016-12-17T18:55:13.563002: step 1181, loss 0.733569, acc 0.546875
2016-12-17T18:55:15.210970: step 1182, loss 0.663378, acc 0.609375
2016-12-17T18:55:16.882340: step 1183, loss 0.705596, acc 0.5
2016-12-17T18:55:18.506272: step 1184, loss 0.691154, acc 0.53125
2016-12-17T18:55:20.221549: step 1185, loss 0.720971, acc 0.484375
2016-12-17T18:55:21.874029: step 1186, loss 0.692333, acc 0.546875
2016-12-17T18:55:23.561321: step 1187, loss 0.681158, acc 0.578125
2016-12-17T18:55:25.213553: step 1188, loss 0.623303, acc 0.640625
2016-12-17T18:55:26.915690: step 1189, loss 0.6864, acc 0.578125
2016-12-17T18:55:28.594608: step 1190, loss 0.730084, acc 0.546875
2016-12-17T18:55:30.312916: step 1191, loss 0.684158, acc 0.609375
2016-12-17T18:55:32.176413: step 1192, loss 0.651516, acc 0.625
2016-12-17T18:55:34.017332: step 1193, loss 0.648268, acc 0.625
2016-12-17T18:55:35.963094: step 1194, loss 0.745093, acc 0.484375
2016-12-17T18:55:37.816113: step 1195, loss 0.700189, acc 0.609375
2016-12-17T18:55:39.486029: step 1196, loss 0.693437, acc 0.625
2016-12-17T18:55:41.199409: step 1197, loss 0.655028, acc 0.640625
2016-12-17T18:55:42.856059: step 1198, loss 0.674646, acc 0.515625
2016-12-17T18:55:44.458561: step 1199, loss 0.668108, acc 0.625
2016-12-17T18:55:46.058523: step 1200, loss 0.675103, acc 0.5625

Evaluation:
2016-12-17T18:56:03.097976: step 1200, loss 0.647595, acc 0.63932

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1482016882/checkpoints/model-1200

2016-12-17T18:56:06.816080: step 1201, loss 0.694425, acc 0.578125
2016-12-17T18:56:08.357803: step 1202, loss 0.644644, acc 0.59375
2016-12-17T18:56:09.952774: step 1203, loss 0.647845, acc 0.6875
2016-12-17T18:56:11.495338: step 1204, loss 0.648966, acc 0.625
2016-12-17T18:56:13.029000: step 1205, loss 0.618315, acc 0.71875
2016-12-17T18:56:14.589926: step 1206, loss 0.660801, acc 0.671875
2016-12-17T18:56:16.144331: step 1207, loss 0.739179, acc 0.609375
2016-12-17T18:56:17.711012: step 1208, loss 0.706604, acc 0.5
2016-12-17T18:56:19.341064: step 1209, loss 0.587942, acc 0.65625
2016-12-17T18:56:20.984198: step 1210, loss 0.648389, acc 0.6875
2016-12-17T18:56:22.704136: step 1211, loss 0.653006, acc 0.609375
2016-12-17T18:56:24.450178: step 1212, loss 0.676944, acc 0.59375
2016-12-17T18:56:26.233065: step 1213, loss 0.655228, acc 0.59375
2016-12-17T18:56:27.944684: step 1214, loss 0.687604, acc 0.609375
2016-12-17T18:56:29.663329: step 1215, loss 0.666956, acc 0.578125
2016-12-17T18:56:31.335969: step 1216, loss 0.624099, acc 0.640625
2016-12-17T18:56:32.992544: step 1217, loss 0.706333, acc 0.59375
2016-12-17T18:56:34.703878: step 1218, loss 0.660678, acc 0.578125
2016-12-17T18:56:36.383288: step 1219, loss 0.618083, acc 0.65625
2016-12-17T18:56:38.171508: step 1220, loss 0.597167, acc 0.65625
2016-12-17T18:56:40.026510: step 1221, loss 0.633991, acc 0.609375
2016-12-17T18:56:41.708925: step 1222, loss 0.683845, acc 0.546875
2016-12-17T18:56:43.325357: step 1223, loss 0.634778, acc 0.59375
2016-12-17T18:56:45.242770: step 1224, loss 0.594463, acc 0.65625
2016-12-17T18:56:46.883999: step 1225, loss 0.606048, acc 0.640625
2016-12-17T18:56:48.596597: step 1226, loss 0.564603, acc 0.6875
2016-12-17T18:56:50.267507: step 1227, loss 0.707211, acc 0.609375
2016-12-17T18:56:52.010474: step 1228, loss 0.666693, acc 0.625
2016-12-17T18:56:54.068850: step 1229, loss 0.71407, acc 0.53125
2016-12-17T18:56:55.813552: step 1230, loss 0.662002, acc 0.609375
2016-12-17T18:56:57.445414: step 1231, loss 0.729966, acc 0.546875
2016-12-17T18:56:59.011615: step 1232, loss 0.68755, acc 0.5625
2016-12-17T18:57:00.565809: step 1233, loss 0.65772, acc 0.640625
2016-12-17T18:57:02.104932: step 1234, loss 0.699771, acc 0.578125
2016-12-17T18:57:03.661114: step 1235, loss 0.598142, acc 0.65625
2016-12-17T18:57:05.178564: step 1236, loss 0.62368, acc 0.640625
2016-12-17T18:57:06.715272: step 1237, loss 0.801926, acc 0.546875
2016-12-17T18:57:08.269422: step 1238, loss 0.785411, acc 0.671875
2016-12-17T18:57:09.810431: step 1239, loss 0.683683, acc 0.609375
2016-12-17T18:57:11.435908: step 1240, loss 0.621211, acc 0.6875
2016-12-17T18:57:13.014338: step 1241, loss 0.690499, acc 0.59375
2016-12-17T18:57:14.620350: step 1242, loss 0.625261, acc 0.578125
2016-12-17T18:57:16.232594: step 1243, loss 0.649076, acc 0.6875
2016-12-17T18:57:17.908123: step 1244, loss 0.680617, acc 0.546875
2016-12-17T18:57:19.621141: step 1245, loss 0.676631, acc 0.5625
2016-12-17T18:57:21.207727: step 1246, loss 0.71453, acc 0.5625
2016-12-17T18:57:22.810911: step 1247, loss 0.679302, acc 0.578125
2016-12-17T18:57:24.471465: step 1248, loss 0.762837, acc 0.59375
2016-12-17T18:57:26.159874: step 1249, loss 0.648162, acc 0.671875
2016-12-17T18:57:27.829336: step 1250, loss 0.60848, acc 0.703125
2016-12-17T18:57:29.485066: step 1251, loss 0.845449, acc 0.46875
2016-12-17T18:57:31.185826: step 1252, loss 0.657081, acc 0.640625
2016-12-17T18:57:32.762934: step 1253, loss 0.661609, acc 0.609375
2016-12-17T18:57:34.409486: step 1254, loss 0.711265, acc 0.484375
2016-12-17T18:57:36.063760: step 1255, loss 0.676044, acc 0.578125
2016-12-17T18:57:37.726312: step 1256, loss 0.625087, acc 0.703125
2016-12-17T18:57:39.315185: step 1257, loss 0.691597, acc 0.625
2016-12-17T18:57:40.921103: step 1258, loss 0.653212, acc 0.65625
2016-12-17T18:57:42.610021: step 1259, loss 0.707365, acc 0.5625
2016-12-17T18:57:44.298411: step 1260, loss 0.738135, acc 0.546875
2016-12-17T18:57:45.948880: step 1261, loss 0.672387, acc 0.609375
2016-12-17T18:57:47.583237: step 1262, loss 0.637785, acc 0.65625
2016-12-17T18:57:49.194148: step 1263, loss 0.626866, acc 0.65625
2016-12-17T18:57:50.830309: step 1264, loss 0.705936, acc 0.546875
2016-12-17T18:57:52.467519: step 1265, loss 0.667606, acc 0.578125
2016-12-17T18:57:54.171947: step 1266, loss 0.680029, acc 0.59375
2016-12-17T18:57:55.740614: step 1267, loss 0.678041, acc 0.59375
2016-12-17T18:57:57.295597: step 1268, loss 0.637109, acc 0.640625
2016-12-17T18:57:58.852432: step 1269, loss 0.63891, acc 0.65625
2016-12-17T18:58:00.584098: step 1270, loss 0.59888, acc 0.75
2016-12-17T18:58:02.388671: step 1271, loss 0.681101, acc 0.625
2016-12-17T18:58:04.110378: step 1272, loss 0.728529, acc 0.59375
2016-12-17T18:58:05.780138: step 1273, loss 0.643372, acc 0.65625
2016-12-17T18:58:07.568918: step 1274, loss 0.689182, acc 0.609375
2016-12-17T18:58:09.235532: step 1275, loss 0.676186, acc 0.5625
2016-12-17T18:58:10.906803: step 1276, loss 0.636948, acc 0.59375
2016-12-17T18:58:12.536389: step 1277, loss 0.612658, acc 0.671875
2016-12-17T18:58:14.150226: step 1278, loss 0.706558, acc 0.53125
2016-12-17T18:58:15.895517: step 1279, loss 0.640787, acc 0.671875
2016-12-17T18:58:17.501638: step 1280, loss 0.7239, acc 0.5625
2016-12-17T18:58:19.111766: step 1281, loss 0.750292, acc 0.546875
2016-12-17T18:58:20.744954: step 1282, loss 0.591188, acc 0.71875
2016-12-17T18:58:22.318932: step 1283, loss 0.622641, acc 0.640625
2016-12-17T18:58:23.903500: step 1284, loss 0.634529, acc 0.65625
2016-12-17T18:58:25.549843: step 1285, loss 0.668063, acc 0.515625
2016-12-17T18:58:27.123254: step 1286, loss 0.636422, acc 0.640625
2016-12-17T18:58:28.754617: step 1287, loss 0.666625, acc 0.515625
2016-12-17T18:58:30.392101: step 1288, loss 0.638464, acc 0.65625
2016-12-17T18:58:32.044021: step 1289, loss 0.621232, acc 0.65625
2016-12-17T18:58:33.690808: step 1290, loss 0.625782, acc 0.703125
2016-12-17T18:58:35.253840: step 1291, loss 0.672175, acc 0.59375
2016-12-17T18:58:36.792504: step 1292, loss 0.677031, acc 0.625
2016-12-17T18:58:38.337336: step 1293, loss 0.708104, acc 0.65625
2016-12-17T18:58:39.947306: step 1294, loss 0.679143, acc 0.59375
2016-12-17T18:58:41.555509: step 1295, loss 0.699288, acc 0.671875
2016-12-17T18:58:43.175343: step 1296, loss 0.718153, acc 0.53125
2016-12-17T18:58:44.767383: step 1297, loss 0.701715, acc 0.609375
2016-12-17T18:58:46.446499: step 1298, loss 0.695497, acc 0.609375
2016-12-17T18:58:48.130067: step 1299, loss 0.716617, acc 0.59375
2016-12-17T18:58:49.833380: step 1300, loss 0.673235, acc 0.5625

Evaluation:
2016-12-17T18:59:03.650320: step 1300, loss 0.640201, acc 0.63835

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1482016882/checkpoints/model-1300

2016-12-17T18:59:07.278542: step 1301, loss 0.66086, acc 0.609375
2016-12-17T18:59:08.868544: step 1302, loss 0.71003, acc 0.5625
2016-12-17T18:59:10.440446: step 1303, loss 0.675885, acc 0.59375
2016-12-17T18:59:12.021782: step 1304, loss 0.851792, acc 0.421875
2016-12-17T18:59:13.603008: step 1305, loss 0.639148, acc 0.671875
2016-12-17T18:59:15.167291: step 1306, loss 0.654423, acc 0.625
2016-12-17T18:59:16.730551: step 1307, loss 0.716911, acc 0.578125
2016-12-17T18:59:18.276623: step 1308, loss 0.630256, acc 0.640625
2016-12-17T18:59:19.907988: step 1309, loss 0.623241, acc 0.59375
2016-12-17T18:59:21.506391: step 1310, loss 0.621585, acc 0.6875
2016-12-17T18:59:23.071711: step 1311, loss 0.64308, acc 0.609375
2016-12-17T18:59:24.679707: step 1312, loss 0.689095, acc 0.640625
2016-12-17T18:59:26.283748: step 1313, loss 0.646554, acc 0.71875
2016-12-17T18:59:27.843639: step 1314, loss 0.750532, acc 0.53125
2016-12-17T18:59:29.404242: step 1315, loss 0.548257, acc 0.6875
2016-12-17T18:59:30.974609: step 1316, loss 0.629245, acc 0.65625
2016-12-17T18:59:32.532662: step 1317, loss 0.710227, acc 0.53125
2016-12-17T18:59:34.105545: step 1318, loss 0.673303, acc 0.546875
2016-12-17T18:59:35.708759: step 1319, loss 0.649323, acc 0.625
2016-12-17T18:59:37.286818: step 1320, loss 0.608356, acc 0.703125
2016-12-17T18:59:38.849174: step 1321, loss 0.669348, acc 0.640625
2016-12-17T18:59:40.422387: step 1322, loss 0.619946, acc 0.671875
2016-12-17T18:59:41.972594: step 1323, loss 0.646237, acc 0.625
2016-12-17T18:59:43.505853: step 1324, loss 0.67855, acc 0.5625
2016-12-17T18:59:45.067088: step 1325, loss 0.643755, acc 0.625
2016-12-17T18:59:46.653802: step 1326, loss 0.653572, acc 0.640625
2016-12-17T18:59:48.201209: step 1327, loss 0.649462, acc 0.625
2016-12-17T18:59:49.741455: step 1328, loss 0.564356, acc 0.734375
2016-12-17T18:59:51.288504: step 1329, loss 0.710195, acc 0.546875
2016-12-17T18:59:52.929591: step 1330, loss 0.613491, acc 0.703125
2016-12-17T18:59:54.472089: step 1331, loss 0.727317, acc 0.5
2016-12-17T18:59:56.031933: step 1332, loss 0.60858, acc 0.65625
2016-12-17T18:59:57.567519: step 1333, loss 0.720828, acc 0.5625
2016-12-17T18:59:59.179789: step 1334, loss 0.641436, acc 0.671875
2016-12-17T19:00:00.719089: step 1335, loss 0.598842, acc 0.703125
2016-12-17T19:00:02.321257: step 1336, loss 0.552091, acc 0.796875
2016-12-17T19:00:03.873539: step 1337, loss 0.723248, acc 0.59375
2016-12-17T19:00:05.434684: step 1338, loss 0.69446, acc 0.578125
2016-12-17T19:00:06.994208: step 1339, loss 0.710268, acc 0.59375
2016-12-17T19:00:08.561508: step 1340, loss 0.687865, acc 0.625
2016-12-17T19:00:10.081495: step 1341, loss 0.714288, acc 0.484375
2016-12-17T19:00:11.614681: step 1342, loss 0.646961, acc 0.609375
2016-12-17T19:00:13.173276: step 1343, loss 0.696318, acc 0.609375
2016-12-17T19:00:14.713247: step 1344, loss 0.704281, acc 0.5625
2016-12-17T19:00:16.260676: step 1345, loss 0.73004, acc 0.515625
2016-12-17T19:00:17.827314: step 1346, loss 0.664611, acc 0.640625
2016-12-17T19:00:19.424044: step 1347, loss 0.615558, acc 0.6875
2016-12-17T19:00:20.983518: step 1348, loss 0.684956, acc 0.546875
2016-12-17T19:00:22.542319: step 1349, loss 0.633195, acc 0.65625
2016-12-17T19:00:24.154129: step 1350, loss 0.742722, acc 0.546875
2016-12-17T19:00:25.797211: step 1351, loss 0.771965, acc 0.46875
2016-12-17T19:00:27.379411: step 1352, loss 0.713678, acc 0.515625
2016-12-17T19:00:28.960296: step 1353, loss 0.698786, acc 0.515625
2016-12-17T19:00:30.518395: step 1354, loss 0.668487, acc 0.59375
2016-12-17T19:00:32.095044: step 1355, loss 0.716413, acc 0.546875
2016-12-17T19:00:33.657090: step 1356, loss 0.686253, acc 0.578125
2016-12-17T19:00:35.227823: step 1357, loss 0.62752, acc 0.671875
2016-12-17T19:00:36.768002: step 1358, loss 0.630466, acc 0.640625
2016-12-17T19:00:38.348213: step 1359, loss 0.712642, acc 0.5625
2016-12-17T19:00:39.925890: step 1360, loss 0.588367, acc 0.640625
2016-12-17T19:00:41.470803: step 1361, loss 0.603373, acc 0.671875
2016-12-17T19:00:43.024120: step 1362, loss 0.637945, acc 0.6875
2016-12-17T19:00:44.561921: step 1363, loss 0.651971, acc 0.734375
2016-12-17T19:00:46.099849: step 1364, loss 0.625749, acc 0.671875
2016-12-17T19:00:47.737993: step 1365, loss 0.655662, acc 0.625
2016-12-17T19:00:49.340277: step 1366, loss 0.659998, acc 0.5625
2016-12-17T19:00:50.946999: step 1367, loss 0.603879, acc 0.71875
2016-12-17T19:00:52.613386: step 1368, loss 0.595217, acc 0.671875
2016-12-17T19:00:54.267340: step 1369, loss 0.661564, acc 0.59375
2016-12-17T19:00:55.876239: step 1370, loss 0.657158, acc 0.640625
2016-12-17T19:00:57.492887: step 1371, loss 0.667231, acc 0.59375
2016-12-17T19:00:59.051052: step 1372, loss 0.651906, acc 0.59375
2016-12-17T19:01:00.590082: step 1373, loss 0.591804, acc 0.703125
2016-12-17T19:01:02.102501: step 1374, loss 0.650844, acc 0.578125
2016-12-17T19:01:03.706724: step 1375, loss 0.637884, acc 0.640625
2016-12-17T19:01:05.278160: step 1376, loss 0.647683, acc 0.625
2016-12-17T19:01:06.860618: step 1377, loss 0.694263, acc 0.65625
2016-12-17T19:01:08.511623: step 1378, loss 0.659674, acc 0.59375
2016-12-17T19:01:10.131769: step 1379, loss 0.626396, acc 0.625
2016-12-17T19:01:11.702042: step 1380, loss 0.564912, acc 0.703125
2016-12-17T19:01:13.253985: step 1381, loss 0.679284, acc 0.59375
2016-12-17T19:01:14.789164: step 1382, loss 0.60868, acc 0.625
2016-12-17T19:01:16.410296: step 1383, loss 0.593346, acc 0.703125
2016-12-17T19:01:17.997451: step 1384, loss 0.641529, acc 0.671875
2016-12-17T19:01:19.582306: step 1385, loss 0.722145, acc 0.5
2016-12-17T19:01:21.208595: step 1386, loss 0.561631, acc 0.78125
2016-12-17T19:01:22.836137: step 1387, loss 0.627973, acc 0.671875
2016-12-17T19:01:24.814075: step 1388, loss 0.666024, acc 0.59375
2016-12-17T19:01:26.482241: step 1389, loss 0.676372, acc 0.65625
2016-12-17T19:01:28.285107: step 1390, loss 0.644751, acc 0.609375
2016-12-17T19:01:29.953090: step 1391, loss 0.693548, acc 0.59375
2016-12-17T19:01:31.774778: step 1392, loss 0.679323, acc 0.640625
2016-12-17T19:01:33.678037: step 1393, loss 0.666537, acc 0.65625
2016-12-17T19:01:35.407887: step 1394, loss 0.696789, acc 0.5625
2016-12-17T19:01:37.144163: step 1395, loss 0.681829, acc 0.640625
2016-12-17T19:01:38.829882: step 1396, loss 0.686848, acc 0.53125
2016-12-17T19:01:40.522899: step 1397, loss 0.733045, acc 0.5625
2016-12-17T19:01:42.260582: step 1398, loss 0.624607, acc 0.671875
2016-12-17T19:01:43.878650: step 1399, loss 0.654199, acc 0.625
2016-12-17T19:01:45.898771: step 1400, loss 0.612221, acc 0.65625

Evaluation:
2016-12-17T19:02:01.612236: step 1400, loss 0.647255, acc 0.63835

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1482016882/checkpoints/model-1400

2016-12-17T19:02:05.371083: step 1401, loss 0.777081, acc 0.515625
2016-12-17T19:02:07.268753: step 1402, loss 0.655372, acc 0.640625
2016-12-17T19:02:09.237170: step 1403, loss 0.564929, acc 0.640625
2016-12-17T19:02:11.401190: step 1404, loss 0.646562, acc 0.578125
2016-12-17T19:02:13.045315: step 1405, loss 0.521479, acc 0.75
2016-12-17T19:02:14.629578: step 1406, loss 0.709144, acc 0.609375
2016-12-17T19:02:16.183755: step 1407, loss 0.611156, acc 0.671875
2016-12-17T19:02:17.738604: step 1408, loss 0.669013, acc 0.59375
2016-12-17T19:02:19.294936: step 1409, loss 0.674237, acc 0.59375
2016-12-17T19:02:20.846438: step 1410, loss 0.641625, acc 0.640625
2016-12-17T19:02:22.398620: step 1411, loss 0.667502, acc 0.640625
2016-12-17T19:02:23.961024: step 1412, loss 0.689204, acc 0.5625
2016-12-17T19:02:25.594945: step 1413, loss 0.658977, acc 0.65625
2016-12-17T19:02:27.175083: step 1414, loss 0.71952, acc 0.53125
2016-12-17T19:02:28.743094: step 1415, loss 0.647808, acc 0.609375
2016-12-17T19:02:30.324669: step 1416, loss 0.650502, acc 0.609375
2016-12-17T19:02:31.879516: step 1417, loss 0.622424, acc 0.59375
2016-12-17T19:02:33.516738: step 1418, loss 0.627579, acc 0.71875
2016-12-17T19:02:35.185524: step 1419, loss 0.605755, acc 0.625
2016-12-17T19:02:36.755569: step 1420, loss 0.762186, acc 0.484375
2016-12-17T19:02:38.308114: step 1421, loss 0.693449, acc 0.578125
2016-12-17T19:02:39.859045: step 1422, loss 0.633875, acc 0.6875
2016-12-17T19:02:41.468285: step 1423, loss 0.636704, acc 0.625
2016-12-17T19:02:43.017802: step 1424, loss 0.661841, acc 0.609375
2016-12-17T19:02:44.574579: step 1425, loss 0.624274, acc 0.625
2016-12-17T19:02:46.100162: step 1426, loss 0.655843, acc 0.65625
2016-12-17T19:02:47.613130: step 1427, loss 0.693598, acc 0.546875
2016-12-17T19:02:49.170941: step 1428, loss 0.705931, acc 0.5
2016-12-17T19:02:50.697159: step 1429, loss 0.698617, acc 0.578125
2016-12-17T19:02:52.249378: step 1430, loss 0.642151, acc 0.59375
2016-12-17T19:02:53.844710: step 1431, loss 0.628626, acc 0.65625
2016-12-17T19:02:55.453559: step 1432, loss 0.657237, acc 0.625
2016-12-17T19:02:57.065937: step 1433, loss 0.61899, acc 0.671875
2016-12-17T19:02:58.839018: step 1434, loss 0.567151, acc 0.6875
2016-12-17T19:03:00.672255: step 1435, loss 0.684049, acc 0.609375
2016-12-17T19:03:02.624684: step 1436, loss 0.643119, acc 0.71875
2016-12-17T19:03:04.485360: step 1437, loss 0.648602, acc 0.640625
2016-12-17T19:03:06.398961: step 1438, loss 0.644919, acc 0.59375
2016-12-17T19:03:08.202959: step 1439, loss 0.671819, acc 0.609375
2016-12-17T19:03:09.786287: step 1440, loss 0.637354, acc 0.625
2016-12-17T19:03:11.370947: step 1441, loss 0.657393, acc 0.5625
2016-12-17T19:03:12.913625: step 1442, loss 0.715699, acc 0.5
2016-12-17T19:03:14.457281: step 1443, loss 0.655866, acc 0.546875
2016-12-17T19:03:15.988976: step 1444, loss 0.674139, acc 0.59375
2016-12-17T19:03:17.553033: step 1445, loss 0.654352, acc 0.71875
2016-12-17T19:03:19.104792: step 1446, loss 0.682403, acc 0.5625
2016-12-17T19:03:20.735429: step 1447, loss 0.747484, acc 0.5625
2016-12-17T19:03:22.313202: step 1448, loss 0.667961, acc 0.5
2016-12-17T19:03:23.924127: step 1449, loss 0.667215, acc 0.5625
2016-12-17T19:03:25.535159: step 1450, loss 0.629024, acc 0.71875
2016-12-17T19:03:27.088200: step 1451, loss 0.687776, acc 0.515625
2016-12-17T19:03:28.639248: step 1452, loss 0.696885, acc 0.609375
2016-12-17T19:03:30.184005: step 1453, loss 0.675955, acc 0.625
2016-12-17T19:03:31.718530: step 1454, loss 0.735128, acc 0.609375
2016-12-17T19:03:33.283719: step 1455, loss 0.653738, acc 0.640625
2016-12-17T19:03:34.905915: step 1456, loss 0.747443, acc 0.4375
2016-12-17T19:03:36.597539: step 1457, loss 0.71259, acc 0.546875
2016-12-17T19:03:38.366148: step 1458, loss 0.651897, acc 0.59375
2016-12-17T19:03:40.144464: step 1459, loss 0.768536, acc 0.390625
2016-12-17T19:03:41.820647: step 1460, loss 0.682308, acc 0.578125
2016-12-17T19:03:43.416344: step 1461, loss 0.656443, acc 0.609375
2016-12-17T19:03:45.011565: step 1462, loss 0.706576, acc 0.5625
2016-12-17T19:03:46.547939: step 1463, loss 0.673798, acc 0.65625
2016-12-17T19:03:48.116493: step 1464, loss 0.788836, acc 0.484375
2016-12-17T19:03:49.677239: step 1465, loss 0.617392, acc 0.609375
2016-12-17T19:03:51.219184: step 1466, loss 0.629139, acc 0.671875
2016-12-17T19:03:52.734573: step 1467, loss 0.658542, acc 0.671875
2016-12-17T19:03:54.258079: step 1468, loss 0.624566, acc 0.6875
2016-12-17T19:03:55.807253: step 1469, loss 0.627058, acc 0.625
2016-12-17T19:03:57.365366: step 1470, loss 0.6771, acc 0.625
2016-12-17T19:03:58.924747: step 1471, loss 0.746584, acc 0.609375
2016-12-17T19:04:00.467643: step 1472, loss 0.74036, acc 0.53125
2016-12-17T19:04:01.993275: step 1473, loss 0.624747, acc 0.671875
2016-12-17T19:04:03.519927: step 1474, loss 0.656446, acc 0.625
2016-12-17T19:04:05.045739: step 1475, loss 0.641932, acc 0.6875
2016-12-17T19:04:06.571766: step 1476, loss 0.701783, acc 0.546875
2016-12-17T19:04:08.105593: step 1477, loss 0.630313, acc 0.5625
2016-12-17T19:04:09.639365: step 1478, loss 0.578233, acc 0.671875
2016-12-17T19:04:11.269945: step 1479, loss 0.607923, acc 0.703125
2016-12-17T19:04:12.797172: step 1480, loss 0.659283, acc 0.625
2016-12-17T19:04:14.385437: step 1481, loss 0.696535, acc 0.5625
2016-12-17T19:04:15.947430: step 1482, loss 0.528447, acc 0.828125
2016-12-17T19:04:17.508550: step 1483, loss 0.646851, acc 0.671875
2016-12-17T19:04:19.069445: step 1484, loss 0.649506, acc 0.65625
2016-12-17T19:04:20.600593: step 1485, loss 0.581104, acc 0.640625
2016-12-17T19:04:22.148254: step 1486, loss 0.654118, acc 0.65625
2016-12-17T19:04:23.709345: step 1487, loss 0.639416, acc 0.625
2016-12-17T19:04:25.249346: step 1488, loss 0.670887, acc 0.53125
2016-12-17T19:04:26.809278: step 1489, loss 0.691483, acc 0.53125
2016-12-17T19:04:28.382306: step 1490, loss 0.615038, acc 0.6875
2016-12-17T19:04:29.960793: step 1491, loss 0.633029, acc 0.625
2016-12-17T19:04:31.499907: step 1492, loss 0.755873, acc 0.5
2016-12-17T19:04:33.023085: step 1493, loss 0.69409, acc 0.546875
2016-12-17T19:04:34.540909: step 1494, loss 0.642866, acc 0.625
2016-12-17T19:04:36.077000: step 1495, loss 0.62258, acc 0.65625
2016-12-17T19:04:37.612671: step 1496, loss 0.615811, acc 0.6875
2016-12-17T19:04:39.159854: step 1497, loss 0.606364, acc 0.75
2016-12-17T19:04:40.715478: step 1498, loss 0.688274, acc 0.59375
2016-12-17T19:04:42.355610: step 1499, loss 0.630958, acc 0.59375
2016-12-17T19:04:43.912145: step 1500, loss 0.636482, acc 0.6875

Evaluation:
2016-12-17T19:04:58.809373: step 1500, loss 0.6415, acc 0.638835

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1482016882/checkpoints/model-1500

2016-12-17T19:05:02.402450: step 1501, loss 0.666511, acc 0.65625
2016-12-17T19:05:03.977324: step 1502, loss 0.643783, acc 0.640625
2016-12-17T19:05:05.521411: step 1503, loss 0.67041, acc 0.578125
2016-12-17T19:05:07.035227: step 1504, loss 0.710966, acc 0.59375
2016-12-17T19:05:08.573536: step 1505, loss 0.645517, acc 0.609375
2016-12-17T19:05:10.072568: step 1506, loss 0.718576, acc 0.515625
2016-12-17T19:05:11.623064: step 1507, loss 0.763847, acc 0.421875
2016-12-17T19:05:13.149771: step 1508, loss 0.67928, acc 0.578125
2016-12-17T19:05:14.722504: step 1509, loss 0.747253, acc 0.375
2016-12-17T19:05:16.260431: step 1510, loss 0.671734, acc 0.578125
2016-12-17T19:05:17.791786: step 1511, loss 0.60169, acc 0.703125
2016-12-17T19:05:19.345257: step 1512, loss 0.661143, acc 0.625
2016-12-17T19:05:20.869280: step 1513, loss 0.656739, acc 0.609375
2016-12-17T19:05:22.393385: step 1514, loss 0.903125, acc 0.515625
2016-12-17T19:05:23.876083: step 1515, loss 0.748425, acc 0.640625
2016-12-17T19:05:25.406669: step 1516, loss 0.65619, acc 0.609375
2016-12-17T19:05:26.930883: step 1517, loss 0.608232, acc 0.703125
2016-12-17T19:05:28.481377: step 1518, loss 0.618005, acc 0.640625
2016-12-17T19:05:30.010392: step 1519, loss 0.709921, acc 0.515625
2016-12-17T19:05:31.574063: step 1520, loss 0.682227, acc 0.578125
2016-12-17T19:05:33.074359: step 1521, loss 0.680691, acc 0.578125
2016-12-17T19:05:34.643177: step 1522, loss 0.659254, acc 0.59375
2016-12-17T19:05:36.160495: step 1523, loss 0.664128, acc 0.6875
2016-12-17T19:05:37.677867: step 1524, loss 0.584159, acc 0.734375
2016-12-17T19:05:39.204990: step 1525, loss 0.799853, acc 0.515625
2016-12-17T19:05:40.725240: step 1526, loss 0.724503, acc 0.5625
2016-12-17T19:05:42.233925: step 1527, loss 0.688655, acc 0.59375
2016-12-17T19:05:43.771291: step 1528, loss 0.708203, acc 0.53125
2016-12-17T19:05:45.320486: step 1529, loss 0.682984, acc 0.59375
2016-12-17T19:05:46.932054: step 1530, loss 0.721103, acc 0.4375
2016-12-17T19:05:48.483955: step 1531, loss 0.726025, acc 0.484375
2016-12-17T19:05:50.038243: step 1532, loss 0.741648, acc 0.421875
2016-12-17T19:05:51.608118: step 1533, loss 0.71781, acc 0.546875
2016-12-17T19:05:53.161557: step 1534, loss 0.679971, acc 0.5625
2016-12-17T19:05:54.722795: step 1535, loss 0.630176, acc 0.65625
2016-12-17T19:05:56.278672: step 1536, loss 0.636813, acc 0.625
2016-12-17T19:05:57.891940: step 1537, loss 0.681651, acc 0.640625
2016-12-17T19:05:59.468047: step 1538, loss 0.66265, acc 0.625
2016-12-17T19:06:01.089896: step 1539, loss 0.879619, acc 0.515625
2016-12-17T19:06:02.636649: step 1540, loss 0.654281, acc 0.609375
2016-12-17T19:06:04.244276: step 1541, loss 0.621362, acc 0.65625
2016-12-17T19:06:05.793749: step 1542, loss 0.664095, acc 0.625
2016-12-17T19:06:07.376905: step 1543, loss 0.717763, acc 0.5
2016-12-17T19:06:08.928781: step 1544, loss 0.628963, acc 0.609375
2016-12-17T19:06:10.429955: step 1545, loss 0.763871, acc 0.5
2016-12-17T19:06:11.964968: step 1546, loss 0.64116, acc 0.640625
2016-12-17T19:06:13.509722: step 1547, loss 0.625104, acc 0.6875
2016-12-17T19:06:15.054403: step 1548, loss 0.671697, acc 0.609375
2016-12-17T19:06:16.577541: step 1549, loss 0.817243, acc 0.515625
2016-12-17T19:06:18.102709: step 1550, loss 0.645366, acc 0.6875
2016-12-17T19:06:19.774545: step 1551, loss 0.62494, acc 0.65625
2016-12-17T19:06:21.329416: step 1552, loss 0.663196, acc 0.609375
2016-12-17T19:06:22.892943: step 1553, loss 0.658576, acc 0.53125
2016-12-17T19:06:24.406540: step 1554, loss 0.680942, acc 0.59375
2016-12-17T19:06:25.926230: step 1555, loss 0.699768, acc 0.546875
2016-12-17T19:06:27.445200: step 1556, loss 0.690411, acc 0.5625
2016-12-17T19:06:28.989416: step 1557, loss 0.743616, acc 0.5
2016-12-17T19:06:30.560467: step 1558, loss 0.652332, acc 0.609375
2016-12-17T19:06:32.076841: step 1559, loss 0.652225, acc 0.609375
2016-12-17T19:06:33.589683: step 1560, loss 0.621187, acc 0.65625
2016-12-17T19:06:35.095073: step 1561, loss 0.633785, acc 0.609375
2016-12-17T19:06:36.633803: step 1562, loss 0.685805, acc 0.625
2016-12-17T19:06:38.145005: step 1563, loss 0.726517, acc 0.609375
2016-12-17T19:06:39.690060: step 1564, loss 0.678031, acc 0.59375
2016-12-17T19:06:41.205424: step 1565, loss 0.68198, acc 0.65625
2016-12-17T19:06:42.663484: step 1566, loss 0.674711, acc 0.625
2016-12-17T19:06:44.165747: step 1567, loss 0.653486, acc 0.59375
2016-12-17T19:06:45.692255: step 1568, loss 0.659022, acc 0.59375
2016-12-17T19:06:47.229545: step 1569, loss 0.682066, acc 0.625
2016-12-17T19:06:48.722079: step 1570, loss 0.639299, acc 0.578125
2016-12-17T19:06:50.249890: step 1571, loss 0.682329, acc 0.578125
2016-12-17T19:06:51.886316: step 1572, loss 0.602215, acc 0.640625
2016-12-17T19:06:53.428376: step 1573, loss 0.656309, acc 0.625
2016-12-17T19:06:55.008449: step 1574, loss 0.618442, acc 0.703125
2016-12-17T19:06:56.565903: step 1575, loss 0.763443, acc 0.546875
2016-12-17T19:06:58.120425: step 1576, loss 0.68902, acc 0.609375
2016-12-17T19:06:59.628017: step 1577, loss 0.632231, acc 0.625
2016-12-17T19:07:01.209662: step 1578, loss 0.707856, acc 0.546875
2016-12-17T19:07:02.718384: step 1579, loss 0.637503, acc 0.65625
2016-12-17T19:07:04.262657: step 1580, loss 0.66435, acc 0.5625
2016-12-17T19:07:05.862515: step 1581, loss 0.640005, acc 0.609375
2016-12-17T19:07:07.416026: step 1582, loss 0.568332, acc 0.671875
2016-12-17T19:07:08.957319: step 1583, loss 0.61416, acc 0.65625
2016-12-17T19:07:10.495051: step 1584, loss 0.639784, acc 0.671875
2016-12-17T19:07:12.080672: step 1585, loss 0.598856, acc 0.734375
2016-12-17T19:07:13.630598: step 1586, loss 0.563243, acc 0.75
2016-12-17T19:07:15.187146: step 1587, loss 0.682983, acc 0.546875
2016-12-17T19:07:16.735497: step 1588, loss 0.65548, acc 0.625
2016-12-17T19:07:18.298915: step 1589, loss 0.676011, acc 0.625
2016-12-17T19:07:19.890900: step 1590, loss 0.601451, acc 0.703125
2016-12-17T19:07:21.473823: step 1591, loss 0.700719, acc 0.609375
2016-12-17T19:07:23.084169: step 1592, loss 0.631028, acc 0.625
2016-12-17T19:07:24.665324: step 1593, loss 0.668883, acc 0.59375
2016-12-17T19:07:26.239350: step 1594, loss 0.679644, acc 0.515625
2016-12-17T19:07:27.826296: step 1595, loss 0.646209, acc 0.640625
2016-12-17T19:07:29.388513: step 1596, loss 0.743598, acc 0.5
2016-12-17T19:07:30.990721: step 1597, loss 0.686626, acc 0.625
2016-12-17T19:07:32.575709: step 1598, loss 0.597287, acc 0.671875
2016-12-17T19:07:34.153734: step 1599, loss 0.623969, acc 0.640625
2016-12-17T19:07:35.676711: step 1600, loss 0.637442, acc 0.65625

Evaluation:
2016-12-17T19:07:51.231343: step 1600, loss 0.661524, acc 0.636408

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1482016882/checkpoints/model-1600

2016-12-17T19:07:54.738565: step 1601, loss 0.779266, acc 0.5625
2016-12-17T19:07:56.438225: step 1602, loss 0.630678, acc 0.671875
2016-12-17T19:07:57.996928: step 1603, loss 0.737367, acc 0.546875
2016-12-17T19:07:59.577421: step 1604, loss 0.643946, acc 0.609375
2016-12-17T19:08:01.127335: step 1605, loss 0.669344, acc 0.609375
2016-12-17T19:08:02.706317: step 1606, loss 0.732943, acc 0.453125
2016-12-17T19:08:04.288504: step 1607, loss 0.647571, acc 0.6875
2016-12-17T19:08:05.837003: step 1608, loss 0.752696, acc 0.453125
2016-12-17T19:08:07.447725: step 1609, loss 0.673289, acc 0.53125
2016-12-17T19:08:09.018106: step 1610, loss 0.672311, acc 0.5625
2016-12-17T19:08:10.527826: step 1611, loss 0.573248, acc 0.6875
2016-12-17T19:08:12.060679: step 1612, loss 0.633674, acc 0.640625
2016-12-17T19:08:13.606189: step 1613, loss 0.606993, acc 0.65625
2016-12-17T19:08:15.111565: step 1614, loss 0.617056, acc 0.71875
2016-12-17T19:08:16.594832: step 1615, loss 0.809315, acc 0.53125
2016-12-17T19:08:18.178871: step 1616, loss 0.72782, acc 0.59375
2016-12-17T19:08:19.753896: step 1617, loss 0.611651, acc 0.59375
2016-12-17T19:08:21.414924: step 1618, loss 0.593104, acc 0.6875
2016-12-17T19:08:23.123768: step 1619, loss 0.729821, acc 0.515625
2016-12-17T19:08:24.793911: step 1620, loss 0.664771, acc 0.609375
2016-12-17T19:08:26.420275: step 1621, loss 0.724469, acc 0.53125
2016-12-17T19:08:28.129047: step 1622, loss 0.722425, acc 0.515625
2016-12-17T19:08:29.737683: step 1623, loss 0.65174, acc 0.609375
2016-12-17T19:08:31.346129: step 1624, loss 0.560693, acc 0.75
2016-12-17T19:08:32.939181: step 1625, loss 0.669715, acc 0.625
2016-12-17T19:08:34.614141: step 1626, loss 0.611433, acc 0.671875
2016-12-17T19:08:36.204206: step 1627, loss 0.829355, acc 0.546875
2016-12-17T19:08:37.773688: step 1628, loss 0.590491, acc 0.6875
2016-12-17T19:08:39.366200: step 1629, loss 0.539765, acc 0.75
2016-12-17T19:08:40.954621: step 1630, loss 0.596149, acc 0.625
2016-12-17T19:08:42.513560: step 1631, loss 0.674645, acc 0.59375
2016-12-17T19:08:44.107931: step 1632, loss 0.626685, acc 0.671875
2016-12-17T19:08:45.682206: step 1633, loss 0.660073, acc 0.546875
2016-12-17T19:08:47.287547: step 1634, loss 0.66847, acc 0.609375
2016-12-17T19:08:48.868732: step 1635, loss 0.637097, acc 0.625
2016-12-17T19:08:50.845883: step 1636, loss 0.711024, acc 0.5625
2016-12-17T19:08:52.473831: step 1637, loss 0.698997, acc 0.53125
2016-12-17T19:08:54.137798: step 1638, loss 0.655327, acc 0.609375
2016-12-17T19:08:55.719594: step 1639, loss 0.70475, acc 0.546875
2016-12-17T19:08:57.351802: step 1640, loss 0.610609, acc 0.625
2016-12-17T19:08:59.049729: step 1641, loss 0.621839, acc 0.625
2016-12-17T19:09:00.785702: step 1642, loss 0.611248, acc 0.65625
2016-12-17T19:09:02.447269: step 1643, loss 0.733016, acc 0.640625
2016-12-17T19:09:03.997502: step 1644, loss 0.721917, acc 0.5625
2016-12-17T19:09:05.549955: step 1645, loss 0.732353, acc 0.515625
2016-12-17T19:09:07.155320: step 1646, loss 0.540968, acc 0.75
2016-12-17T19:09:08.846403: step 1647, loss 0.648172, acc 0.640625
2016-12-17T19:09:10.556253: step 1648, loss 0.711648, acc 0.53125
2016-12-17T19:09:12.159870: step 1649, loss 0.609937, acc 0.71875
2016-12-17T19:09:13.786302: step 1650, loss 0.607421, acc 0.671875
2016-12-17T19:09:15.395073: step 1651, loss 0.665581, acc 0.59375
2016-12-17T19:09:16.969486: step 1652, loss 0.674094, acc 0.578125
2016-12-17T19:09:18.521286: step 1653, loss 0.634019, acc 0.640625
2016-12-17T19:09:20.070802: step 1654, loss 0.805171, acc 0.484375
2016-12-17T19:09:21.629214: step 1655, loss 0.672915, acc 0.59375
2016-12-17T19:09:23.205472: step 1656, loss 0.633101, acc 0.625
2016-12-17T19:09:24.763479: step 1657, loss 0.65578, acc 0.578125
2016-12-17T19:09:26.388045: step 1658, loss 0.698682, acc 0.5625
2016-12-17T19:09:27.966545: step 1659, loss 0.700906, acc 0.53125
2016-12-17T19:09:29.563253: step 1660, loss 0.639285, acc 0.59375
2016-12-17T19:09:31.146860: step 1661, loss 0.673445, acc 0.53125
2016-12-17T19:09:32.753736: step 1662, loss 0.672522, acc 0.609375
2016-12-17T19:09:34.364735: step 1663, loss 0.653498, acc 0.625
2016-12-17T19:09:35.972924: step 1664, loss 0.613729, acc 0.65625
2016-12-17T19:09:37.512500: step 1665, loss 0.681137, acc 0.59375
2016-12-17T19:09:39.060280: step 1666, loss 0.673759, acc 0.609375
2016-12-17T19:09:40.630099: step 1667, loss 0.638665, acc 0.671875
2016-12-17T19:09:42.177885: step 1668, loss 0.562451, acc 0.71875
2016-12-17T19:09:43.683921: step 1669, loss 0.714475, acc 0.578125
2016-12-17T19:09:45.250059: step 1670, loss 0.698145, acc 0.609375
2016-12-17T19:09:46.801439: step 1671, loss 0.843895, acc 0.453125
2016-12-17T19:09:48.378983: step 1672, loss 0.691125, acc 0.578125
2016-12-17T19:09:50.050549: step 1673, loss 0.679232, acc 0.515625
2016-12-17T19:09:51.808858: step 1674, loss 0.657374, acc 0.578125
2016-12-17T19:09:53.524725: step 1675, loss 0.730484, acc 0.515625
2016-12-17T19:09:55.086797: step 1676, loss 0.633798, acc 0.640625
2016-12-17T19:09:56.675528: step 1677, loss 0.621783, acc 0.59375
2016-12-17T19:09:58.233549: step 1678, loss 0.679019, acc 0.65625
2016-12-17T19:09:59.777393: step 1679, loss 0.79273, acc 0.609375
2016-12-17T19:10:01.347861: step 1680, loss 0.698921, acc 0.65625
2016-12-17T19:10:02.938911: step 1681, loss 0.61492, acc 0.640625
2016-12-17T19:10:04.653596: step 1682, loss 0.574759, acc 0.640625
2016-12-17T19:10:06.263603: step 1683, loss 0.757613, acc 0.609375
2016-12-17T19:10:07.881767: step 1684, loss 0.575945, acc 0.71875
2016-12-17T19:10:09.444808: step 1685, loss 0.692388, acc 0.625
2016-12-17T19:10:11.001000: step 1686, loss 0.708819, acc 0.484375
2016-12-17T19:10:12.568491: step 1687, loss 0.669621, acc 0.625
2016-12-17T19:10:14.150751: step 1688, loss 0.6938, acc 0.546875
2016-12-17T19:10:15.713023: step 1689, loss 0.643006, acc 0.640625
2016-12-17T19:10:17.247708: step 1690, loss 0.674278, acc 0.484375
2016-12-17T19:10:18.814493: step 1691, loss 0.691693, acc 0.5
2016-12-17T19:10:20.363569: step 1692, loss 0.723556, acc 0.46875
2016-12-17T19:10:21.926616: step 1693, loss 0.738779, acc 0.5
2016-12-17T19:10:23.464055: step 1694, loss 0.643443, acc 0.625
2016-12-17T19:10:24.985996: step 1695, loss 0.581619, acc 0.640625
2016-12-17T19:10:26.546453: step 1696, loss 0.691043, acc 0.640625
2016-12-17T19:10:28.120950: step 1697, loss 0.648498, acc 0.671875
2016-12-17T19:10:29.649085: step 1698, loss 0.697467, acc 0.640625
2016-12-17T19:10:31.198017: step 1699, loss 0.752742, acc 0.609375
2016-12-17T19:10:32.785924: step 1700, loss 0.700159, acc 0.578125

Evaluation:
2016-12-17T19:10:49.204275: step 1700, loss 0.638808, acc 0.640291

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1482016882/checkpoints/model-1700

2016-12-17T19:10:52.899322: step 1701, loss 0.672027, acc 0.609375
2016-12-17T19:10:54.512426: step 1702, loss 0.719886, acc 0.546875
2016-12-17T19:10:56.087345: step 1703, loss 0.698525, acc 0.515625
2016-12-17T19:10:57.689161: step 1704, loss 0.656863, acc 0.625
2016-12-17T19:10:59.262803: step 1705, loss 0.662002, acc 0.609375
2016-12-17T19:11:00.866611: step 1706, loss 0.699315, acc 0.5625
2016-12-17T19:11:02.461901: step 1707, loss 0.663545, acc 0.59375
2016-12-17T19:11:04.090096: step 1708, loss 0.617082, acc 0.640625
2016-12-17T19:11:05.740816: step 1709, loss 0.652221, acc 0.640625
2016-12-17T19:11:07.372385: step 1710, loss 0.707894, acc 0.59375
2016-12-17T19:11:09.082822: step 1711, loss 0.691084, acc 0.578125
2016-12-17T19:11:10.745876: step 1712, loss 0.699503, acc 0.53125
2016-12-17T19:11:12.336056: step 1713, loss 0.642769, acc 0.65625
2016-12-17T19:11:13.898555: step 1714, loss 0.619821, acc 0.640625
2016-12-17T19:11:15.429341: step 1715, loss 0.651442, acc 0.625
2016-12-17T19:11:16.993537: step 1716, loss 0.670444, acc 0.578125
2016-12-17T19:11:18.573033: step 1717, loss 0.666276, acc 0.59375
2016-12-17T19:11:20.094229: step 1718, loss 0.595791, acc 0.65625
2016-12-17T19:11:21.644353: step 1719, loss 0.701991, acc 0.53125
2016-12-17T19:11:23.197799: step 1720, loss 0.737762, acc 0.578125
2016-12-17T19:11:24.739561: step 1721, loss 0.560452, acc 0.71875
2016-12-17T19:11:26.299491: step 1722, loss 0.623744, acc 0.703125
2016-12-17T19:11:27.842706: step 1723, loss 0.754441, acc 0.453125
2016-12-17T19:11:29.374324: step 1724, loss 0.663868, acc 0.625
2016-12-17T19:11:30.915099: step 1725, loss 0.61167, acc 0.703125
2016-12-17T19:11:32.462804: step 1726, loss 0.65122, acc 0.578125
2016-12-17T19:11:33.991636: step 1727, loss 0.636767, acc 0.609375
2016-12-17T19:11:35.555158: step 1728, loss 0.678436, acc 0.625
2016-12-17T19:11:37.105597: step 1729, loss 0.672954, acc 0.609375
2016-12-17T19:11:38.680132: step 1730, loss 0.69663, acc 0.609375
2016-12-17T19:11:40.189186: step 1731, loss 0.70893, acc 0.609375
2016-12-17T19:11:41.842026: step 1732, loss 0.622512, acc 0.625
2016-12-17T19:11:43.451554: step 1733, loss 0.657389, acc 0.625
2016-12-17T19:11:45.169103: step 1734, loss 0.646803, acc 0.609375
2016-12-17T19:11:46.849471: step 1735, loss 0.61586, acc 0.625
2016-12-17T19:11:48.456818: step 1736, loss 0.647457, acc 0.65625
2016-12-17T19:11:50.021444: step 1737, loss 0.683911, acc 0.578125
2016-12-17T19:11:51.614692: step 1738, loss 0.77552, acc 0.484375
2016-12-17T19:11:53.159460: step 1739, loss 0.632488, acc 0.625
2016-12-17T19:11:54.686633: step 1740, loss 0.707017, acc 0.515625
2016-12-17T19:11:56.199265: step 1741, loss 0.695653, acc 0.5625
2016-12-17T19:11:57.766010: step 1742, loss 0.6769, acc 0.609375
2016-12-17T19:11:59.344259: step 1743, loss 0.683655, acc 0.5
2016-12-17T19:12:00.907876: step 1744, loss 0.699825, acc 0.5625
2016-12-17T19:12:02.491238: step 1745, loss 0.692991, acc 0.53125
2016-12-17T19:12:04.036178: step 1746, loss 0.661455, acc 0.640625
2016-12-17T19:12:05.589026: step 1747, loss 0.622487, acc 0.65625
2016-12-17T19:12:07.153207: step 1748, loss 0.651738, acc 0.671875
2016-12-17T19:12:08.723382: step 1749, loss 0.656677, acc 0.640625
2016-12-17T19:12:10.281169: step 1750, loss 0.617172, acc 0.65625
2016-12-17T19:12:11.861561: step 1751, loss 0.664728, acc 0.640625
2016-12-17T19:12:13.532486: step 1752, loss 0.723121, acc 0.609375
2016-12-17T19:12:15.127527: step 1753, loss 0.737573, acc 0.578125
2016-12-17T19:12:16.703602: step 1754, loss 0.800056, acc 0.5
2016-12-17T19:12:18.266375: step 1755, loss 0.668659, acc 0.5625
2016-12-17T19:12:19.813767: step 1756, loss 0.690782, acc 0.46875
2016-12-17T19:12:21.381119: step 1757, loss 0.704708, acc 0.53125
2016-12-17T19:12:22.925533: step 1758, loss 0.73603, acc 0.515625
2016-12-17T19:12:24.457622: step 1759, loss 0.742784, acc 0.4375
2016-12-17T19:12:25.974069: step 1760, loss 0.664097, acc 0.609375
2016-12-17T19:12:27.506728: step 1761, loss 0.682138, acc 0.625
2016-12-17T19:12:29.036848: step 1762, loss 0.727337, acc 0.625
2016-12-17T19:12:30.544786: step 1763, loss 0.634667, acc 0.703125
2016-12-17T19:12:32.057071: step 1764, loss 0.778035, acc 0.578125
2016-12-17T19:12:33.568648: step 1765, loss 0.814975, acc 0.5625
2016-12-17T19:12:35.074889: step 1766, loss 0.631645, acc 0.640625
2016-12-17T19:12:36.574321: step 1767, loss 0.703809, acc 0.625
2016-12-17T19:12:38.079816: step 1768, loss 0.70618, acc 0.53125
2016-12-17T19:12:39.563019: step 1769, loss 0.738766, acc 0.5
2016-12-17T19:12:41.066306: step 1770, loss 0.815283, acc 0.390625
2016-12-17T19:12:42.576982: step 1771, loss 0.77356, acc 0.46875
2016-12-17T19:12:44.125529: step 1772, loss 0.708474, acc 0.59375
2016-12-17T19:12:45.757846: step 1773, loss 0.68901, acc 0.5625
2016-12-17T19:12:47.330903: step 1774, loss 0.651048, acc 0.640625
2016-12-17T19:12:48.884372: step 1775, loss 0.725957, acc 0.59375
2016-12-17T19:12:50.506504: step 1776, loss 0.772891, acc 0.546875
2016-12-17T19:12:52.171916: step 1777, loss 0.644951, acc 0.640625
2016-12-17T19:12:53.794833: step 1778, loss 0.622138, acc 0.703125
2016-12-17T19:12:55.341746: step 1779, loss 0.585204, acc 0.6875
2016-12-17T19:12:56.896439: step 1780, loss 0.715989, acc 0.53125
2016-12-17T19:12:58.475808: step 1781, loss 0.622719, acc 0.71875
2016-12-17T19:13:00.075616: step 1782, loss 0.729441, acc 0.421875
2016-12-17T19:13:01.676736: step 1783, loss 0.704043, acc 0.546875
2016-12-17T19:13:03.322172: step 1784, loss 0.641691, acc 0.609375
2016-12-17T19:13:04.993594: step 1785, loss 0.628258, acc 0.625
2016-12-17T19:13:06.588882: step 1786, loss 0.726051, acc 0.546875
2016-12-17T19:13:08.205345: step 1787, loss 0.695458, acc 0.640625
2016-12-17T19:13:09.769041: step 1788, loss 0.672747, acc 0.609375
2016-12-17T19:13:11.321910: step 1789, loss 0.724939, acc 0.59375
2016-12-17T19:13:12.881329: step 1790, loss 0.772009, acc 0.53125
2016-12-17T19:13:14.435814: step 1791, loss 0.663366, acc 0.578125
2016-12-17T19:13:16.003181: step 1792, loss 0.658969, acc 0.625
2016-12-17T19:13:17.687998: step 1793, loss 0.683953, acc 0.5625
2016-12-17T19:13:19.260062: step 1794, loss 0.688827, acc 0.515625
2016-12-17T19:13:20.826073: step 1795, loss 0.698211, acc 0.53125
2016-12-17T19:13:22.395470: step 1796, loss 0.705435, acc 0.5
2016-12-17T19:13:23.932864: step 1797, loss 0.691962, acc 0.578125
2016-12-17T19:13:25.488891: step 1798, loss 0.643479, acc 0.71875
2016-12-17T19:13:27.015090: step 1799, loss 0.624092, acc 0.65625
2016-12-17T19:13:28.559393: step 1800, loss 0.605824, acc 0.6875

Evaluation:
2016-12-17T19:13:42.990515: step 1800, loss 0.676962, acc 0.631553

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1482016882/checkpoints/model-1800

2016-12-17T19:13:46.594667: step 1801, loss 0.679623, acc 0.65625
2016-12-17T19:13:48.165100: step 1802, loss 0.520868, acc 0.765625
2016-12-17T19:13:49.807851: step 1803, loss 0.738614, acc 0.625
2016-12-17T19:13:51.409806: step 1804, loss 0.66609, acc 0.609375
2016-12-17T19:13:53.018672: step 1805, loss 0.678771, acc 0.6875
2016-12-17T19:13:54.569354: step 1806, loss 0.609624, acc 0.65625
2016-12-17T19:13:56.127412: step 1807, loss 0.676135, acc 0.578125
2016-12-17T19:13:57.685878: step 1808, loss 0.709779, acc 0.484375
2016-12-17T19:13:59.204291: step 1809, loss 0.696456, acc 0.5625
2016-12-17T19:14:00.765913: step 1810, loss 0.68187, acc 0.546875
2016-12-17T19:14:02.301817: step 1811, loss 0.685868, acc 0.578125
2016-12-17T19:14:03.818059: step 1812, loss 0.667194, acc 0.578125
2016-12-17T19:14:05.379395: step 1813, loss 0.666057, acc 0.578125
2016-12-17T19:14:06.930883: step 1814, loss 0.71219, acc 0.53125
2016-12-17T19:14:08.436219: step 1815, loss 0.628875, acc 0.65625
2016-12-17T19:14:09.995155: step 1816, loss 0.70237, acc 0.5625
2016-12-17T19:14:11.544351: step 1817, loss 0.657988, acc 0.671875
2016-12-17T19:14:13.091840: step 1818, loss 0.661684, acc 0.578125
2016-12-17T19:14:14.636191: step 1819, loss 0.611519, acc 0.703125
2016-12-17T19:14:16.187960: step 1820, loss 0.650571, acc 0.640625
2016-12-17T19:14:17.786976: step 1821, loss 0.676817, acc 0.5625
2016-12-17T19:14:19.295930: step 1822, loss 0.692689, acc 0.65625
2016-12-17T19:14:20.785576: step 1823, loss 0.696456, acc 0.578125
2016-12-17T19:14:22.399018: step 1824, loss 0.655235, acc 0.5625
2016-12-17T19:14:23.948764: step 1825, loss 0.617897, acc 0.703125
2016-12-17T19:14:25.492919: step 1826, loss 0.682455, acc 0.59375
2016-12-17T19:14:27.048082: step 1827, loss 0.665944, acc 0.609375
2016-12-17T19:14:28.614100: step 1828, loss 0.747057, acc 0.53125
2016-12-17T19:14:30.188915: step 1829, loss 0.670626, acc 0.59375
2016-12-17T19:14:31.766333: step 1830, loss 0.700828, acc 0.609375
2016-12-17T19:14:33.304907: step 1831, loss 0.654431, acc 0.625
2016-12-17T19:14:34.868110: step 1832, loss 0.701509, acc 0.546875
2016-12-17T19:14:36.492027: step 1833, loss 0.672152, acc 0.5625
2016-12-17T19:14:38.073706: step 1834, loss 0.703791, acc 0.59375
2016-12-17T19:14:39.645303: step 1835, loss 0.662033, acc 0.59375
2016-12-17T19:14:41.146255: step 1836, loss 0.629956, acc 0.625
2016-12-17T19:14:42.710484: step 1837, loss 0.580474, acc 0.734375
2016-12-17T19:14:44.235603: step 1838, loss 0.606995, acc 0.671875
2016-12-17T19:14:45.740561: step 1839, loss 0.688672, acc 0.703125
2016-12-17T19:14:47.295438: step 1840, loss 0.724569, acc 0.546875
2016-12-17T19:14:48.819217: step 1841, loss 0.636509, acc 0.625
2016-12-17T19:14:50.370707: step 1842, loss 0.607101, acc 0.609375
2016-12-17T19:14:51.885804: step 1843, loss 0.675428, acc 0.5625
2016-12-17T19:14:53.443932: step 1844, loss 0.661177, acc 0.578125
2016-12-17T19:14:55.083962: step 1845, loss 0.660046, acc 0.625
2016-12-17T19:14:56.695549: step 1846, loss 0.733326, acc 0.5
2016-12-17T19:14:58.254007: step 1847, loss 0.697984, acc 0.515625
2016-12-17T19:14:59.822187: step 1848, loss 0.683931, acc 0.53125
2016-12-17T19:15:01.384494: step 1849, loss 0.652236, acc 0.625
2016-12-17T19:15:02.910078: step 1850, loss 0.721091, acc 0.46875
2016-12-17T19:15:04.459917: step 1851, loss 0.686073, acc 0.578125
2016-12-17T19:15:05.998098: step 1852, loss 0.679486, acc 0.578125
2016-12-17T19:15:07.547110: step 1853, loss 0.605007, acc 0.703125
2016-12-17T19:15:09.080071: step 1854, loss 0.645371, acc 0.671875
2016-12-17T19:15:10.649754: step 1855, loss 0.632065, acc 0.671875
2016-12-17T19:15:12.194550: step 1856, loss 0.817008, acc 0.5
2016-12-17T19:15:13.728051: step 1857, loss 0.677966, acc 0.671875
2016-12-17T19:15:15.311851: step 1858, loss 0.757797, acc 0.546875
2016-12-17T19:15:16.906758: step 1859, loss 0.682825, acc 0.59375
2016-12-17T19:15:18.438553: step 1860, loss 0.656963, acc 0.578125
2016-12-17T19:15:20.023106: step 1861, loss 0.63462, acc 0.609375
2016-12-17T19:15:21.568876: step 1862, loss 0.685945, acc 0.625
2016-12-17T19:15:23.150311: step 1863, loss 0.632823, acc 0.671875
2016-12-17T19:15:24.669768: step 1864, loss 0.650419, acc 0.625
2016-12-17T19:15:26.292490: step 1865, loss 0.633002, acc 0.65625
2016-12-17T19:15:27.890091: step 1866, loss 0.664557, acc 0.625
2016-12-17T19:15:29.490409: step 1867, loss 0.775572, acc 0.46875
2016-12-17T19:15:31.073896: step 1868, loss 0.572978, acc 0.671875
2016-12-17T19:15:32.602321: step 1869, loss 0.750932, acc 0.5625
2016-12-17T19:15:34.122343: step 1870, loss 0.569171, acc 0.6875
2016-12-17T19:15:35.608149: step 1871, loss 0.720792, acc 0.5625
2016-12-17T19:15:37.153323: step 1872, loss 0.657237, acc 0.578125
2016-12-17T19:15:38.664628: step 1873, loss 0.654081, acc 0.6875
2016-12-17T19:15:40.191429: step 1874, loss 0.589499, acc 0.703125
2016-12-17T19:15:41.738909: step 1875, loss 0.698628, acc 0.5625
2016-12-17T19:15:43.257568: step 1876, loss 0.594093, acc 0.71875
2016-12-17T19:15:44.770704: step 1877, loss 0.591333, acc 0.75
2016-12-17T19:15:46.235889: step 1878, loss 0.713591, acc 0.578125
2016-12-17T19:15:47.752120: step 1879, loss 0.639312, acc 0.703125
2016-12-17T19:15:49.266878: step 1880, loss 0.625314, acc 0.59375
2016-12-17T19:15:50.807869: step 1881, loss 0.623975, acc 0.671875
2016-12-17T19:15:52.381752: step 1882, loss 0.708205, acc 0.5625
2016-12-17T19:15:53.920115: step 1883, loss 0.633843, acc 0.703125
2016-12-17T19:15:55.478482: step 1884, loss 0.598854, acc 0.65625
2016-12-17T19:15:57.059643: step 1885, loss 0.706686, acc 0.59375
2016-12-17T19:15:58.740482: step 1886, loss 0.609317, acc 0.671875
2016-12-17T19:16:00.361754: step 1887, loss 0.657981, acc 0.609375
2016-12-17T19:16:01.944830: step 1888, loss 0.580916, acc 0.75
2016-12-17T19:16:03.508094: step 1889, loss 0.666175, acc 0.59375
2016-12-17T19:16:05.082334: step 1890, loss 0.661072, acc 0.546875
2016-12-17T19:16:06.626185: step 1891, loss 0.684532, acc 0.625
2016-12-17T19:16:08.272659: step 1892, loss 0.716748, acc 0.5625
2016-12-17T19:16:09.900600: step 1893, loss 0.689461, acc 0.578125
2016-12-17T19:16:11.514274: step 1894, loss 0.646016, acc 0.65625
2016-12-17T19:16:13.216262: step 1895, loss 0.649463, acc 0.609375
2016-12-17T19:16:14.857599: step 1896, loss 0.685449, acc 0.53125
2016-12-17T19:16:16.388989: step 1897, loss 0.6592, acc 0.53125
2016-12-17T19:16:17.938124: step 1898, loss 0.641582, acc 0.625
2016-12-17T19:16:19.531642: step 1899, loss 0.632262, acc 0.625
2016-12-17T19:16:21.137249: step 1900, loss 0.668534, acc 0.65625

Evaluation:
2016-12-17T19:16:36.318415: step 1900, loss 0.654105, acc 0.636408

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1482016882/checkpoints/model-1900

2016-12-17T19:16:39.819352: step 1901, loss 0.685995, acc 0.65625
2016-12-17T19:16:41.384324: step 1902, loss 0.674845, acc 0.59375
2016-12-17T19:16:42.970179: step 1903, loss 0.569388, acc 0.765625
2016-12-17T19:16:44.483788: step 1904, loss 0.645736, acc 0.671875
2016-12-17T19:16:46.035650: step 1905, loss 0.690806, acc 0.578125
2016-12-17T19:16:47.540346: step 1906, loss 0.601278, acc 0.71875
2016-12-17T19:16:49.113568: step 1907, loss 0.591244, acc 0.6875
2016-12-17T19:16:50.621816: step 1908, loss 0.698015, acc 0.578125
2016-12-17T19:16:52.178548: step 1909, loss 0.611303, acc 0.671875
2016-12-17T19:16:53.746662: step 1910, loss 0.677002, acc 0.609375
2016-12-17T19:16:55.312993: step 1911, loss 0.713362, acc 0.578125
2016-12-17T19:16:56.851195: step 1912, loss 0.663362, acc 0.640625
2016-12-17T19:16:58.355201: step 1913, loss 0.675001, acc 0.578125
2016-12-17T19:16:59.849338: step 1914, loss 0.60465, acc 0.65625
2016-12-17T19:17:01.444971: step 1915, loss 0.660878, acc 0.640625
2016-12-17T19:17:03.015517: step 1916, loss 0.661913, acc 0.609375
2016-12-17T19:17:04.640997: step 1917, loss 0.655309, acc 0.671875
2016-12-17T19:17:06.460940: step 1918, loss 0.686431, acc 0.625
2016-12-17T19:17:08.162617: step 1919, loss 0.695354, acc 0.515625
2016-12-17T19:17:09.860174: step 1920, loss 0.614694, acc 0.671875
2016-12-17T19:17:11.441299: step 1921, loss 0.610899, acc 0.640625
2016-12-17T19:17:13.029518: step 1922, loss 0.615248, acc 0.65625
2016-12-17T19:17:14.626365: step 1923, loss 0.670773, acc 0.65625
2016-12-17T19:17:16.221404: step 1924, loss 0.647035, acc 0.59375
2016-12-17T19:17:17.859457: step 1925, loss 0.68251, acc 0.578125
2016-12-17T19:17:19.505651: step 1926, loss 0.637065, acc 0.609375
2016-12-17T19:17:21.200793: step 1927, loss 0.624286, acc 0.6875
2016-12-17T19:17:22.908833: step 1928, loss 0.688439, acc 0.53125
2016-12-17T19:17:24.455742: step 1929, loss 0.588088, acc 0.625
2016-12-17T19:17:26.072894: step 1930, loss 0.664249, acc 0.59375
2016-12-17T19:17:27.608654: step 1931, loss 0.681985, acc 0.578125
2016-12-17T19:17:29.223165: step 1932, loss 0.717524, acc 0.59375
2016-12-17T19:17:30.835881: step 1933, loss 0.718512, acc 0.5
2016-12-17T19:17:32.512268: step 1934, loss 0.660429, acc 0.640625
2016-12-17T19:17:34.158267: step 1935, loss 0.684846, acc 0.59375
2016-12-17T19:17:35.769175: step 1936, loss 0.732752, acc 0.40625
2016-12-17T19:17:37.387681: step 1937, loss 0.685804, acc 0.5
2016-12-17T19:17:39.148217: step 1938, loss 0.71204, acc 0.53125
2016-12-17T19:17:40.907380: step 1939, loss 0.647291, acc 0.59375
2016-12-17T19:17:42.525562: step 1940, loss 0.628861, acc 0.6875
2016-12-17T19:17:44.127260: step 1941, loss 0.663113, acc 0.625
2016-12-17T19:17:45.786558: step 1942, loss 0.668654, acc 0.640625
2016-12-17T19:17:47.405458: step 1943, loss 0.710497, acc 0.625
2016-12-17T19:17:49.046919: step 1944, loss 0.80593, acc 0.546875
2016-12-17T19:17:50.708694: step 1945, loss 0.692449, acc 0.625
2016-12-17T19:17:52.307514: step 1946, loss 0.659845, acc 0.640625
2016-12-17T19:17:54.029487: step 1947, loss 0.659177, acc 0.609375
2016-12-17T19:17:55.706297: step 1948, loss 0.679947, acc 0.59375
2016-12-17T19:17:57.336847: step 1949, loss 0.627172, acc 0.75
2016-12-17T19:17:58.970477: step 1950, loss 0.700553, acc 0.53125
2016-12-17T19:18:00.696254: step 1951, loss 0.702009, acc 0.515625
2016-12-17T19:18:02.451114: step 1952, loss 0.616549, acc 0.625
2016-12-17T19:18:04.149791: step 1953, loss 0.691129, acc 0.65625
2016-12-17T19:18:05.848249: step 1954, loss 0.616055, acc 0.6875
2016-12-17T19:18:07.531335: step 1955, loss 0.577942, acc 0.671875
2016-12-17T19:18:09.276344: step 1956, loss 0.716615, acc 0.609375
2016-12-17T19:18:11.145015: step 1957, loss 0.736709, acc 0.515625
2016-12-17T19:18:12.941231: step 1958, loss 0.662339, acc 0.609375
2016-12-17T19:18:14.558552: step 1959, loss 0.599356, acc 0.6875
2016-12-17T19:18:16.147149: step 1960, loss 0.608263, acc 0.65625
2016-12-17T19:18:17.739540: step 1961, loss 0.672078, acc 0.515625
2016-12-17T19:18:19.400355: step 1962, loss 0.68483, acc 0.546875
2016-12-17T19:18:21.095517: step 1963, loss 0.617539, acc 0.71875
2016-12-17T19:18:22.787626: step 1964, loss 0.663921, acc 0.625
2016-12-17T19:18:24.499257: step 1965, loss 0.696483, acc 0.515625
2016-12-17T19:18:26.159659: step 1966, loss 0.691486, acc 0.578125
2016-12-17T19:18:27.788001: step 1967, loss 0.650757, acc 0.59375
2016-12-17T19:18:29.401549: step 1968, loss 0.642596, acc 0.609375
2016-12-17T19:18:31.051223: step 1969, loss 0.653323, acc 0.609375
2016-12-17T19:18:32.746743: step 1970, loss 0.639245, acc 0.640625
2016-12-17T19:18:34.359303: step 1971, loss 0.733879, acc 0.53125
2016-12-17T19:18:36.023190: step 1972, loss 0.661523, acc 0.609375
2016-12-17T19:18:37.645930: step 1973, loss 0.662665, acc 0.578125
2016-12-17T19:18:39.305575: step 1974, loss 0.670656, acc 0.625
2016-12-17T19:18:40.951467: step 1975, loss 0.636681, acc 0.625
2016-12-17T19:18:42.647278: step 1976, loss 0.667462, acc 0.59375
2016-12-17T19:18:44.283991: step 1977, loss 0.712431, acc 0.484375
2016-12-17T19:18:45.820491: step 1978, loss 0.694867, acc 0.640625
2016-12-17T19:18:47.353172: step 1979, loss 0.651193, acc 0.671875
2016-12-17T19:18:48.884305: step 1980, loss 0.611039, acc 0.65625
2016-12-17T19:18:50.408883: step 1981, loss 0.69541, acc 0.578125
2016-12-17T19:18:51.939088: step 1982, loss 0.655786, acc 0.671875
2016-12-17T19:18:53.474601: step 1983, loss 0.666102, acc 0.625
2016-12-17T19:18:55.009220: step 1984, loss 0.66413, acc 0.5625
2016-12-17T19:18:56.509174: step 1985, loss 0.633015, acc 0.640625
2016-12-17T19:18:58.013421: step 1986, loss 0.721176, acc 0.5625
2016-12-17T19:18:59.534064: step 1987, loss 0.680628, acc 0.578125
2016-12-17T19:19:01.085273: step 1988, loss 0.627811, acc 0.546875
2016-12-17T19:19:02.585224: step 1989, loss 0.727845, acc 0.484375
2016-12-17T19:19:04.115327: step 1990, loss 0.747549, acc 0.4375
2016-12-17T19:19:05.646226: step 1991, loss 0.6763, acc 0.578125
2016-12-17T19:19:07.172056: step 1992, loss 0.707719, acc 0.5625
2016-12-17T19:19:08.731211: step 1993, loss 0.648759, acc 0.546875
2016-12-17T19:19:10.260448: step 1994, loss 0.650691, acc 0.546875
2016-12-17T19:19:11.812071: step 1995, loss 0.674496, acc 0.5625
2016-12-17T19:19:13.431502: step 1996, loss 0.628592, acc 0.640625
2016-12-17T19:19:15.223148: step 1997, loss 0.604928, acc 0.625
2016-12-17T19:19:16.887113: step 1998, loss 0.679645, acc 0.609375
2016-12-17T19:19:18.452283: step 1999, loss 0.731824, acc 0.515625
2016-12-17T19:19:19.995271: step 2000, loss 0.609826, acc 0.640625

Evaluation:
2016-12-17T19:19:35.432273: step 2000, loss 0.638561, acc 0.640777

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1482016882/checkpoints/model-2000
