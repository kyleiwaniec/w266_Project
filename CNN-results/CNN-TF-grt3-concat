python train.py --positive_data_file data/pos_data_train --negative_data_file data/neg_data_train --dev_sample_percentage .005

Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=64
CHECKPOINT_EVERY=100
DEV_SAMPLE_PERCENTAGE=0.005
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=128
EVALUATE_EVERY=100
FILTER_SIZES=3,4,5
L2_REG_LAMBDA=0.0
LOG_DEVICE_PLACEMENT=False
NEGATIVE_DATA_FILE=data/neg_data_train
NUM_EPOCHS=200
NUM_FILTERS=128
POSITIVE_DATA_FILE=data/pos_data_train

Loading data...
('l-positive_examples', 35244)
('l-negative_examples', 187973)
('type', 223217, 223217)
['whats', 'a', 'easy', 'way', 'to', 'memorize', 'this?', '1:', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '2:', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '3:', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '4:', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '5:', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '6:', '\n', '', '', '1x1=1', '', '', '', '', '', '', '', '', '', '', '', '', '', '2x1=2', '', '', '', '', '', '', '', '', '', '', '', '', '3x1=3', '', '', '', '', '', '', '', '', '', '', '', '', '4x1=4', '', '', '', '', '', '', '', '', '', '', '', '', '5x1=5', '', '', '', '', '', '', '', '', '', '', '', '', '6x1=6', '\n', '', '', '1x2=2', '', '', '', '', '', '', '', '', '', '', '', '', '', '2x2=4', '', '', '', '', '', '', '', '', '', '', '', '', '3x2=6', '', '', '', '', '', '', '', '', '', '', '', '', '4x2=8', '', '', '', '', '', '', '', '', '', '', '', '', '5x2=10', '', '', '', '', '', '', '', '', '', '', '6x2=12', '\n', '', '', '1x3=3', '', '', '', '', '', '', '', '', '', '', '', '', '', '2x3=6', '', '', '', '', '', '', '', '', '', '', '', '', '3x3=9', '', '', '', '', '', '', '', '', '', '', '', '', '4x3=12', '', '', '', '', '', '', '', '', '', '', '5x3=15', '', '', '', '', '', '', '', '', '', '', '6x3=18', '\n', '', '', '1x4=4', '', '', '', '', '', '', '', '', '', '', '', '', '', '2x4=8', '', '', '', '', '', '', '', '', '', '', '', '', '3x4=12', '', '', '', '', '', '', '', '', '', '', '4x4=16', '', '', '', '', '', '', '', '', '', '', '5x4=20', '', '', '', '', '', '', '', '', '', '', '6x4=24', '\n', '', '', '1x5=5', '', '', '', '', '', '', '', '', '', '', '', '', '', '2x5=10', '', '', '', '', '', '', '', '', '', '', '3x5=15', '', '', '', '', '', '', '', '', '', '', '4x5=20', '', '', '', '', '', '', '', '', '', '', '5x5=25', '', '', '', '', '', '', '', '', '', '', '6x5=30', '\n', '', '', '1x6=6', '', '', '', '', '', '', '', '', '', '', '', '', '', '2x6=12', '', '', '', '', '', '', '', '', '', '', '3x6=18', '', '', '', '', '', '', '', '', '', '', '4x6=24', '', '', '', '', '', '', '', '', '', '', '5x6=30', '', '', '', '', '', '', '', '', '', '', '6x6=36', '\n', '', '', '1x7=7', '', '', '', '', '', '', '', '', '', '', '', '', '', '2x7=14', '', '', '', '', '', '', '', '', '', '', '3x7=21', '', '', '', '', '', '', '', '', '', '', '4x7=28', '', '', '', '', '', '', '', '', '', '', '5x7=35', '', '', '', '', '', '', '', '', '', '', '6x7=42', '\n', '', '', '1x8=8', '', '', '', '', '', '', '', '', '', '', '', '', '', '2x8=16', '', '', '', '', '', '', '', '', '', '', '3x8=24', '', '', '', '', '', '', '', '', '', '', '4x8=32', '', '', '', '', '', '', '', '', '', '', '5x8=40', '', '', '', '', '', '', '', '', '', '', '6x8=48', '\n', '', '', '1x9=9', '', '', '', '', '', '', '', '', '', '', '', '', '', '2x9=18', '', '', '', '', '', '', '', '', '', '', '3x9=27', '', '', '', '', '', '', '', '', '', '', '4x9=36', '', '', '', '', '', '', '', '', '', '', '5x9=45', '', '', '', '', '', '', '', '', '', '', '6x9=54', '\n', '', '', '1x10=10', '', '', '', '', '', '', '', '', '2x10=20', '', '', '', '', '', '', '', '', '3x10=30', '', '', '', '', '', '', '', '', '4x10=40', '', '', '', '', '', '', '', '', '5x10=50', '', '', '', '', '', '', '', '', '6x10=60', '\n', '', '', '1x11=11', '', '', '', '', '', '', '', '', '2x11=22', '', '', '', '', '', '', '', '', '3x11=33', '', '', '', '', '', '', '', '', '4x11=44', '', '', '', '', '', '', '', '', '5x11=55', '', '', '', '', '', '', '', '', '6x11=66', '\n', '', '', '1x12=12', '', '', '', '', '', '', '', '', '2x12=24', '', '', '', '', '', '', '', '', '3x12=36', '', '', '', '', '', '', '', '', '4x12=48', '', '', '', '', '', '', '', '', '5x12=60', '', '', '', '', '', '', '', '', '6x12=72\n\n7:', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '8:', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '9:', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '10:', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '11:', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '12:', '\n', '', '7x1=7', '', '', '', '', '', '', '', '', '', '', '', '', '8x1=8', '', '', '', '', '', '', '', '', '', '', '', '', '9x1=9', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '10x1=10', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '11x1=11', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '12x1=12', '\n', '', '', '', '7x2=14', '', '', '', '', '', '', '', '', '', '8x2=16', '', '', '', '', '', '', '', '', '', '', '9x2=18', '', '', '', '', '', '', '', '', '', '', '', '', '', '10x2=20', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '11x2=22', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '12x2=24', '\n', '', '', '', '7x3=21', '', '', '', '', '', '', '', '', '', '8x3=24', '', '', '', '', '', '', '', '', '', '', '9x3=27', '', '', '', '', '', '', '', '', '', '', '', '', '', '10x3=30', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '11x3=33', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '12x3=36', '\n', '', '', '', '7x4=28', '', '', '', '', '', '', '', '', '', '8x4=32', '', '', '', '', '', '', '', '', '', '', '9x4=36', '', '', '', '', '', '', '', '', '', '', '', '', '', '10x4=40', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '11x4=44', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '12x4=48', '\n', '', '', '', '7x5=35', '', '', '', '', '', '', '', '', '', '8x5=40', '', '', '', '', '', '', '', '', '', '', '9x5=45', '', '', '', '', '', '', '', '', '', '', '', '', '', '10x5=50', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '11x5=55', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '12x5=60', '\n', '', '', '', '7x6=42', '', '', '', '', '', '', '', '', '', '8x6=48', '', '', '', '', '', '', '', '', '', '', '9x6=54', '', '', '', '', '', '', '', '', '', '', '', '', '', '10x6=60', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '11x6=66', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '12x6=72', '\n', '', '', '', '7x7=49', '', '', '', '', '', '', '', '', '', '8x7=56', '', '', '', '', '', '', '', '', '', '', '9x7=63', '', '', '', '', '', '', '', '', '', '', '', '', '', '10x7=70', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '11x7=77', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '12x7=84', '\n', '', '', '', '7x8=56', '', '', '', '', '', '', '', '', '', '8x8=64', '', '', '', '', '', '', '', '', '', '', '9x8=72', '', '', '', '', '', '', '', '', '', '', '', '', '', '10x8=80', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '11x8=88', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '12x8=96', '\n', '', '', '', '7x9=63', '', '', '', '', '', '', '', '', '', '8x9=72', '', '', '', '', '', '', '', '', '', '', '9x9=81', '', '', '', '', '', '', '', '', '', '', '', '', '', '10x9=90', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '11x9=99', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '12x9=108', '\n', '', '', '', '7x10=70', '', '', '', '', '', '', '', '8x10=80', '', '', '', '', '', '', '', '', '9x10=90', '', '', '', '', '', '', '', '', '', '', '10x10=100', '', '', '', '', '', '', '', '', '', '', '', '11x10=110', '', '', '', '', '', '', '', '', '', '', '12x10=120', '\n', '', '', '', '7x11=77', '', '', '', '', '', '', '', '8x11=88', '', '', '', '', '', '', '', '', '9x11=99', '', '', '', '', '', '', '', '', '', '', '10x11=110', '', '', '', '', '', '', '', '', '', '', '', '11x11=121', '', '', '', '', '', '', '', '', '', '', '12x11=132', '\n', '', '', '', '7x12=84', '', '', '', '', '', '', '', '8x12=96', '', '', '', '', '', '', '', '', '9x12=108', '', '', '', '', '', '', '', '', '10x12=120', '', '', '', '', '', '', '', '', '', '', '', '11x12=132', '', '', '', '', '', '', '', '', '', '', '12x12=144']
Build vocabulary...
('max_document_length...', 1720)
('x shape', (223217, 1720))
Split train/test set...
Vocabulary Size: 164035
Train/Dev split: 222101/1116
Writing to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481476583

2016-12-11T12:16:29.643441: step 1, loss 2.40021, acc 0.5625
2016-12-11T12:16:33.123539: step 2, loss 1.91835, acc 0.734375
2016-12-11T12:16:36.538594: step 3, loss 1.57577, acc 0.78125
2016-12-11T12:16:39.711183: step 4, loss 1.51483, acc 0.71875
2016-12-11T12:16:42.979017: step 5, loss 0.869053, acc 0.828125
2016-12-11T12:16:46.302450: step 6, loss 1.44414, acc 0.78125
2016-12-11T12:16:49.831210: step 7, loss 1.45054, acc 0.71875
2016-12-11T12:16:53.118944: step 8, loss 1.61424, acc 0.765625
2016-12-11T12:16:56.439284: step 9, loss 1.89245, acc 0.75
2016-12-11T12:16:59.688067: step 10, loss 1.0895, acc 0.671875
2016-12-11T12:17:03.085541: step 11, loss 1.02369, acc 0.703125
2016-12-11T12:17:06.643490: step 12, loss 1.23965, acc 0.75
2016-12-11T12:17:09.948564: step 13, loss 1.13168, acc 0.765625
2016-12-11T12:17:13.944349: step 14, loss 2.10966, acc 0.640625
2016-12-11T12:17:17.795568: step 15, loss 1.2854, acc 0.703125
2016-12-11T12:17:21.838041: step 16, loss 1.73646, acc 0.671875
2016-12-11T12:17:25.636673: step 17, loss 2.08049, acc 0.6875
2016-12-11T12:17:29.066966: step 18, loss 1.03744, acc 0.734375
2016-12-11T12:17:32.359631: step 19, loss 0.676859, acc 0.84375
2016-12-11T12:17:35.759856: step 20, loss 1.35385, acc 0.671875
2016-12-11T12:17:39.350625: step 21, loss 1.43194, acc 0.671875
2016-12-11T12:17:42.660496: step 22, loss 1.60676, acc 0.75
2016-12-11T12:17:46.104509: step 23, loss 0.782013, acc 0.8125
2016-12-11T12:17:49.528178: step 24, loss 0.813728, acc 0.859375
2016-12-11T12:17:52.870206: step 25, loss 1.13676, acc 0.78125
2016-12-11T12:17:56.398251: step 26, loss 1.38287, acc 0.703125
2016-12-11T12:17:59.813336: step 27, loss 1.56642, acc 0.65625
2016-12-11T12:18:03.677317: step 28, loss 1.41347, acc 0.8125
2016-12-11T12:18:07.530958: step 29, loss 1.79302, acc 0.625
2016-12-11T12:18:11.220273: step 30, loss 0.875064, acc 0.8125
2016-12-11T12:18:14.641257: step 31, loss 1.06745, acc 0.734375
2016-12-11T12:18:17.854959: step 32, loss 2.05252, acc 0.671875
2016-12-11T12:18:21.076643: step 33, loss 0.885358, acc 0.828125
2016-12-11T12:18:24.346740: step 34, loss 1.07962, acc 0.796875
2016-12-11T12:18:27.709591: step 35, loss 1.09722, acc 0.78125
2016-12-11T12:18:30.985558: step 36, loss 1.75251, acc 0.671875
2016-12-11T12:18:34.191633: step 37, loss 1.30463, acc 0.703125
2016-12-11T12:18:37.293681: step 38, loss 1.35377, acc 0.703125
2016-12-11T12:18:40.459213: step 39, loss 1.09734, acc 0.703125
2016-12-11T12:18:43.740962: step 40, loss 1.13685, acc 0.765625
2016-12-11T12:18:46.850545: step 41, loss 1.21135, acc 0.703125
2016-12-11T12:18:49.965949: step 42, loss 1.32844, acc 0.734375
2016-12-11T12:18:53.079019: step 43, loss 0.790734, acc 0.703125
2016-12-11T12:18:56.150086: step 44, loss 1.471, acc 0.734375
2016-12-11T12:18:59.321109: step 45, loss 1.4249, acc 0.75
2016-12-11T12:19:02.546972: step 46, loss 0.749755, acc 0.84375
2016-12-11T12:19:05.664653: step 47, loss 0.427347, acc 0.828125
2016-12-11T12:19:08.741907: step 48, loss 1.0933, acc 0.6875
2016-12-11T12:19:11.795093: step 49, loss 1.09187, acc 0.765625
2016-12-11T12:19:14.978328: step 50, loss 1.43595, acc 0.734375
2016-12-11T12:19:18.049643: step 51, loss 0.988226, acc 0.8125
2016-12-11T12:19:21.164285: step 52, loss 0.608518, acc 0.75
2016-12-11T12:19:24.171492: step 53, loss 1.43145, acc 0.734375
2016-12-11T12:19:27.171962: step 54, loss 1.9777, acc 0.71875
2016-12-11T12:19:30.229206: step 55, loss 1.43473, acc 0.796875
2016-12-11T12:19:33.349644: step 56, loss 1.33859, acc 0.765625
2016-12-11T12:19:36.451099: step 57, loss 1.1081, acc 0.71875
2016-12-11T12:19:39.448761: step 58, loss 1.31158, acc 0.71875
2016-12-11T12:19:42.479138: step 59, loss 0.681376, acc 0.78125
2016-12-11T12:19:45.570286: step 60, loss 1.32266, acc 0.75
2016-12-11T12:19:48.648962: step 61, loss 0.735334, acc 0.796875
2016-12-11T12:19:51.721096: step 62, loss 1.27926, acc 0.671875
2016-12-11T12:19:54.803346: step 63, loss 0.914313, acc 0.75
2016-12-11T12:19:57.876654: step 64, loss 1.25187, acc 0.703125
2016-12-11T12:20:00.922377: step 65, loss 0.872255, acc 0.765625
2016-12-11T12:20:04.112967: step 66, loss 0.5154, acc 0.90625
2016-12-11T12:20:07.346638: step 67, loss 0.790248, acc 0.859375
2016-12-11T12:20:10.495200: step 68, loss 1.5345, acc 0.765625
2016-12-11T12:20:13.662509: step 69, loss 1.81413, acc 0.671875
2016-12-11T12:20:16.826290: step 70, loss 1.01852, acc 0.796875
2016-12-11T12:20:19.941636: step 71, loss 1.99339, acc 0.640625
2016-12-11T12:20:23.063092: step 72, loss 0.902537, acc 0.84375
2016-12-11T12:20:26.177961: step 73, loss 1.15802, acc 0.796875
2016-12-11T12:20:29.284039: step 74, loss 1.47196, acc 0.640625
2016-12-11T12:20:32.330275: step 75, loss 1.48559, acc 0.703125
2016-12-11T12:20:35.458809: step 76, loss 0.710583, acc 0.796875
2016-12-11T12:20:38.689939: step 77, loss 1.05917, acc 0.765625
2016-12-11T12:20:41.816742: step 78, loss 0.897752, acc 0.78125
2016-12-11T12:20:44.947732: step 79, loss 1.15743, acc 0.703125
2016-12-11T12:20:48.083662: step 80, loss 1.13043, acc 0.703125
2016-12-11T12:20:51.178369: step 81, loss 1.07961, acc 0.75
2016-12-11T12:20:54.224625: step 82, loss 1.11538, acc 0.78125
2016-12-11T12:20:57.288788: step 83, loss 0.862475, acc 0.828125
2016-12-11T12:21:00.407649: step 84, loss 1.1496, acc 0.859375
2016-12-11T12:21:03.480683: step 85, loss 1.18193, acc 0.765625
2016-12-11T12:21:06.596548: step 86, loss 1.79246, acc 0.734375
2016-12-11T12:21:09.700015: step 87, loss 0.738804, acc 0.796875
2016-12-11T12:21:13.085582: step 88, loss 1.07517, acc 0.796875
2016-12-11T12:21:16.400046: step 89, loss 1.21746, acc 0.765625
2016-12-11T12:21:19.618020: step 90, loss 0.870492, acc 0.78125
2016-12-11T12:21:22.734161: step 91, loss 1.45336, acc 0.703125
2016-12-11T12:21:25.859281: step 92, loss 1.02758, acc 0.765625
2016-12-11T12:21:28.934066: step 93, loss 1.47063, acc 0.625
2016-12-11T12:21:31.915241: step 94, loss 1.0191, acc 0.734375
2016-12-11T12:21:35.068056: step 95, loss 0.8718, acc 0.71875
2016-12-11T12:21:38.222306: step 96, loss 1.19199, acc 0.75
2016-12-11T12:21:41.276542: step 97, loss 1.01623, acc 0.703125
2016-12-11T12:21:44.474303: step 98, loss 1.17085, acc 0.65625
2016-12-11T12:21:47.761072: step 99, loss 1.29634, acc 0.734375
2016-12-11T12:21:50.878279: step 100, loss 0.867163, acc 0.8125

Evaluation:
2016-12-11T12:22:17.027941: step 100, loss 0.82693, acc 0.841398

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481476583/checkpoints/model-100

2016-12-11T12:22:22.343944: step 101, loss 0.72684, acc 0.8125
2016-12-11T12:22:25.343498: step 102, loss 0.983257, acc 0.734375
2016-12-11T12:22:28.463379: step 103, loss 0.644458, acc 0.828125
2016-12-11T12:22:31.964789: step 104, loss 1.30122, acc 0.6875
2016-12-11T12:22:35.391190: step 105, loss 1.1202, acc 0.734375
2016-12-11T12:22:38.531754: step 106, loss 1.32328, acc 0.78125
2016-12-11T12:22:41.617834: step 107, loss 1.2165, acc 0.71875
2016-12-11T12:22:44.903618: step 108, loss 0.899675, acc 0.78125
2016-12-11T12:22:47.993336: step 109, loss 1.135, acc 0.734375
2016-12-11T12:22:51.218565: step 110, loss 1.14743, acc 0.734375
2016-12-11T12:22:55.104267: step 111, loss 1.2389, acc 0.734375
2016-12-11T12:22:58.792450: step 112, loss 0.820085, acc 0.78125
2016-12-11T12:23:02.528064: step 113, loss 1.25102, acc 0.6875
2016-12-11T12:23:06.289423: step 114, loss 0.810198, acc 0.828125
2016-12-11T12:23:09.687893: step 115, loss 1.1803, acc 0.765625
2016-12-11T12:23:13.092642: step 116, loss 0.950989, acc 0.78125
2016-12-11T12:23:16.132592: step 117, loss 0.77263, acc 0.84375
2016-12-11T12:23:19.208702: step 118, loss 1.58782, acc 0.8125
2016-12-11T12:23:22.287445: step 119, loss 1.0226, acc 0.8125
2016-12-11T12:23:25.572188: step 120, loss 1.44952, acc 0.71875
2016-12-11T12:23:28.682561: step 121, loss 0.904758, acc 0.828125
2016-12-11T12:23:31.712307: step 122, loss 0.928454, acc 0.8125
2016-12-11T12:23:34.819909: step 123, loss 1.17273, acc 0.703125
2016-12-11T12:23:37.883342: step 124, loss 1.31502, acc 0.640625
2016-12-11T12:23:40.966320: step 125, loss 1.23459, acc 0.65625
2016-12-11T12:23:43.991879: step 126, loss 1.30237, acc 0.640625
2016-12-11T12:23:47.002438: step 127, loss 0.869056, acc 0.671875
2016-12-11T12:23:50.119346: step 128, loss 1.1588, acc 0.6875
2016-12-11T12:23:53.206345: step 129, loss 0.455798, acc 0.875
2016-12-11T12:23:56.309260: step 130, loss 0.735375, acc 0.78125
2016-12-11T12:23:59.470035: step 131, loss 1.3889, acc 0.75
2016-12-11T12:24:02.533515: step 132, loss 0.871518, acc 0.859375
2016-12-11T12:24:05.584596: step 133, loss 1.35623, acc 0.765625
2016-12-11T12:24:08.634368: step 134, loss 0.960488, acc 0.84375
2016-12-11T12:24:11.718409: step 135, loss 0.902894, acc 0.828125
2016-12-11T12:24:14.770270: step 136, loss 0.989296, acc 0.8125
2016-12-11T12:24:17.832952: step 137, loss 1.19609, acc 0.796875
2016-12-11T12:24:20.922399: step 138, loss 0.46955, acc 0.859375
2016-12-11T12:24:23.943765: step 139, loss 1.05513, acc 0.8125
2016-12-11T12:24:27.026104: step 140, loss 0.662559, acc 0.828125
2016-12-11T12:24:30.295561: step 141, loss 1.13835, acc 0.6875
2016-12-11T12:24:33.328786: step 142, loss 0.935636, acc 0.75
2016-12-11T12:24:36.384238: step 143, loss 0.916013, acc 0.765625
2016-12-11T12:24:39.433755: step 144, loss 1.08168, acc 0.75
2016-12-11T12:24:42.467188: step 145, loss 1.00757, acc 0.6875
2016-12-11T12:24:45.551601: step 146, loss 1.192, acc 0.65625
2016-12-11T12:24:48.600330: step 147, loss 0.759008, acc 0.703125
2016-12-11T12:24:51.730670: step 148, loss 0.928685, acc 0.75
2016-12-11T12:24:54.750748: step 149, loss 0.828664, acc 0.78125
2016-12-11T12:24:57.813378: step 150, loss 1.12636, acc 0.71875
2016-12-11T12:25:00.951927: step 151, loss 0.891305, acc 0.828125
2016-12-11T12:25:04.100102: step 152, loss 0.940746, acc 0.765625
2016-12-11T12:25:07.171430: step 153, loss 1.32716, acc 0.75
2016-12-11T12:25:10.188497: step 154, loss 1.04348, acc 0.734375
2016-12-11T12:25:13.222566: step 155, loss 1.29803, acc 0.75
2016-12-11T12:25:16.309689: step 156, loss 0.971424, acc 0.78125
2016-12-11T12:25:19.468072: step 157, loss 0.942947, acc 0.765625
2016-12-11T12:25:22.466023: step 158, loss 0.725539, acc 0.734375
2016-12-11T12:25:25.462975: step 159, loss 1.03452, acc 0.6875
2016-12-11T12:25:28.538145: step 160, loss 0.746323, acc 0.78125
2016-12-11T12:25:31.618775: step 161, loss 0.85175, acc 0.78125
2016-12-11T12:25:34.856127: step 162, loss 0.67917, acc 0.6875
2016-12-11T12:25:37.974266: step 163, loss 1.14119, acc 0.734375
2016-12-11T12:25:41.001616: step 164, loss 0.831666, acc 0.796875
2016-12-11T12:25:44.051727: step 165, loss 0.891104, acc 0.78125
2016-12-11T12:25:47.081809: step 166, loss 0.976317, acc 0.734375
2016-12-11T12:25:50.179167: step 167, loss 1.11196, acc 0.75
2016-12-11T12:25:53.197228: step 168, loss 1.47501, acc 0.6875
2016-12-11T12:25:56.224119: step 169, loss 0.460913, acc 0.875
2016-12-11T12:25:59.289948: step 170, loss 0.889349, acc 0.734375
2016-12-11T12:26:02.384998: step 171, loss 0.793048, acc 0.6875
2016-12-11T12:26:05.514760: step 172, loss 0.903638, acc 0.6875
2016-12-11T12:26:08.660677: step 173, loss 1.15905, acc 0.625
2016-12-11T12:26:11.721544: step 174, loss 1.26274, acc 0.625
2016-12-11T12:26:14.783742: step 175, loss 0.721683, acc 0.734375
2016-12-11T12:26:17.886196: step 176, loss 1.21903, acc 0.71875
2016-12-11T12:26:20.973634: step 177, loss 0.725443, acc 0.765625
2016-12-11T12:26:24.064635: step 178, loss 1.06167, acc 0.6875
2016-12-11T12:26:27.049617: step 179, loss 0.6847, acc 0.734375
2016-12-11T12:26:30.128634: step 180, loss 0.736908, acc 0.734375
2016-12-11T12:26:33.226259: step 181, loss 0.800012, acc 0.796875
2016-12-11T12:26:36.390584: step 182, loss 1.35876, acc 0.671875
2016-12-11T12:26:39.610188: step 183, loss 0.744341, acc 0.8125
2016-12-11T12:26:42.774076: step 184, loss 0.985227, acc 0.75
2016-12-11T12:26:45.812790: step 185, loss 0.918804, acc 0.78125
2016-12-11T12:26:48.964132: step 186, loss 0.687421, acc 0.8125
2016-12-11T12:26:52.057995: step 187, loss 0.560014, acc 0.859375
2016-12-11T12:26:55.148476: step 188, loss 1.1147, acc 0.671875
2016-12-11T12:26:58.228364: step 189, loss 0.8193, acc 0.78125
2016-12-11T12:27:01.311354: step 190, loss 0.243542, acc 0.875
2016-12-11T12:27:04.437158: step 191, loss 1.266, acc 0.796875
2016-12-11T12:27:07.548972: step 192, loss 1.02362, acc 0.765625
2016-12-11T12:27:10.834270: step 193, loss 0.704455, acc 0.78125
2016-12-11T12:27:14.030484: step 194, loss 0.90639, acc 0.734375
2016-12-11T12:27:17.174174: step 195, loss 1.09253, acc 0.75
2016-12-11T12:27:20.267229: step 196, loss 0.879655, acc 0.796875
2016-12-11T12:27:23.314841: step 197, loss 0.7823, acc 0.8125
2016-12-11T12:27:26.352322: step 198, loss 0.803652, acc 0.765625
2016-12-11T12:27:29.480005: step 199, loss 0.870436, acc 0.75
2016-12-11T12:27:32.521016: step 200, loss 0.610103, acc 0.859375

Evaluation:
2016-12-11T12:27:56.477634: step 200, loss 0.514336, acc 0.840502

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481476583/checkpoints/model-200

2016-12-11T12:28:01.778688: step 201, loss 0.76596, acc 0.703125
2016-12-11T12:28:05.208790: step 202, loss 0.976484, acc 0.671875
2016-12-11T12:28:08.862408: step 203, loss 0.736383, acc 0.75
2016-12-11T12:28:12.456173: step 204, loss 0.737595, acc 0.703125
2016-12-11T12:28:15.699937: step 205, loss 0.562197, acc 0.8125
2016-12-11T12:28:18.921265: step 206, loss 0.755406, acc 0.75
2016-12-11T12:28:22.066955: step 207, loss 0.55504, acc 0.734375
2016-12-11T12:28:25.199987: step 208, loss 1.16809, acc 0.609375
2016-12-11T12:28:28.230698: step 209, loss 1.20767, acc 0.6875
2016-12-11T12:28:31.317743: step 210, loss 0.785825, acc 0.734375
2016-12-11T12:28:34.381682: step 211, loss 0.646064, acc 0.78125
2016-12-11T12:28:37.423070: step 212, loss 0.824314, acc 0.8125
2016-12-11T12:28:40.514745: step 213, loss 0.572868, acc 0.8125
2016-12-11T12:28:43.592466: step 214, loss 1.13552, acc 0.78125
2016-12-11T12:28:46.632601: step 215, loss 0.612694, acc 0.765625
2016-12-11T12:28:49.698813: step 216, loss 1.13586, acc 0.703125
2016-12-11T12:28:52.968909: step 217, loss 0.62239, acc 0.8125
2016-12-11T12:28:55.996602: step 218, loss 0.739273, acc 0.84375
2016-12-11T12:28:59.073293: step 219, loss 0.868671, acc 0.671875
2016-12-11T12:29:02.204272: step 220, loss 0.881431, acc 0.71875
2016-12-11T12:29:05.223641: step 221, loss 0.894506, acc 0.71875
2016-12-11T12:29:08.265947: step 222, loss 0.440715, acc 0.765625
2016-12-11T12:29:11.417464: step 223, loss 0.645499, acc 0.703125
2016-12-11T12:29:14.447853: step 224, loss 0.993378, acc 0.71875
2016-12-11T12:29:17.553619: step 225, loss 0.525864, acc 0.84375
2016-12-11T12:29:20.664223: step 226, loss 0.248468, acc 0.921875
2016-12-11T12:29:23.817790: step 227, loss 0.584659, acc 0.8125
2016-12-11T12:29:26.927331: step 228, loss 0.49103, acc 0.890625
2016-12-11T12:29:30.033294: step 229, loss 1.03573, acc 0.859375
2016-12-11T12:29:33.104574: step 230, loss 0.47501, acc 0.890625
2016-12-11T12:29:36.158821: step 231, loss 0.754252, acc 0.8125
2016-12-11T12:29:39.268274: step 232, loss 0.867605, acc 0.75
2016-12-11T12:29:42.333471: step 233, loss 1.00215, acc 0.796875
2016-12-11T12:29:45.408858: step 234, loss 1.17216, acc 0.78125
2016-12-11T12:29:48.509697: step 235, loss 0.657379, acc 0.8125
2016-12-11T12:29:51.612694: step 236, loss 0.783541, acc 0.75
2016-12-11T12:29:54.641947: step 237, loss 0.873522, acc 0.6875
2016-12-11T12:29:57.903983: step 238, loss 0.486529, acc 0.78125
2016-12-11T12:30:00.927431: step 239, loss 1.10467, acc 0.546875
2016-12-11T12:30:03.998652: step 240, loss 0.668351, acc 0.734375
2016-12-11T12:30:07.006704: step 241, loss 0.882326, acc 0.796875
2016-12-11T12:30:10.067232: step 242, loss 0.739078, acc 0.75
2016-12-11T12:30:13.128100: step 243, loss 0.472437, acc 0.8125
2016-12-11T12:30:16.175504: step 244, loss 0.234788, acc 0.90625
2016-12-11T12:30:19.290975: step 245, loss 0.992618, acc 0.796875
2016-12-11T12:30:22.506037: step 246, loss 0.692234, acc 0.859375
2016-12-11T12:30:25.501202: step 247, loss 0.623813, acc 0.84375
2016-12-11T12:30:28.621600: step 248, loss 0.626216, acc 0.828125
2016-12-11T12:30:31.845485: step 249, loss 0.688225, acc 0.859375
2016-12-11T12:30:34.880138: step 250, loss 0.512685, acc 0.859375
2016-12-11T12:30:37.881986: step 251, loss 0.514646, acc 0.8125
2016-12-11T12:30:40.934821: step 252, loss 0.639218, acc 0.84375
2016-12-11T12:30:44.066477: step 253, loss 0.731429, acc 0.8125
2016-12-11T12:30:47.066176: step 254, loss 0.603129, acc 0.796875
2016-12-11T12:30:50.117475: step 255, loss 0.94624, acc 0.640625
2016-12-11T12:30:53.175091: step 256, loss 0.714593, acc 0.6875
2016-12-11T12:30:56.210586: step 257, loss 0.904377, acc 0.6875
2016-12-11T12:30:59.295335: step 258, loss 0.685749, acc 0.6875
2016-12-11T12:31:02.622904: step 259, loss 0.717569, acc 0.796875
2016-12-11T12:31:05.666979: step 260, loss 0.718119, acc 0.78125
2016-12-11T12:31:08.723584: step 261, loss 0.658168, acc 0.859375
2016-12-11T12:31:11.764888: step 262, loss 0.530165, acc 0.859375
2016-12-11T12:31:14.831795: step 263, loss 0.764926, acc 0.8125
2016-12-11T12:31:17.902438: step 264, loss 0.782778, acc 0.828125
2016-12-11T12:31:20.902582: step 265, loss 0.48457, acc 0.859375
2016-12-11T12:31:23.938943: step 266, loss 0.58654, acc 0.8125
2016-12-11T12:31:26.964399: step 267, loss 1.13943, acc 0.671875
2016-12-11T12:31:30.039419: step 268, loss 0.514137, acc 0.84375
2016-12-11T12:31:33.151659: step 269, loss 0.653164, acc 0.828125
2016-12-11T12:31:36.270938: step 270, loss 0.521755, acc 0.796875
2016-12-11T12:31:39.378423: step 271, loss 0.600273, acc 0.796875
2016-12-11T12:31:42.375213: step 272, loss 0.935491, acc 0.71875
2016-12-11T12:31:45.420595: step 273, loss 0.814619, acc 0.734375
2016-12-11T12:31:48.552103: step 274, loss 0.717419, acc 0.796875
2016-12-11T12:31:51.587641: step 275, loss 0.79447, acc 0.8125
2016-12-11T12:31:54.624576: step 276, loss 0.992789, acc 0.703125
2016-12-11T12:31:57.653748: step 277, loss 0.610468, acc 0.828125
2016-12-11T12:32:00.769535: step 278, loss 0.654296, acc 0.796875
2016-12-11T12:32:03.833161: step 279, loss 0.575831, acc 0.78125
2016-12-11T12:32:07.113694: step 280, loss 0.866173, acc 0.6875
2016-12-11T12:32:10.162910: step 281, loss 0.728851, acc 0.765625
2016-12-11T12:32:13.178750: step 282, loss 1.1273, acc 0.625
2016-12-11T12:32:16.273335: step 283, loss 0.829635, acc 0.6875
2016-12-11T12:32:19.327647: step 284, loss 0.699678, acc 0.6875
2016-12-11T12:32:22.384931: step 285, loss 0.57382, acc 0.8125
2016-12-11T12:32:25.419043: step 286, loss 0.726604, acc 0.78125
2016-12-11T12:32:28.439741: step 287, loss 0.914047, acc 0.75
2016-12-11T12:32:31.516797: step 288, loss 0.81081, acc 0.765625
2016-12-11T12:32:34.667265: step 289, loss 0.482481, acc 0.875
2016-12-11T12:32:37.778493: step 290, loss 0.815395, acc 0.765625
2016-12-11T12:32:40.930111: step 291, loss 0.64026, acc 0.828125
2016-12-11T12:32:43.995978: step 292, loss 0.608142, acc 0.8125
2016-12-11T12:32:47.028057: step 293, loss 0.798219, acc 0.75
2016-12-11T12:32:50.089005: step 294, loss 0.738198, acc 0.734375
2016-12-11T12:32:53.098887: step 295, loss 0.510215, acc 0.796875
2016-12-11T12:32:56.107690: step 296, loss 0.604781, acc 0.75
2016-12-11T12:32:59.206454: step 297, loss 0.575915, acc 0.78125
2016-12-11T12:33:02.814093: step 298, loss 0.944827, acc 0.734375
2016-12-11T12:33:05.996508: step 299, loss 0.535193, acc 0.828125
2016-12-11T12:33:09.296440: step 300, loss 0.646122, acc 0.765625

Evaluation:
2016-12-11T12:33:32.196047: step 300, loss 0.554602, acc 0.841398

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481476583/checkpoints/model-300

2016-12-11T12:33:37.405121: step 301, loss 0.551898, acc 0.796875
2016-12-11T12:33:40.360346: step 302, loss 0.648607, acc 0.828125
2016-12-11T12:33:43.531239: step 303, loss 1.45475, acc 0.671875
2016-12-11T12:33:46.633106: step 304, loss 0.93958, acc 0.75
2016-12-11T12:33:49.751803: step 305, loss 0.569405, acc 0.828125
2016-12-11T12:33:52.887156: step 306, loss 0.642098, acc 0.703125
2016-12-11T12:33:55.958464: step 307, loss 0.564014, acc 0.84375
2016-12-11T12:33:58.949238: step 308, loss 0.608572, acc 0.734375
2016-12-11T12:34:01.957299: step 309, loss 0.726239, acc 0.703125
2016-12-11T12:34:05.048896: step 310, loss 0.862617, acc 0.734375
2016-12-11T12:34:08.087663: step 311, loss 0.720167, acc 0.78125
2016-12-11T12:34:11.122290: step 312, loss 0.520382, acc 0.8125
2016-12-11T12:34:14.264824: step 313, loss 0.753402, acc 0.734375
2016-12-11T12:34:17.477637: step 314, loss 0.641722, acc 0.78125
2016-12-11T12:34:20.568038: step 315, loss 0.711214, acc 0.75
2016-12-11T12:34:23.600615: step 316, loss 0.687935, acc 0.765625
2016-12-11T12:34:26.612910: step 317, loss 0.599131, acc 0.828125
2016-12-11T12:34:29.676395: step 318, loss 0.671807, acc 0.75
2016-12-11T12:34:32.699587: step 319, loss 0.441503, acc 0.828125
2016-12-11T12:34:35.706036: step 320, loss 0.584959, acc 0.765625
2016-12-11T12:34:38.801376: step 321, loss 0.494188, acc 0.859375
2016-12-11T12:34:41.800762: step 322, loss 0.528743, acc 0.734375
2016-12-11T12:34:44.865109: step 323, loss 0.578358, acc 0.75
2016-12-11T12:34:47.985501: step 324, loss 0.604049, acc 0.734375
2016-12-11T12:34:51.145338: step 325, loss 0.530311, acc 0.828125
2016-12-11T12:34:54.234959: step 326, loss 0.497239, acc 0.84375
2016-12-11T12:34:57.209333: step 327, loss 0.495202, acc 0.84375
2016-12-11T12:35:00.306957: step 328, loss 0.680157, acc 0.84375
2016-12-11T12:35:03.372318: step 329, loss 0.368767, acc 0.875
2016-12-11T12:35:06.410684: step 330, loss 0.530417, acc 0.875
2016-12-11T12:35:09.514803: step 331, loss 0.577078, acc 0.8125
2016-12-11T12:35:12.571219: step 332, loss 0.733028, acc 0.734375
2016-12-11T12:35:15.680763: step 333, loss 0.664751, acc 0.71875
2016-12-11T12:35:18.756131: step 334, loss 0.700322, acc 0.75
2016-12-11T12:35:22.032426: step 335, loss 0.768316, acc 0.71875
2016-12-11T12:35:25.089337: step 336, loss 0.62409, acc 0.75
2016-12-11T12:35:28.132562: step 337, loss 0.818834, acc 0.640625
2016-12-11T12:35:31.135473: step 338, loss 0.626311, acc 0.6875
2016-12-11T12:35:34.186460: step 339, loss 0.527205, acc 0.734375
2016-12-11T12:35:37.266753: step 340, loss 0.495232, acc 0.78125
2016-12-11T12:35:40.277088: step 341, loss 0.920955, acc 0.703125
2016-12-11T12:35:43.289357: step 342, loss 0.66804, acc 0.78125
2016-12-11T12:35:46.358190: step 343, loss 0.513696, acc 0.78125
2016-12-11T12:35:49.450968: step 344, loss 0.572989, acc 0.84375
2016-12-11T12:35:52.649011: step 345, loss 0.919303, acc 0.75
2016-12-11T12:35:55.773927: step 346, loss 0.635229, acc 0.8125
2016-12-11T12:35:58.816926: step 347, loss 0.703171, acc 0.75
2016-12-11T12:36:01.832013: step 348, loss 0.557327, acc 0.78125
2016-12-11T12:36:04.900253: step 349, loss 0.56945, acc 0.796875
2016-12-11T12:36:08.027403: step 350, loss 0.633984, acc 0.734375
2016-12-11T12:36:11.009612: step 351, loss 0.512355, acc 0.734375
2016-12-11T12:36:14.130822: step 352, loss 0.651822, acc 0.6875
2016-12-11T12:36:17.226527: step 353, loss 0.583149, acc 0.75
2016-12-11T12:36:20.227142: step 354, loss 0.756923, acc 0.734375
2016-12-11T12:36:23.270589: step 355, loss 0.750773, acc 0.6875
2016-12-11T12:36:26.548662: step 356, loss 0.321632, acc 0.90625
2016-12-11T12:36:29.626674: step 357, loss 0.480602, acc 0.828125
2016-12-11T12:36:32.676608: step 358, loss 1.16214, acc 0.609375
2016-12-11T12:36:35.656261: step 359, loss 0.540193, acc 0.8125
2016-12-11T12:36:38.674037: step 360, loss 0.463834, acc 0.859375
2016-12-11T12:36:41.753031: step 361, loss 0.501204, acc 0.796875
2016-12-11T12:36:44.892496: step 362, loss 0.422102, acc 0.765625
2016-12-11T12:36:47.938080: step 363, loss 0.513164, acc 0.796875
2016-12-11T12:36:50.965633: step 364, loss 0.580315, acc 0.8125
2016-12-11T12:36:54.059325: step 365, loss 0.402142, acc 0.859375
2016-12-11T12:36:57.174331: step 366, loss 0.617627, acc 0.765625
2016-12-11T12:37:00.342841: step 367, loss 0.391123, acc 0.875
2016-12-11T12:37:03.460495: step 368, loss 0.513079, acc 0.78125
2016-12-11T12:37:06.526532: step 369, loss 0.684681, acc 0.796875
2016-12-11T12:37:09.538669: step 370, loss 0.806638, acc 0.765625
2016-12-11T12:37:12.593603: step 371, loss 0.332546, acc 0.875
2016-12-11T12:37:15.656156: step 372, loss 0.516697, acc 0.8125
2016-12-11T12:37:18.834277: step 373, loss 0.563507, acc 0.796875
2016-12-11T12:37:21.892859: step 374, loss 0.548303, acc 0.828125
2016-12-11T12:37:24.900335: step 375, loss 0.410589, acc 0.84375
2016-12-11T12:37:28.031864: step 376, loss 0.585863, acc 0.796875
2016-12-11T12:37:31.249894: step 377, loss 0.535252, acc 0.78125
2016-12-11T12:37:34.361339: step 378, loss 0.721829, acc 0.71875
2016-12-11T12:37:37.383819: step 379, loss 0.500712, acc 0.765625
2016-12-11T12:37:40.470059: step 380, loss 0.436619, acc 0.84375
2016-12-11T12:37:43.558251: step 381, loss 0.526145, acc 0.734375
2016-12-11T12:37:46.573551: step 382, loss 0.559044, acc 0.765625
2016-12-11T12:37:49.703910: step 383, loss 0.476487, acc 0.78125
2016-12-11T12:37:52.818201: step 384, loss 0.613595, acc 0.734375
2016-12-11T12:37:55.895920: step 385, loss 0.510529, acc 0.84375
2016-12-11T12:37:59.010219: step 386, loss 0.588322, acc 0.828125
2016-12-11T12:38:02.482314: step 387, loss 0.721408, acc 0.859375
2016-12-11T12:38:06.375969: step 388, loss 0.86629, acc 0.75
2016-12-11T12:38:09.752800: step 389, loss 0.391302, acc 0.859375
2016-12-11T12:38:12.804673: step 390, loss 0.448679, acc 0.796875
2016-12-11T12:38:16.273443: step 391, loss 0.608006, acc 0.8125
2016-12-11T12:38:19.337420: step 392, loss 0.48142, acc 0.796875
2016-12-11T12:38:22.318623: step 393, loss 0.371443, acc 0.890625
2016-12-11T12:38:25.342122: step 394, loss 0.464935, acc 0.859375
2016-12-11T12:38:28.309835: step 395, loss 0.462741, acc 0.796875
2016-12-11T12:38:31.301387: step 396, loss 0.503757, acc 0.828125
2016-12-11T12:38:34.430510: step 397, loss 0.486521, acc 0.90625
2016-12-11T12:38:37.508098: step 398, loss 0.606497, acc 0.78125
2016-12-11T12:38:40.497172: step 399, loss 0.636292, acc 0.765625
2016-12-11T12:38:43.569842: step 400, loss 0.701441, acc 0.828125

Evaluation:
2016-12-11T12:39:07.231737: step 400, loss 0.446044, acc 0.841398

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481476583/checkpoints/model-400

2016-12-11T12:39:12.009596: step 401, loss 0.371465, acc 0.859375
2016-12-11T12:39:14.965561: step 402, loss 0.560664, acc 0.796875
2016-12-11T12:39:17.977373: step 403, loss 0.586641, acc 0.71875
2016-12-11T12:39:21.136842: step 404, loss 0.492463, acc 0.84375
2016-12-11T12:39:24.210095: step 405, loss 0.468517, acc 0.78125
2016-12-11T12:39:27.309087: step 406, loss 0.547427, acc 0.796875
2016-12-11T12:39:30.328700: step 407, loss 0.572856, acc 0.8125
2016-12-11T12:39:33.336634: step 408, loss 0.575418, acc 0.71875
2016-12-11T12:39:36.347298: step 409, loss 0.516531, acc 0.765625
2016-12-11T12:39:39.401137: step 410, loss 0.417731, acc 0.828125
2016-12-11T12:39:42.521401: step 411, loss 0.37464, acc 0.859375
2016-12-11T12:39:45.589428: step 412, loss 0.516727, acc 0.78125
2016-12-11T12:39:48.638396: step 413, loss 0.766504, acc 0.828125
2016-12-11T12:39:51.631288: step 414, loss 0.807513, acc 0.78125
2016-12-11T12:39:54.587790: step 415, loss 0.38555, acc 0.859375
2016-12-11T12:39:57.584649: step 416, loss 0.365288, acc 0.859375
2016-12-11T12:40:00.538035: step 417, loss 0.599375, acc 0.734375
2016-12-11T12:40:03.512744: step 418, loss 0.559331, acc 0.78125
2016-12-11T12:40:06.543350: step 419, loss 0.467229, acc 0.828125
2016-12-11T12:40:09.754740: step 420, loss 0.407448, acc 0.796875
2016-12-11T12:40:12.973556: step 421, loss 0.377689, acc 0.84375
2016-12-11T12:40:16.000239: step 422, loss 0.564141, acc 0.796875
2016-12-11T12:40:18.941886: step 423, loss 0.507351, acc 0.875
2016-12-11T12:40:21.955548: step 424, loss 0.633901, acc 0.765625
2016-12-11T12:40:24.994958: step 425, loss 0.522454, acc 0.84375
2016-12-11T12:40:28.075875: step 426, loss 0.523401, acc 0.828125
2016-12-11T12:40:31.032216: step 427, loss 0.513736, acc 0.796875
2016-12-11T12:40:34.137510: step 428, loss 0.535946, acc 0.8125
2016-12-11T12:40:37.213852: step 429, loss 0.413534, acc 0.84375
2016-12-11T12:40:40.315965: step 430, loss 0.631013, acc 0.734375
2016-12-11T12:40:43.283224: step 431, loss 0.545601, acc 0.78125
2016-12-11T12:40:46.479491: step 432, loss 0.574261, acc 0.765625
2016-12-11T12:40:49.553678: step 433, loss 0.478649, acc 0.8125
2016-12-11T12:40:52.509418: step 434, loss 0.80424, acc 0.78125
2016-12-11T12:40:55.520860: step 435, loss 0.435279, acc 0.84375
2016-12-11T12:40:58.520371: step 436, loss 0.451842, acc 0.875
2016-12-11T12:41:01.528357: step 437, loss 0.430006, acc 0.859375
2016-12-11T12:41:04.681172: step 438, loss 0.607686, acc 0.78125
2016-12-11T12:41:07.741668: step 439, loss 0.449436, acc 0.828125
2016-12-11T12:41:10.776179: step 440, loss 0.500294, acc 0.796875
2016-12-11T12:41:13.877646: step 441, loss 0.377904, acc 0.875
2016-12-11T12:41:16.981297: step 442, loss 0.671656, acc 0.765625
2016-12-11T12:41:20.101887: step 443, loss 0.399181, acc 0.875
2016-12-11T12:41:23.123435: step 444, loss 0.394561, acc 0.796875
2016-12-11T12:41:26.125092: step 445, loss 0.38992, acc 0.875
2016-12-11T12:41:29.137146: step 446, loss 0.588008, acc 0.765625
2016-12-11T12:41:32.112112: step 447, loss 0.499644, acc 0.8125
2016-12-11T12:41:35.100945: step 448, loss 0.432896, acc 0.828125
2016-12-11T12:41:37.985354: step 449, loss 0.65914, acc 0.765625
2016-12-11T12:41:40.991409: step 450, loss 0.436637, acc 0.796875
2016-12-11T12:41:44.006580: step 451, loss 0.457396, acc 0.796875
2016-12-11T12:41:46.946812: step 452, loss 0.511461, acc 0.765625
2016-12-11T12:41:50.117613: step 453, loss 0.416031, acc 0.796875
2016-12-11T12:41:53.149782: step 454, loss 0.477196, acc 0.796875
2016-12-11T12:41:56.141637: step 455, loss 0.439895, acc 0.84375
2016-12-11T12:41:59.150174: step 456, loss 0.366506, acc 0.875
2016-12-11T12:42:02.128133: step 457, loss 0.340018, acc 0.875
2016-12-11T12:42:05.140091: step 458, loss 0.38396, acc 0.890625
2016-12-11T12:42:08.149269: step 459, loss 0.674403, acc 0.75
2016-12-11T12:42:11.144576: step 460, loss 0.492279, acc 0.875
2016-12-11T12:42:14.203361: step 461, loss 0.495636, acc 0.8125
2016-12-11T12:42:17.179568: step 462, loss 0.504717, acc 0.84375
2016-12-11T12:42:20.179479: step 463, loss 0.379404, acc 0.84375
2016-12-11T12:42:23.339567: step 464, loss 0.3891, acc 0.828125
2016-12-11T12:42:26.369740: step 465, loss 0.496673, acc 0.828125
2016-12-11T12:42:29.307992: step 466, loss 0.422105, acc 0.859375
2016-12-11T12:42:32.260512: step 467, loss 0.294963, acc 0.90625
2016-12-11T12:42:35.260138: step 468, loss 0.33823, acc 0.890625
2016-12-11T12:42:38.206343: step 469, loss 0.480416, acc 0.828125
2016-12-11T12:42:41.169717: step 470, loss 0.524503, acc 0.875
2016-12-11T12:42:44.145893: step 471, loss 0.485096, acc 0.875
2016-12-11T12:42:47.123798: step 472, loss 0.612768, acc 0.828125
2016-12-11T12:42:50.122783: step 473, loss 0.510818, acc 0.859375
2016-12-11T12:42:53.168729: step 474, loss 0.499169, acc 0.8125
2016-12-11T12:42:56.313483: step 475, loss 0.414138, acc 0.84375
2016-12-11T12:42:59.305843: step 476, loss 0.53974, acc 0.796875
2016-12-11T12:43:02.761070: step 477, loss 0.454693, acc 0.765625
2016-12-11T12:43:06.125801: step 478, loss 0.475758, acc 0.828125
2016-12-11T12:43:09.284034: step 479, loss 0.526886, acc 0.734375
2016-12-11T12:43:12.650759: step 480, loss 0.579664, acc 0.671875
2016-12-11T12:43:15.733881: step 481, loss 0.684927, acc 0.71875
2016-12-11T12:43:18.703285: step 482, loss 0.591093, acc 0.78125
2016-12-11T12:43:21.626317: step 483, loss 0.463223, acc 0.875
2016-12-11T12:43:24.577944: step 484, loss 0.468604, acc 0.828125
2016-12-11T12:43:27.741508: step 485, loss 0.50229, acc 0.859375
2016-12-11T12:43:30.674396: step 486, loss 0.473512, acc 0.8125
2016-12-11T12:43:33.628748: step 487, loss 0.469456, acc 0.828125
2016-12-11T12:43:36.573854: step 488, loss 0.433335, acc 0.84375
2016-12-11T12:43:39.572395: step 489, loss 0.476083, acc 0.828125
2016-12-11T12:43:42.508708: step 490, loss 0.525556, acc 0.796875
2016-12-11T12:43:45.500081: step 491, loss 0.646086, acc 0.8125
2016-12-11T12:43:48.519416: step 492, loss 0.534289, acc 0.78125
2016-12-11T12:43:51.520909: step 493, loss 0.466783, acc 0.84375
2016-12-11T12:43:54.540235: step 494, loss 0.457118, acc 0.8125
2016-12-11T12:43:57.531539: step 495, loss 0.493358, acc 0.84375
2016-12-11T12:44:00.673991: step 496, loss 0.488086, acc 0.78125
2016-12-11T12:44:03.672749: step 497, loss 0.342672, acc 0.890625
2016-12-11T12:44:06.583636: step 498, loss 0.467168, acc 0.828125
2016-12-11T12:44:09.582069: step 499, loss 0.582533, acc 0.8125
2016-12-11T12:44:12.461521: step 500, loss 0.471601, acc 0.828125

Evaluation:
2016-12-11T12:44:36.117225: step 500, loss 0.432801, acc 0.841398

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481476583/checkpoints/model-500

2016-12-11T12:44:41.216394: step 501, loss 0.684892, acc 0.75
2016-12-11T12:44:44.167329: step 502, loss 0.564686, acc 0.8125
2016-12-11T12:44:47.080920: step 503, loss 0.522488, acc 0.84375
2016-12-11T12:44:50.177787: step 504, loss 0.39321, acc 0.84375
2016-12-11T12:44:53.223298: step 505, loss 0.555901, acc 0.75
2016-12-11T12:44:56.299988: step 506, loss 0.36868, acc 0.90625
2016-12-11T12:44:59.324130: step 507, loss 0.457947, acc 0.8125
2016-12-11T12:45:02.329551: step 508, loss 0.558836, acc 0.734375
2016-12-11T12:45:05.370068: step 509, loss 0.411425, acc 0.875
2016-12-11T12:45:08.448743: step 510, loss 0.393985, acc 0.84375
2016-12-11T12:45:11.518268: step 511, loss 0.335365, acc 0.90625
2016-12-11T12:45:14.527470: step 512, loss 0.366446, acc 0.859375
2016-12-11T12:45:17.471639: step 513, loss 0.400273, acc 0.875
2016-12-11T12:45:20.494237: step 514, loss 0.622682, acc 0.734375
2016-12-11T12:45:23.497441: step 515, loss 0.66136, acc 0.765625
2016-12-11T12:45:26.481318: step 516, loss 0.378658, acc 0.84375
2016-12-11T12:45:29.424080: step 517, loss 0.550448, acc 0.75
2016-12-11T12:45:32.377022: step 518, loss 0.350898, acc 0.875
2016-12-11T12:45:35.327557: step 519, loss 0.548884, acc 0.8125
2016-12-11T12:45:38.294227: step 520, loss 0.355891, acc 0.84375
2016-12-11T12:45:41.476368: step 521, loss 0.288334, acc 0.921875
2016-12-11T12:45:44.482120: step 522, loss 0.574607, acc 0.8125
2016-12-11T12:45:47.507186: step 523, loss 0.384388, acc 0.890625
2016-12-11T12:45:50.521504: step 524, loss 0.459104, acc 0.859375
2016-12-11T12:45:53.574113: step 525, loss 0.70399, acc 0.78125
2016-12-11T12:45:56.584276: step 526, loss 0.474356, acc 0.84375
2016-12-11T12:45:59.528840: step 527, loss 0.506872, acc 0.8125
2016-12-11T12:46:02.532348: step 528, loss 0.458068, acc 0.828125
2016-12-11T12:46:05.440291: step 529, loss 0.548968, acc 0.75
2016-12-11T12:46:08.446670: step 530, loss 0.613073, acc 0.65625
2016-12-11T12:46:11.357293: step 531, loss 0.470769, acc 0.75
2016-12-11T12:46:14.608939: step 532, loss 0.374022, acc 0.890625
2016-12-11T12:46:17.597205: step 533, loss 0.580051, acc 0.796875
2016-12-11T12:46:20.581918: step 534, loss 0.376726, acc 0.890625
2016-12-11T12:46:23.587386: step 535, loss 0.648768, acc 0.796875
2016-12-11T12:46:26.594117: step 536, loss 0.503331, acc 0.84375
2016-12-11T12:46:29.562138: step 537, loss 0.367574, acc 0.859375
2016-12-11T12:46:32.509742: step 538, loss 0.736212, acc 0.734375
2016-12-11T12:46:35.547562: step 539, loss 0.440277, acc 0.84375
2016-12-11T12:46:38.570831: step 540, loss 0.487944, acc 0.8125
2016-12-11T12:46:41.523072: step 541, loss 0.500098, acc 0.78125
2016-12-11T12:46:44.508676: step 542, loss 0.519264, acc 0.75
2016-12-11T12:46:47.673831: step 543, loss 0.475469, acc 0.84375
2016-12-11T12:46:50.619457: step 544, loss 0.469416, acc 0.84375
2016-12-11T12:46:53.605693: step 545, loss 0.388215, acc 0.875
2016-12-11T12:46:56.533321: step 546, loss 0.452215, acc 0.84375
2016-12-11T12:46:59.476449: step 547, loss 0.419707, acc 0.875
2016-12-11T12:47:02.471381: step 548, loss 0.591726, acc 0.8125
2016-12-11T12:47:05.506473: step 549, loss 0.404993, acc 0.875
2016-12-11T12:47:08.531341: step 550, loss 0.392301, acc 0.859375
2016-12-11T12:47:11.502576: step 551, loss 0.488813, acc 0.828125
2016-12-11T12:47:14.518428: step 552, loss 0.400641, acc 0.84375
2016-12-11T12:47:17.527933: step 553, loss 0.46079, acc 0.734375
2016-12-11T12:47:20.611789: step 554, loss 0.461927, acc 0.796875
2016-12-11T12:47:23.679527: step 555, loss 0.487642, acc 0.796875
2016-12-11T12:47:26.644041: step 556, loss 0.524424, acc 0.828125
2016-12-11T12:47:29.602192: step 557, loss 0.47217, acc 0.828125
2016-12-11T12:47:32.484795: step 558, loss 0.521188, acc 0.796875
2016-12-11T12:47:35.451185: step 559, loss 0.496683, acc 0.84375
2016-12-11T12:47:38.425957: step 560, loss 0.45188, acc 0.828125
2016-12-11T12:47:41.399916: step 561, loss 0.319993, acc 0.890625
2016-12-11T12:47:44.397948: step 562, loss 0.31885, acc 0.90625
2016-12-11T12:47:47.348889: step 563, loss 0.461901, acc 0.84375
2016-12-11T12:47:50.439426: step 564, loss 0.358407, acc 0.890625
2016-12-11T12:47:53.621700: step 565, loss 0.442392, acc 0.828125
2016-12-11T12:47:56.616663: step 566, loss 0.465026, acc 0.828125
2016-12-11T12:47:59.631481: step 567, loss 0.544885, acc 0.8125
2016-12-11T12:48:03.045139: step 568, loss 0.455014, acc 0.828125
2016-12-11T12:48:06.324307: step 569, loss 0.539373, acc 0.8125
2016-12-11T12:48:09.553507: step 570, loss 0.340273, acc 0.875
2016-12-11T12:48:12.745994: step 571, loss 0.39138, acc 0.890625
2016-12-11T12:48:15.761535: step 572, loss 0.493004, acc 0.828125
2016-12-11T12:48:18.698132: step 573, loss 0.322644, acc 0.921875
2016-12-11T12:48:21.684624: step 574, loss 0.602304, acc 0.796875
2016-12-11T12:48:24.866710: step 575, loss 0.548135, acc 0.828125
2016-12-11T12:48:27.848641: step 576, loss 0.681795, acc 0.78125
2016-12-11T12:48:30.725645: step 577, loss 0.380355, acc 0.890625
2016-12-11T12:48:33.648745: step 578, loss 0.423143, acc 0.84375
2016-12-11T12:48:36.717842: step 579, loss 0.456828, acc 0.828125
2016-12-11T12:48:39.776904: step 580, loss 0.411165, acc 0.84375
2016-12-11T12:48:42.730583: step 581, loss 0.283489, acc 0.9375
2016-12-11T12:48:45.688571: step 582, loss 0.511544, acc 0.78125
2016-12-11T12:48:48.694884: step 583, loss 0.548727, acc 0.78125
2016-12-11T12:48:51.678498: step 584, loss 0.466884, acc 0.828125
2016-12-11T12:48:54.680750: step 585, loss 0.277311, acc 0.9375
2016-12-11T12:48:57.840458: step 586, loss 0.337582, acc 0.90625
2016-12-11T12:49:00.906129: step 587, loss 0.422182, acc 0.875
2016-12-11T12:49:03.928652: step 588, loss 0.61453, acc 0.765625
2016-12-11T12:49:07.009491: step 589, loss 0.381392, acc 0.859375
2016-12-11T12:49:10.009516: step 590, loss 0.580682, acc 0.78125
2016-12-11T12:49:13.045142: step 591, loss 0.333641, acc 0.90625
2016-12-11T12:49:16.104057: step 592, loss 0.483192, acc 0.859375
2016-12-11T12:49:19.097509: step 593, loss 0.431294, acc 0.859375
2016-12-11T12:49:22.031283: step 594, loss 0.468652, acc 0.859375
2016-12-11T12:49:24.992923: step 595, loss 0.322983, acc 0.875
2016-12-11T12:49:28.040257: step 596, loss 0.399599, acc 0.828125
2016-12-11T12:49:31.118867: step 597, loss 0.40022, acc 0.859375
2016-12-11T12:49:34.077750: step 598, loss 0.44571, acc 0.828125
2016-12-11T12:49:37.112075: step 599, loss 0.625466, acc 0.796875
2016-12-11T12:49:40.193023: step 600, loss 0.514336, acc 0.796875

Evaluation:
2016-12-11T12:50:01.085860: step 600, loss 0.42359, acc 0.841398

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481476583/checkpoints/model-600

2016-12-11T12:50:05.856647: step 601, loss 0.554281, acc 0.8125
2016-12-11T12:50:08.747408: step 602, loss 0.55388, acc 0.8125
2016-12-11T12:50:11.739132: step 603, loss 0.449412, acc 0.828125
2016-12-11T12:50:14.824063: step 604, loss 0.472335, acc 0.796875
2016-12-11T12:50:17.844831: step 605, loss 0.543352, acc 0.796875
2016-12-11T12:50:20.866879: step 606, loss 0.338826, acc 0.90625
2016-12-11T12:50:23.934683: step 607, loss 0.398486, acc 0.890625
2016-12-11T12:50:26.983483: step 608, loss 0.535724, acc 0.8125
2016-12-11T12:50:30.025370: step 609, loss 0.507208, acc 0.8125
2016-12-11T12:50:33.082507: step 610, loss 0.42736, acc 0.84375
2016-12-11T12:50:36.177534: step 611, loss 0.390412, acc 0.90625
2016-12-11T12:50:39.164904: step 612, loss 0.47503, acc 0.828125
2016-12-11T12:50:42.095225: step 613, loss 0.556333, acc 0.765625
2016-12-11T12:50:45.039502: step 614, loss 0.611084, acc 0.78125
2016-12-11T12:50:48.043314: step 615, loss 0.549698, acc 0.75
2016-12-11T12:50:50.963662: step 616, loss 0.513658, acc 0.796875
2016-12-11T12:50:54.027812: step 617, loss 0.52091, acc 0.734375
2016-12-11T12:50:57.042789: step 618, loss 0.4342, acc 0.8125
2016-12-11T12:50:59.986883: step 619, loss 0.469949, acc 0.84375
2016-12-11T12:51:03.061431: step 620, loss 0.492961, acc 0.796875
2016-12-11T12:51:06.079711: step 621, loss 0.553465, acc 0.796875
2016-12-11T12:51:09.218868: step 622, loss 0.620921, acc 0.796875
2016-12-11T12:51:12.298132: step 623, loss 0.533776, acc 0.8125
2016-12-11T12:51:15.348910: step 624, loss 0.478231, acc 0.84375
2016-12-11T12:51:18.311952: step 625, loss 0.435732, acc 0.859375
2016-12-11T12:51:21.320831: step 626, loss 0.420604, acc 0.84375
2016-12-11T12:51:24.315048: step 627, loss 0.451842, acc 0.8125
2016-12-11T12:51:27.315002: step 628, loss 0.495171, acc 0.8125
2016-12-11T12:51:30.240368: step 629, loss 0.499397, acc 0.859375
2016-12-11T12:51:33.179009: step 630, loss 0.313265, acc 0.890625
2016-12-11T12:51:36.088889: step 631, loss 0.523463, acc 0.8125
2016-12-11T12:51:39.123359: step 632, loss 0.351215, acc 0.875
2016-12-11T12:51:42.156583: step 633, loss 0.363881, acc 0.890625
2016-12-11T12:51:45.168630: step 634, loss 0.426622, acc 0.84375
2016-12-11T12:51:48.141988: step 635, loss 0.362421, acc 0.890625
2016-12-11T12:51:51.161031: step 636, loss 0.733915, acc 0.765625
2016-12-11T12:51:54.188713: step 637, loss 0.398879, acc 0.875
2016-12-11T12:51:57.141425: step 638, loss 0.549675, acc 0.796875
2016-12-11T12:52:00.145070: step 639, loss 0.427857, acc 0.859375
2016-12-11T12:52:03.175411: step 640, loss 0.455317, acc 0.8125
2016-12-11T12:52:06.108032: step 641, loss 0.316458, acc 0.921875
2016-12-11T12:52:09.036005: step 642, loss 0.406023, acc 0.90625
2016-12-11T12:52:12.155450: step 643, loss 0.313345, acc 0.90625
2016-12-11T12:52:15.139191: step 644, loss 0.592829, acc 0.796875
2016-12-11T12:52:18.117405: step 645, loss 0.441345, acc 0.875
2016-12-11T12:52:21.078326: step 646, loss 0.442039, acc 0.890625
2016-12-11T12:52:24.064096: step 647, loss 0.565644, acc 0.828125
2016-12-11T12:52:27.089505: step 648, loss 0.358784, acc 0.875
2016-12-11T12:52:30.034035: step 649, loss 0.47863, acc 0.84375
2016-12-11T12:52:33.003272: step 650, loss 0.551165, acc 0.765625
2016-12-11T12:52:35.943163: step 651, loss 0.486299, acc 0.796875
2016-12-11T12:52:38.892697: step 652, loss 0.491351, acc 0.796875
2016-12-11T12:52:41.865788: step 653, loss 0.51675, acc 0.796875
2016-12-11T12:52:45.002984: step 654, loss 0.41909, acc 0.78125
2016-12-11T12:52:47.969052: step 655, loss 0.537724, acc 0.734375
2016-12-11T12:52:50.911750: step 656, loss 0.388539, acc 0.890625
2016-12-11T12:52:53.847487: step 657, loss 0.417854, acc 0.84375
2016-12-11T12:52:56.797026: step 658, loss 0.382748, acc 0.890625
2016-12-11T12:52:59.789737: step 659, loss 0.228717, acc 0.9375
2016-12-11T12:53:03.275189: step 660, loss 0.492293, acc 0.84375
2016-12-11T12:53:06.520653: step 661, loss 0.770539, acc 0.734375
2016-12-11T12:53:09.614125: step 662, loss 0.342504, acc 0.890625
2016-12-11T12:53:12.919649: step 663, loss 0.463804, acc 0.828125
2016-12-11T12:53:15.982228: step 664, loss 0.42366, acc 0.828125
2016-12-11T12:53:19.010058: step 665, loss 0.521446, acc 0.8125
2016-12-11T12:53:22.004673: step 666, loss 0.443492, acc 0.859375
2016-12-11T12:53:24.897321: step 667, loss 0.406037, acc 0.859375
2016-12-11T12:53:27.829356: step 668, loss 0.465267, acc 0.828125
2016-12-11T12:53:30.952707: step 669, loss 0.472048, acc 0.828125
2016-12-11T12:53:33.918939: step 670, loss 0.580882, acc 0.796875
2016-12-11T12:53:36.871026: step 671, loss 0.686213, acc 0.765625
2016-12-11T12:53:40.007262: step 672, loss 0.445563, acc 0.8125
2016-12-11T12:53:43.117924: step 673, loss 0.26825, acc 0.921875
2016-12-11T12:53:46.084878: step 674, loss 0.425071, acc 0.84375
2016-12-11T12:53:49.149911: step 675, loss 0.306438, acc 0.9375
2016-12-11T12:53:52.129475: step 676, loss 0.46289, acc 0.84375
2016-12-11T12:53:55.109897: step 677, loss 0.362292, acc 0.890625
2016-12-11T12:53:58.165269: step 678, loss 0.692084, acc 0.734375
2016-12-11T12:54:01.112362: step 679, loss 0.412009, acc 0.84375
2016-12-11T12:54:04.129764: step 680, loss 0.381518, acc 0.875
2016-12-11T12:54:07.029800: step 681, loss 0.646469, acc 0.71875
2016-12-11T12:54:09.982846: step 682, loss 0.446176, acc 0.828125
2016-12-11T12:54:12.932035: step 683, loss 0.472358, acc 0.796875
2016-12-11T12:54:15.891727: step 684, loss 0.808997, acc 0.578125
2016-12-11T12:54:18.920470: step 685, loss 0.560191, acc 0.75
2016-12-11T12:54:21.996280: step 686, loss 0.548603, acc 0.71875
2016-12-11T12:54:25.045307: step 687, loss 0.58818, acc 0.671875
2016-12-11T12:54:28.040286: step 688, loss 0.437522, acc 0.828125
2016-12-11T12:54:31.042830: step 689, loss 0.469357, acc 0.8125
2016-12-11T12:54:33.985461: step 690, loss 0.393814, acc 0.828125
2016-12-11T12:54:36.893713: step 691, loss 0.507293, acc 0.828125
2016-12-11T12:54:39.887751: step 692, loss 0.593822, acc 0.765625
2016-12-11T12:54:42.783175: step 693, loss 0.395798, acc 0.859375
2016-12-11T12:54:45.769078: step 694, loss 0.453878, acc 0.859375
2016-12-11T12:54:48.799176: step 695, loss 0.437667, acc 0.828125
2016-12-11T12:54:51.736370: step 696, loss 0.344038, acc 0.875
2016-12-11T12:54:54.868859: step 697, loss 0.474976, acc 0.796875
2016-12-11T12:54:57.894336: step 698, loss 0.412895, acc 0.890625
2016-12-11T12:55:00.883726: step 699, loss 0.372684, acc 0.859375
2016-12-11T12:55:03.871833: step 700, loss 0.583339, acc 0.75

Evaluation:
2016-12-11T12:55:28.385047: step 700, loss 0.433642, acc 0.841398

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481476583/checkpoints/model-700

2016-12-11T12:55:33.439046: step 701, loss 0.345205, acc 0.875
2016-12-11T12:55:36.407309: step 702, loss 0.385134, acc 0.875
2016-12-11T12:55:39.322949: step 703, loss 0.391335, acc 0.890625
2016-12-11T12:55:42.314609: step 704, loss 0.383498, acc 0.859375
2016-12-11T12:55:45.342982: step 705, loss 0.426801, acc 0.8125
2016-12-11T12:55:48.309835: step 706, loss 0.565731, acc 0.796875
2016-12-11T12:55:51.280789: step 707, loss 0.375194, acc 0.859375
2016-12-11T12:55:54.272627: step 708, loss 0.411258, acc 0.828125
2016-12-11T12:55:57.220545: step 709, loss 0.411755, acc 0.84375
2016-12-11T12:56:00.174935: step 710, loss 0.542474, acc 0.71875
2016-12-11T12:56:03.371862: step 711, loss 0.441637, acc 0.828125
2016-12-11T12:56:06.362915: step 712, loss 0.417973, acc 0.875
2016-12-11T12:56:09.359412: step 713, loss 0.558239, acc 0.796875
2016-12-11T12:56:12.291513: step 714, loss 0.316484, acc 0.921875
2016-12-11T12:56:15.277086: step 715, loss 0.423273, acc 0.84375
2016-12-11T12:56:18.311241: step 716, loss 0.437015, acc 0.84375
2016-12-11T12:56:21.278922: step 717, loss 0.483563, acc 0.828125
2016-12-11T12:56:24.208647: step 718, loss 0.406394, acc 0.859375
2016-12-11T12:56:27.122586: step 719, loss 0.637591, acc 0.796875
2016-12-11T12:56:30.098296: step 720, loss 0.345513, acc 0.890625
2016-12-11T12:56:33.067075: step 721, loss 0.700121, acc 0.6875
2016-12-11T12:56:36.200053: step 722, loss 0.43597, acc 0.828125
2016-12-11T12:56:39.231069: step 723, loss 0.565455, acc 0.71875
2016-12-11T12:56:42.136199: step 724, loss 0.526695, acc 0.703125
2016-12-11T12:56:45.146120: step 725, loss 0.484717, acc 0.796875
2016-12-11T12:56:48.106264: step 726, loss 0.367636, acc 0.90625
2016-12-11T12:56:51.107544: step 727, loss 0.53352, acc 0.78125
2016-12-11T12:56:54.101176: step 728, loss 0.43067, acc 0.859375
2016-12-11T12:56:57.036108: step 729, loss 0.552402, acc 0.8125
2016-12-11T12:56:59.979733: step 730, loss 0.556944, acc 0.8125
2016-12-11T12:57:02.947162: step 731, loss 0.256143, acc 0.9375
2016-12-11T12:57:05.927450: step 732, loss 0.488761, acc 0.828125
2016-12-11T12:57:09.145251: step 733, loss 0.33862, acc 0.890625
2016-12-11T12:57:12.183561: step 734, loss 0.520325, acc 0.8125
2016-12-11T12:57:15.120539: step 735, loss 0.44806, acc 0.859375
2016-12-11T12:57:18.140727: step 736, loss 0.436415, acc 0.84375
2016-12-11T12:57:21.097985: step 737, loss 0.516845, acc 0.765625
2016-12-11T12:57:24.099345: step 738, loss 0.444072, acc 0.875
2016-12-11T12:57:27.031418: step 739, loss 0.452206, acc 0.84375
2016-12-11T12:57:30.027912: step 740, loss 0.459133, acc 0.796875
2016-12-11T12:57:33.005299: step 741, loss 0.53683, acc 0.8125
2016-12-11T12:57:35.943605: step 742, loss 0.532556, acc 0.796875
2016-12-11T12:57:38.910503: step 743, loss 0.346199, acc 0.921875
2016-12-11T12:57:42.069775: step 744, loss 0.523253, acc 0.796875
2016-12-11T12:57:45.055187: step 745, loss 0.567736, acc 0.8125
2016-12-11T12:57:48.064150: step 746, loss 0.566934, acc 0.75
2016-12-11T12:57:51.016599: step 747, loss 0.517952, acc 0.828125
2016-12-11T12:57:53.969982: step 748, loss 0.369177, acc 0.921875
2016-12-11T12:57:56.922412: step 749, loss 0.54393, acc 0.78125
2016-12-11T12:57:59.949636: step 750, loss 0.386339, acc 0.875
2016-12-11T12:58:03.489508: step 751, loss 0.582388, acc 0.75
2016-12-11T12:58:06.733295: step 752, loss 0.298732, acc 0.90625
2016-12-11T12:58:10.131940: step 753, loss 0.485455, acc 0.84375
2016-12-11T12:58:13.331303: step 754, loss 0.480134, acc 0.859375
2016-12-11T12:58:16.374431: step 755, loss 0.404056, acc 0.875
2016-12-11T12:58:19.380207: step 756, loss 0.416701, acc 0.84375
2016-12-11T12:58:22.320183: step 757, loss 0.390046, acc 0.859375
2016-12-11T12:58:25.317407: step 758, loss 0.435551, acc 0.84375
2016-12-11T12:58:28.274096: step 759, loss 0.386151, acc 0.875
2016-12-11T12:58:31.167028: step 760, loss 0.434474, acc 0.84375
2016-12-11T12:58:34.113188: step 761, loss 0.495707, acc 0.828125
2016-12-11T12:58:37.077427: step 762, loss 0.465412, acc 0.828125
2016-12-11T12:58:40.234902: step 763, loss 0.458369, acc 0.84375
2016-12-11T12:58:43.233732: step 764, loss 0.38461, acc 0.859375
2016-12-11T12:58:46.398106: step 765, loss 0.576163, acc 0.796875
2016-12-11T12:58:49.459221: step 766, loss 0.39765, acc 0.84375
2016-12-11T12:58:52.428507: step 767, loss 0.431561, acc 0.84375
2016-12-11T12:58:55.423739: step 768, loss 0.422842, acc 0.84375
2016-12-11T12:58:58.415161: step 769, loss 0.48279, acc 0.796875
2016-12-11T12:59:01.409690: step 770, loss 0.347089, acc 0.875
2016-12-11T12:59:04.350817: step 771, loss 0.450317, acc 0.828125
2016-12-11T12:59:07.262589: step 772, loss 0.395162, acc 0.859375
2016-12-11T12:59:10.183319: step 773, loss 0.41347, acc 0.84375
2016-12-11T12:59:13.145874: step 774, loss 0.377816, acc 0.890625
2016-12-11T12:59:16.062476: step 775, loss 0.643674, acc 0.78125
2016-12-11T12:59:19.285828: step 776, loss 0.418361, acc 0.84375
2016-12-11T12:59:22.362655: step 777, loss 0.265609, acc 0.9375
2016-12-11T12:59:25.386323: step 778, loss 0.558113, acc 0.828125
2016-12-11T12:59:28.322657: step 779, loss 0.63331, acc 0.734375
2016-12-11T12:59:31.275095: step 780, loss 0.386566, acc 0.859375
2016-12-11T12:59:34.217685: step 781, loss 0.429437, acc 0.875
2016-12-11T12:59:37.135650: step 782, loss 0.317238, acc 0.890625
2016-12-11T12:59:40.131240: step 783, loss 0.461794, acc 0.828125
2016-12-11T12:59:43.071077: step 784, loss 0.454046, acc 0.84375
2016-12-11T12:59:46.072433: step 785, loss 0.62407, acc 0.75
2016-12-11T12:59:49.061440: step 786, loss 0.395272, acc 0.875
2016-12-11T12:59:52.169225: step 787, loss 0.523359, acc 0.796875
2016-12-11T12:59:55.204404: step 788, loss 0.45659, acc 0.859375
2016-12-11T12:59:58.201975: step 789, loss 0.638041, acc 0.765625
2016-12-11T13:00:01.143544: step 790, loss 0.396469, acc 0.84375
2016-12-11T13:00:04.160518: step 791, loss 0.497172, acc 0.796875
2016-12-11T13:00:07.139801: step 792, loss 0.358476, acc 0.890625
2016-12-11T13:00:10.124877: step 793, loss 0.373278, acc 0.890625
2016-12-11T13:00:13.082405: step 794, loss 0.48342, acc 0.828125
2016-12-11T13:00:15.998357: step 795, loss 0.425595, acc 0.84375
2016-12-11T13:00:19.325628: step 796, loss 0.29799, acc 0.921875
2016-12-11T13:00:22.811680: step 797, loss 0.378624, acc 0.875
2016-12-11T13:00:26.290800: step 798, loss 0.490747, acc 0.8125
2016-12-11T13:00:29.491531: step 799, loss 0.43868, acc 0.84375
2016-12-11T13:00:32.667177: step 800, loss 0.317338, acc 0.890625