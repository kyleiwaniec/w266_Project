python train.py --positive_data_file data/pos_data_train --negative_data_file data/neg_data_train --dev_sample_percentage .005

Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=64
CHECKPOINT_EVERY=100
DEV_SAMPLE_PERCENTAGE=0.005
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=128
EVALUATE_EVERY=100
FILTER_SIZES=3,4,5
L2_REG_LAMBDA=0.0
LOG_DEVICE_PLACEMENT=False
NEGATIVE_DATA_FILE=data/neg_data_train
NUM_EPOCHS=200
NUM_FILTERS=128
POSITIVE_DATA_FILE=data/pos_data_train

Loading data...
('l-positive_examples', 157242)
('l-negative_examples', 117641)
('type', 274883, 274883)
['copy', 'and', 'paste', 'this', 'code', 'onto', 'your', 'aduino', 'sketchpad:\n\n/*', '-*-', 'mode:', 'c++;', 'c-basic-offset:', '4;', 'indent-tabs-mode:', 'nil;', 'tab-width:', '4', '-*-', '*/\n/*', 'vi:', 'set', 'ts=4', 'sw=4', 'expandtab:', '(add', 'to', '~/.vimrc:', 'set', 'modeline', 'modelines=5)', '*/\n/*\n', '', '', 'unitone\n', '', '', 'basic', 'tests\n', '', '', '\n', '', '', 'pin', 'outs\n', '', '', 'a5-rear', 'bumper', 'switch', 'by', 'speaker\n', '', '', 'a4-front', 'bumper', 'switch', 'by', 'ir', 'receiver\n', '', '', 'a1-green', 'leds\n', '', '', 'a0-red', 'leds\n', '', '', 'a2-blue', 'leds\n\n', '', '', 'd9-motor', 'l3\n', '', '', 'd3-sound', 'play\n', '', '', 'd7-motor', 'l4\n', '', '', 'd2-sound', 'rec\n', '', '', 'd5-camera', 'enbable\n', '', '', 'd6\xe2\x80\x93motor', 'e1-2', '/e3-4', 'yellow', 'on', 'manuel\n', '', '', 'd9-open-', 'unused', 'green', 'wire\n', '', '', 'd8-motor', 'l2\n', '', '', 'd10-motor', 'l1\n', '', '', 'd11-front', 'ir', 'sensor\n', '', '', 'd12-rear', 'ir', 'sensor\n', '', '', 'd4-camera', 'shutter\n*/\n\n#include', '<irremote.h>\n\n\nconst', 'int', 'remote_1_1', '', '', '', '', '=', '0x597b;\nconst', 'int', 'remote_1_2', '', '', '', '', '=', '0x10ce;\nconst', 'int', 'remote_2_1', '', '', '', '', '=', '0x797e;\nconst', 'int', 'remote_2_2', '', '', '', '', '=', '0x20cd;\nconst', 'int', 'remote_3_1', '', '', '', '', '=', '0x1dbf;\nconst', 'int', 'remote_3_2', '', '', '', '', '=', '0x30cc;\nconst', 'int', 'remote_4_1', '', '', '', '', '=', '0x40cb;\nconst', 'int', 'remote_4_2', '', '', '', '', '=', '0x53d6;\nconst', 'int', 'remote_5_1', '', '', '', '', '=', '0x50ca;\nconst', 'int', 'remote_5_2', '', '', '', '', '=', '0x4edf;\nconst', 'int', 'remote_6_1', '', '', '', '', '=', '0xda9a;\nconst', 'int', 'remote_6_2', '', '', '', '', '=', '0x60c9;\nconst', 'int', 'remote_7_1', '', '', '', '', '=', '0x70c8;\nconst', 'int', 'remote_7_2', '', '', '', '', '=', '0x70db;\nconst', 'int', 'remote_8_1', '', '', '', '', '=', '0x0e7e;\nconst', 'int', 'remote_8_2', '', '', '', '', '=', '0x80c7;\nconst', 'int', 'remote_9_1', '', '', '', '', '=', '0x1877;\nconst', 'int', 'remote_9_2', '', '', '', '', '=', '0x90c6;\nconst', 'int', 'remote_dot_1', '', '', '=', '0xaf5f;\nconst', 'int', 'remote_dot_2', '', '', '=', '0x7018;\nconst', 'int', 'remote_0_1', '', '', '', '', '=', '0xdf7a;\nconst', 'int', 'remote_0_2', '', '', '', '', '=', '0x00cf;\nconst', 'int', 'remote_enter_1', '=', '0x400b;\nconst', 'int', 'remote_enter_2', '=', '0xb4fe;\nconst', 'int', 'remote_rew_1', '', '', '=', '0x79c3;\nconst', 'int', 'remote_rew_2', '', '', '=', '0xd1e2;\nconst', 'int', 'remote_play_1', '', '=', '0xb57f;\nconst', 'int', 'remote_play_2', '', '=', '0x51ea;\nconst', 'int', 'remote_ff_1', '', '', '', '=', '0x0fba;\nconst', 'int', 'remote_ff_2', '', '', '', '=', '0xc1e3;\nconst', 'int', 'remote_rec_1', '', '', '=', '0x2ec3;\nconst', 'int', 'remote_rec_2', '', '', '=', '0x71e8;\nconst', 'int', 'remote_stop_1', '', '=', '0x3abf;\nconst', 'int', 'remote_stop_2', '', '=', '0xf1e0;\nconst', 'int', 'remote_pause_1', '=', '0xcb5f;\nconst', 'int', 'remote_pause_2', '=', '0x91e6;\nconst', 'int', 'remote_pwr_1', '', '', '=', '0xb3c4;\nconst', 'int', 'remote_pwr_2', '', '', '=', '0xa0d5;\n\nconst', 'int', 'pin_bmp_r', '=', 'a5;\nconst', 'int', 'pin_bmp_f', '=', 'a4;\nconst', 'int', 'pin_led_g', '=', 'a1;\nconst', 'int', 'pin_led_r', '=', 'a0;\nconst', 'int', 'pin_led_b', '=', 'a2;\n\nconst', 'int', 'pin_cam_img', '=', '4;\nconst', 'int', 'pin_aud_r', '', '', '=', '2;\nconst', 'int', 'pin_aud_p', '', '', '=', '3;\nconst', 'int', 'pin_cam_pwr', '=', '5;\nconst', 'int', 'pin_rir_rcv', '=', '12;\nconst', 'int', 'pin_fir_rcv', '=', '11;\n\nconst', 'int', 'pin_mtr_en', '', '=', '6;\nconst', 'int', 'pin_mtr_l1', '', '=', '10;\nconst', 'int', 'pin_mtr_l2', '', '=', '8;\nconst', 'int', 'pin_mtr_l3', '', '=', '9;\nconst', 'int', 'pin_mtr_l4', '', '=', '7;\n\nconst', 'int', 'full', '=', '255;\nconst', 'int', 'slow', '=', '150;\n\n\n\nirrecv', 'irrecv(pin_rir_rcv);\ndecode_results', 'results;\n\n\n//', 'the', 'setup', 'routine', 'runs', 'once', 'when', 'you', 'press', 'reset:\nvoid', 'setup()', '{\n', '', 'pinmode(pin_led_r,', 'output);\n', '', 'pinmode(pin_led_g,', 'output);\n', '', 'pinmode(pin_led_b,', 'output);\n\n', '', 'pinmode(pin_bmp_r,', 'input);\n', '', 'pinmode(pin_bmp_f,', 'input);\n\n', '', 'pinmode(pin_aud_r,', 'output);\n', '', 'pinmode(pin_aud_p,', 'output);\n\n', '', 'pinmode(pin_cam_pwr,', 'output);\n', '', 'pinmode(pin_cam_img,', 'output);\n', '', '\n', '', 'pinmode(pin_mtr_en,', 'output);\n', '', 'pinmode(pin_mtr_l1,', 'output);\n', '', 'pinmode(pin_mtr_l2,', 'output);\n', '', 'pinmode(pin_mtr_l3,', 'output);\n', '', 'pinmode(pin_mtr_l4,', 'output);\n\n', '', 'serial.begin(115200);\n', '', 'irrecv.enableirin();', '//', 'start', 'the', 'receiver\n}\n\nvoid', 'loop()', '{\n', '', 'if', '(irrecv.decode(&results))', '{\n', '', '', '', 'int', 'code', '=', 'results.value', '&', '0xffff;\n', '', '', '', '//serial.println(code,', 'hex);\n', '', '', '', 'dispatchcommand(code);\n', '', '', '', 'irrecv.resume();', '//', 'receive', 'the', 'next', 'value\n', '', '}\n', '', 'if', '(fwdbump())', '{\n', '', '', '', 'playsound();\n', '', '', '', 'lightsflash();\n', '', '}\n', '', 'if', '(bwdbump())', '{\n', '', '', '', 'playsound();\n', '', '', '', 'lightsflash();\n', '', '}', '', '\n}\n\nint', 'lastbutton', '=', '0;\n\nvoid', 'dispatchcommand(int', 'code)', '{\n', '', 'switch', '(code)', '{\n', '', '', '', '', '', 'case', 'remote_1_1:\n', '', '', '', '', '', 'case', 'remote_1_2:\n', '', '', '', '', '', '', '', 'serial.println("1");\n', '', '', '', '', '', '', '', 'waddle();\n', '', '', '', '', '', '', '', 'break;\n', '', '', '', '', '', 'case', 'remote_2_1:\n', '', '', '', '', '', 'case', 'remote_2_2:\n', '', '', '', '', '', '', '', 'serial.println("2");\n', '', '', '', '', '', '', '', 'wiggle();\n', '', '', '', '', '', '', '', 'break;\n', '', '', '', '', '', 'case', 'remote_3_1:\n', '', '', '', '', '', 'case', 'remote_3_2:\n', '', '', '', '', '', '', '', 'serial.println("3");\n', '', '', '', '', '', '', '', 'powerslideright();\n', '', '', '', '', '', '', '', 'break;\n', '', '', '', '', '', 'case', 'remote_4_1:\n', '', '', '', '', '', 'case', 'remote_4_2:\n', '', '', '', '', '', '', '', 'serial.println("4");\n', '', '', '', '', '', '', '', 'spinright(3000);\n', '', '', '', '', '', '', '', 'break;\n', '', '', '', '', '', 'case', 'remote_5_1:\n', '', '', '', '', '', 'case', 'remote_5_2:\n', '', '', '', '', '', '', '', 'serial.println("5");\n', '', '', '', '', '', '', '', 'spinleft(3000);\n', '', '', '', '', '', '', '', 'break;\n', '', '', '', '', '', 'case', 'remote_6_1:\n', '', '', '', '', '', 'case', 'remote_6_2:\n', '', '', '', '', '', '', '', 'serial.println("6");\n', '', '', '', '', '', '', '', 'powerslideleft();\n', '', '', '', '', '', '', '', 'break;\n', '', '', '', '', '', 'case', 'remote_7_1:\n', '', '', '', '', '', 'case', 'remote_7_2:\n', '', '', '', '', '', '', '', 'serial.println("7");\n', '', '', '', '', '', '', '', 'rightfwd();\n', '', '', '', '', '', '', '', 'break;\n', '', '', '', '', '', 'case', 'remote_8_1:\n', '', '', '', '', '', 'case', 'remote_8_2:\n', '', '', '', '', '', '', '', 'serial.println("8");\n', '', '', '', '', '', '', '', 'figureeight();\n', '', '', '', '', '', '', '', 'break;\n', '', '', '', '', '', 'case', 'remote_9_1:\n', '', '', '', '', '', 'case', 'remote_9_2:\n', '', '', '', '', '', '', '', 'serial.println("9");\n', '', '', '', '', '', '', '', 'waddlebwd();\n', '', '', '', '', '', '', '', 'break;\n', '', '', '', '', '', 'case', 'remote_dot_1:\n', '', '', '', '', '', 'case', 'remote_dot_2:\n', '', '', '', '', '', '', '', 'serial.println("dot");\n', '', '', '', '', '', '', '', 'lastbutton', '=', 'remote_dot_1;\n', '', '', '', '', '', '', '', 'break;\n', '', '', '', '', '', 'case', 'remote_0_1:\n', '', '', '', '', '', 'case', 'remote_0_2:\n', '', '', '', '', '', '', '', 'serial.println("0");\n', '', '', '', '', '', '', '', 'break;\n', '', '', '', '', '', 'case', 'remote_enter_1:\n', '', '', '', '', '', 'case', 'remote_enter_2:\n', '', '', '', '', '', '', '', 'serial.println("enter");\n', '', '', '', '', '', '', '', 'if', '(lastbutton', '==', 'remote_dot_1)', '{\n', '', '', '', '', '', '', '', '', '', '', '', 'recordsound();\n', '', '', '', '', '', '', '', '}\n', '', '', '', '', '', '', '', 'break;\n', '', '', '', '', '', 'case', 'remote_play_1:\n', '', '', '', '', '', 'case', 'remote_play_2:\n', '', '', '', '', '', '', '', 'serial.println("play");\n', '', '', '', '', '', '', '', 'playsound();\n', '', '', '', '', '', '', '', 'break;\n', '', '', '', '', '', 'case', 'remote_rew_1:\n', '', '', '', '', '', 'case', 'remote_rew_2:\n', '', '', '', '', '', '', '', 'serial.println("rew");\n', '', '', '', '', '', '', '', '//bwdspeed(slow);\n', '', '', '', '', '', '', '', '//bwd(0);\n', '', '', '', '', '', '', '', 'bwdfaster();\n', '', '', '', '', '', '', '', 'break;\n', '', '', '', '', '', 'case', 'remote_ff_1:\n', '', '', '', '', '', 'case', 'remote_ff_2:\n', '', '', '', '', '', '', '', 'serial.println("ff");\n', '', '', '', '', '', '', '', '//fwdspeed(slow);\n', '', '', '', '', '', '', '', '//fwd(0);\n', '', '', '', '', '', '', '', 'fwdfaster();\n', '', '', '', '', '', '', '', 'break;\n', '', '', '', '', '', 'case', 'remote_rec_1:\n', '', '', '', '', '', 'case', 'remote_rec_2:\n', '', '', '', '', '', '', '', 'serial.println("rec");\n', '', '', '', '', '', '', '', 'recordimage();\n', '', '', '', '', '', '', '', 'break;\n', '', '', '', '', '', 'case', 'remote_stop_1:\n', '', '', '', '', '', 'case', 'remote_stop_2:\n', '', '', '', '', '', '', '', 'serial.println("stop");\n', '', '', '', '', '', '', '', 'stopsound();\n', '', '', '', '', '', '', '', 'powerstop();\n', '', '', '', '', '', '', '', 'break;\n', '', '', '', '', '', 'case', 'remote_pause_1:\n', '', '', '', '', '', 'case', 'remote_pause_2:\n', '', '', '', '', '', '', '', 'serial.println("pause");\n', '', '', '', '', '', '', '', 'rightstop();\n', '', '', '', '', '', '', '', 'leftstop();\n', '', '', '', '', '', '', '', 'break;\n', '', '', '', '', '', 'case', 'remote_pwr_1:\n', '', '', '', '', '', 'case', 'remote_pwr_2:\n', '', '', '', '', '', '', '', 'serial.println("on/off");\n', '', '', '', '', '', '', '', 'lightsflash();\n', '', '', '', '', '', '', '', 'break;\n', '', '', '', '', '', 'default:\n', '', '', '', '', '', '', '', 'break;\n', '', '}\n}\n\n//', 'behaviors\n\nvoid', 'lightsflash()', '{\n', '', 'lightson(100,', '1,', '0,', '0);\n', '', 'lightson(100,', '0,', '1,', '0);\n', '', 'lightson(100,', '0,', '0,', '1);\n', '', '\n', '', 'lightson(300,', '1,', '0,', '1);\n', '', 'lightson(300,', '1,', '1,', '0);\n', '', 'lightson(300,', '0,', '1,', '1);\n', '', '\n', '', 'lightson(100,', '1,', '0,', '0);\n', '', 'lightson(100,', '0,', '1,', '0);\n', '', 'lightson(100,', '0,', '0,', '1);\n', '', '\n', '', 'lightson(100,', '1,', '0,', '1);\n', '', 'lightson(100,', '1,', '1,', '0);\n', '', 'lightson(100,', '0,', '1,', '1);\n', '', '\n', '', 'lightson(300,', '1,', '0,', '0);\n', '', 'lightson(300,', '0,', '1,', '0);\n', '', 'lightson(300,', '0,', '0,', '1);\n}\n\nvoid', 'powerslideleft()', '{\n', '', 'fwdspeed(full);\n', '', 'fwd(2000);\n', '', 'spinleft(500);\n', '', 'bwd(300);\n}\n\nvoid', 'powerslideright()', '{\n', '', 'fwdspeed(full);\n', '', 'fwd(2000);\n', '', 'spinleft(500);\n', '', 'bwd(300);\n}\n\n\nvoid', 'figureeight()', '{\n', '', 'leftfwd();\n', '', 'rightfwd();\n', '', '//delay(1000);\n', '', 'lightson(300,', '1,', '0,', '0);\n', '', 'lightson(300,', '0,', '1,', '0);\n', '', 'lightson(300,', '0,', '0,', '1);\n', '', 'leftstop();\n', '', '//delay(1000);\n', '', 'lightson(300,', '1,', '0,', '0);\n', '', 'lightson(300,', '0,', '1,', '0);\n', '', 'lightson(300,', '0,', '0,', '1);\n', '', 'leftfwd();\n', '', '//delay(1000);\n', '', 'lightson(300,', '1,', '0,', '0);\n', '', 'lightson(300,', '0,', '1,', '0);\n', '', 'lightson(300,', '0,', '0,', '1);\n', '', 'rightstop();\n', '', '//delay(1000);\n', '', 'lightson(300,', '1,', '0,', '0);\n', '', 'lightson(300,', '0,', '1,', '0);\n', '', 'lightson(300,', '0,', '0,', '1);\n', '', 'rightfwd();\n', '', '//delay(1000);\n', '', 'lightson(300,', '1,', '0,', '0);\n', '', 'lightson(300,', '0,', '1,', '0);\n', '', 'lightson(300,', '0,', '0,', '1);\n', '', 'leftstop();\n', '', 'rightstop();\n}\n\nvoid', 'wiggle()', '{\n', '', 'fwdspeed(random(full-20,', 'full));', '', '\n', '', 'bwdspeed(random(full-20,', 'full));', '', '\n', '', 'for', '(int', 'i=0;', 'i', '<', '10;', 'i++)', '{\n', '', '', '', 'spinleft(random(150,220));', '\n', '', '', '', 'delay(random(150,220));', '\n', '', '', '', 'spinright(random(150,220));\n', '', '', '', 'delay(random(150,220));\n', '', '}\n', '', 'stop();\n', '', 'lightsflash();\n}\n\nvoid', 'waddle()', '{\n', '', 'fwdspeed(full);\n', '', 'for', '(int', 'i=0;', 'i', '<', '5;', 'i++)', '{\n', '', '', '', 'waddleleft();\n', '', '', '', '//delay(1000);\n', '', '', '', 'lightson(300,', '1,', '0,', '0);\n', '', '', '', 'lightson(300,', '0,', '1,', '0);\n', '', '', '', 'lightson(300,', '0,', '0,', '1);\n', '', '', '', 'waddleright();\n', '', '', '', '//delay(1000);\n', '', '', '', 'lightson(300,', '1,', '0,', '0);\n', '', '', '', 'lightson(300,', '0,', '1,', '0);\n', '', '', '', 'lightson(300,', '0,', '0,', '1);\n', '', '}\n}\n\nvoid', 'waddleleft()', '{\n', '', 'rightstop();\n', '', 'leftfwd();\n', '', '//delay(300);\n', '', 'lightson(300,', '1,', '0,', '0);\n', '', 'leftstop();\n', '', 'rightfwd();\n', '', '//delay(300);\n', '', 'lightson(300,', '0,', '1,', '0);\n', '', 'rightstop();\n', '', 'leftfwd();\n', '', '//delay(300);\n', '', 'lightson(300,', '0,', '0,', '1);\n', '', 'leftstop();\n', '', 'rightfwd();\n', '', '//delay(300);\n', '', 'lightson(300,', '0,', '1,', '0);\n', '', 'rightstop();\n}\n\nvoid', 'waddleright()', '{\n', '', 'leftstop();\n', '', 'rightfwd();\n', '', '//delay(300);\n', '', 'lightson(300,', '1,', '0,', '0);\n', '', 'rightstop();\n', '', 'leftfwd();\n', '', '//delay(300);\n', '', 'lightson(300,', '0,', '1,', '0);\n', '', 'leftstop();\n', '', 'rightfwd();\n', '', '//delay(300);\n', '', 'lightson(300,', '0,', '0,', '1);\n', '', 'rightstop();\n', '', 'leftfwd();\n', '', '//delay(300);\n', '', 'lightson(300,', '0,', '1,', '0);\n', '', 'leftstop();\n}\n\nvoid', 'waddlebwd()', '{\n', '', 'fwdspeed(full);\n', '', 'for', '(int', 'i=0;', 'i', '<', '5;', 'i++)', '{\n', '', '', '', 'waddleleft();\n', '', '', '', '//delay(1000);\n', '', '', '', 'lightson(300,', '1,', '0,', '0);\n', '', '', '', 'lightson(300,', '0,', '1,', '0);\n', '', '', '', 'lightson(300,', '0,', '0,', '1);\n', '', '', '', 'waddleright();\n', '', '', '', '//delay(1000);\n', '', '', '', 'lightson(300,', '1,', '0,', '0);\n', '', '', '', 'lightson(300,', '0,', '1,', '0);\n', '', '', '', 'lightson(300,', '0,', '0,', '1);\n', '', '}\n}\n\nvoid', 'waddleleftbwd()', '{\n', '', 'rightstop();\n', '', 'leftbwd();\n', '', '//delay(300);\n', '', 'lightson(300,', '1,', '0,', '0);\n', '', 'leftstop();\n', '', 'rightbwd();\n', '', '//delay(300);\n', '', 'lightson(300,', '0,', '1,', '0);\n', '', 'rightstop();\n', '', 'leftbwd();\n', '', '//delay(300);\n', '', 'lightson(300,', '0,', '0,', '1);\n', '', 'leftstop();\n', '', 'rightbwd();\n', '', '//delay(300);\n', '', 'lightson(300,', '0,', '1,', '0);\n', '', 'rightstop();\n}\n\nvoid', 'waddlerightbwd()', '{\n', '', 'leftstop();\n', '', 'rightbwd();\n', '', '//delay(300);\n', '', 'lightson(300,', '1,', '0,', '0);\n', '', 'rightstop();\n', '', 'leftbwd();\n', '', '//delay(300);\n', '', 'lightson(300,', '0,', '1,', '0);\n', '', 'leftstop();\n', '', 'rightbwd();\n', '', '//delay(300);\n', '', 'lightson(300,', '0,', '0,', '1);\n', '', 'rightstop();\n', '', 'leftbwd();\n', '', '//delay(300);\n', '', 'lightson(300,', '0,', '1,', '0);\n', '', 'leftstop();\n}\n\n\n//', 'primitives\n\n\nint', 'direction', '=', '0;\nint', 'rightfwdspeed', '=', '128;\nint', 'leftfwdspeed', '=', '128;\nint', 'rightbwdspeed', '=', '128;\nint', 'leftbwdspeed', '=', '128;\n\n\n\nvoid', 'powerstop()', '{\n', '', 'if', '(direction', '>', '0)', '{\n', '', '', '', 'bwdspeed(full);\n', '', '', '', 'bwd(200);\n', '', '}\n', '', 'else', 'if', '(direction', '<', '0)', '{\n', '', '', '', 'fwdspeed(full);\n', '', '', '', 'fwd(200);\n', '', '}\n', '', 'stop();\n', '', 'direction', '=', '0;\n', '', 'fwdspeed(slow);\n', '', 'bwdspeed(slow);\n}\n\nvoid', 'stop()', '{\n', '', 'leftstop();\n', '', 'rightstop();\n', '', 'direction', '=', '0;\n}\n\nvoid', 'fwd(int', 't)', '{\n', '', 'serial.println("fwd()");\n', '', 'rightfwd();\n', '', 'leftfwd();\n', '', 'direction', '=', '1;\n', '', 'if', '(t', '>', '0)', '{\n', '', '', '', 'delay(t);\n', '', '', '', 'rightstop();\n', '', '', '', 'leftstop();\n', '', '', '', 'direction', '=', '0;\n', '', '}\n}\n\nvoid', 'bwd(int', 't)', '{\n', '', 'serial.println("bwd()");\n', '', 'rightbwd();\n', '', 'leftbwd();\n', '', 'direction', '=', '-1;\n', '', 'if', '(t', '>', '0)', '{\n', '', '', '', 'delay(t);\n', '', '', '', 'rightstop();\n', '', '', '', 'leftstop();\n', '', '', '', 'direction', '=', '0;\n', '', '}\n}\n\nvoid', 'spinleft(int', 't)', '{\n', '', 'serial.println("spinleft()");\n', '', 'rightfwd();\n', '', 'leftbwd();\n', '', 'delay(t);\n', '', 'rightstop();\n', '', 'leftstop();\n}\n\nvoid', 'spinright(int', 't)', '{\n', '', 'serial.println("spinright()");\n', '', 'leftfwd();\n', '', 'rightbwd();\n', '', 'delay(t);\n', '', 'rightstop();\n', '', 'leftstop();\n}\n\nvoid', 'reversedirection()', '{\n', '', 'if', '(direction', '<', '0)', '{\n', '', '', '', 'fwd(10);\n', '', '}\n', '', 'else', '{\n', '', '', '', 'bwd(10);\n', '', '}\n}\n\nconst', 'int', 'rightfwdcalibration', '=', '0;\nconst', 'int', 'leftfwdcalibration', '=', '0;\nconst', 'int', 'speedincr', '=', '10;\n\nvoid', 'fwdfaster()', '{\n', '', 'if', '(rightfwdspeed', '>', 'full', '||', 'leftfwdspeed', '>', 'full)', '{\n', '', '', '', '//', 'do', 'nothing\n', '', '}\n', '', 'else', 'if', '(direction', '>', '0)', '{\n', '', '', '', 'leftfwdspeed', '-=', 'speedincr;\n', '', '', '', 'rightfwdspeed', '+=', 'speedincr;\n', '', '', '', 'fwd(0);\n', '', '}\n', '', 'else', 'if', '(direction', '<', '0)', '{\n', '', '', '', 'if', '(leftbwdspeed', '>', 'slow)', '{\n', '', '', '', '', '', 'leftbwdspeed', '+=', 'speedincr;\n', '', '', '', '', '', 'rightbwdspeed', '-=', 'speedincr;\n', '', '', '', '', '', 'bwd(0);\n', '', '', '', '}\n', '', '', '', 'else', '{\n', '', '', '', '', '', 'fwdspeed(slow);\n', '', '', '', '', '', 'fwd(0);\n', '', '', '', '}\n', '', '}\n', '', 'else', '{\n', '', '', '', 'fwdspeed(slow);\n', '', '', '', 'fwd(0);\n', '', '}\n}\n\nvoid', 'bwdfaster()', '{\n', '', 'if', '(rightbwdspeed', '>', 'full', '||', 'leftbwdspeed', '>', 'full)', '{\n', '', '', '', '//', 'do', 'nothing\n', '', '}\n', '', 'else', 'if', '(direction', '>', '0', '&&', 'rightfwdspeed', '>', 'slow)', '{\n', '', '', '', 'if', '(leftbwdspeed', '>', 'slow)', '{\n', '', '', '', '', '', 'bwdspeed(slow);\n', '', '', '', '', '', 'bwd(0);\n', '', '', '', '}\n', '', '', '', 'else', '{\n', '', '', '', '', '', 'leftfwdspeed', '+=', 'speedincr;\n', '', '', '', '', '', 'rightfwdspeed', '-=', 'speedincr;\n', '', '', '', '', '', 'fwd(0);\n', '', '', '', '}\n', '', '}\n', '', 'else', 'if', '(direction', '<', '0)', '{\n', '', '', '', 'leftbwdspeed', '-=', 'speedincr;\n', '', '', '', 'rightbwdspeed', '+=', 'speedincr;\n', '', '', '', 'bwd(0);\n', '', '}\n', '', 'else', '{\n', '', '', '', 'bwdspeed(slow);\n', '', '', '', 'bwd(0);\n', '', '}\n}\n\nvoid', 'fwdspeed(int', 'n)', '{\n', '', 'serial.println("fwdspeed");\n', '', 'leftfwdspeed', '=', '255', '-', 'n', '+', 'leftfwdcalibration;\n', '', 'rightfwdspeed', '=', 'n', '-', 'rightfwdcalibration;\n', '', 'serial.println(leftfwdspeed);\n', '', 'serial.println(rightfwdspeed);\n}\n\nvoid', 'leftfwd()', '{\n', '', 'serial.println("leftfwd()");\n', '', 'serial.println(leftfwdspeed);\n', '', 'analogwrite(pin_mtr_l1,', 'leftfwdspeed);\n', '', 'digitalwrite(pin_mtr_l2,', 'high);\n', '', 'digitalwrite(pin_mtr_en,', 'high);\n}\n\nvoid', 'rightfwd()', '{\n', '', 'serial.println("rightfwd()");\n', '', 'serial.println(rightfwdspeed);\n', '', 'analogwrite(pin_mtr_l3,', 'rightfwdspeed);\n', '', 'digitalwrite(pin_mtr_l4,', 'low);\n', '', 'digitalwrite(pin_mtr_en,', 'high);\n}\n\nconst', 'int', 'leftbwdcalibration', '=', '0;\nconst', 'int', 'rightbwdcalibration', '=', '-20;\n\nvoid', 'bwdspeed(int', 'n)', '{\n', '', 'rightbwdspeed', '=', 'n', '+', 'leftbwdcalibration;\n', '', 'leftbwdspeed', '=', '255', '-', 'n', '-', 'rightfwdcalibration;\n}\n\n\n\nvoid', 'leftbwd()', '{\n', '', 'serial.println("leftbwd()");\n', '', 'analogwrite(pin_mtr_l1,', 'rightbwdspeed);\n', '', 'digitalwrite(pin_mtr_l2,', 'low);\n', '', 'digitalwrite(pin_mtr_en,', 'high);\n}\n\n\nvoid', 'rightbwd()', '{\n', '', 'serial.println("leftbwd()");\n', '', 'analogwrite(pin_mtr_l3,', 'leftbwdspeed);\n', '', 'digitalwrite(pin_mtr_l4,', 'high);\n', '', 'digitalwrite(pin_mtr_en,', 'high);\n}\n\nvoid', 'leftstop()', '{\n', '', 'digitalwrite(pin_mtr_l1,', 'low);\n', '', 'digitalwrite(pin_mtr_l2,', 'low);\n}\n\nvoid', 'rightstop()', '{\n', '', 'digitalwrite(pin_mtr_l3,', 'low);\n', '', 'digitalwrite(pin_mtr_l4,', 'low);\n}\n\nvoid', 'lightson(int', 't,', 'int', 'r,', 'int', 'g,', 'int', 'b)', '{\n', '', 'if', '(r', '!=', '0)', 'digitalwrite(pin_led_r,', 'high);\n', '', 'if', '(g', '!=', '0)', 'digitalwrite(pin_led_g,', 'high);\n', '', 'if', '(b', '!=', '0)', 'digitalwrite(pin_led_b,', 'high);\n', '', 'digitalwrite(pin_led_r,', 'low);\n', '', 'digitalwrite(pin_led_g,', 'low);\n', '', 'digitalwrite(pin_led_b,', 'low);\n', '', 'delay(t);\n}\n\nvoid', 'recordimage()', '{\n', '', 'digitalwrite(pin_cam_pwr,', 'high);\n', '', 'delay(1000);\n', '', 'digitalwrite(pin_cam_pwr,', 'low);\n}\n\nvoid', 'recordsound()', '{\n', '', 'digitalwrite(pin_aud_r,', 'high);\n}\n\nvoid', 'stopsound()', '{\n', '', 'digitalwrite(pin_aud_r,', 'low);\n}\n\nvoid', 'playsound()', '{\n', '', 'digitalwrite(pin_aud_p,', 'high);\n', '', 'delay(500);\n', '', 'digitalwrite(pin_aud_p,', 'low);', '', '\n}\n\nboolean', 'bwdbump()', '{\n', '', 'int', 'in', '=', 'digitalread(pin_bmp_r);\n', '', 'if', '(in)', '{\n', '', '', '', 'serial.println("bwdbump");\n', '', '', '', 'delay(1000);\n', '', '', '', 'reversedirection();\n', '', '}\n', '', 'return', 'in;\n}\n\nboolean', 'fwdbump()', '{\n', '', 'int', 'in', '=', 'digitalread(pin_bmp_f);\n', '', 'if', '(in)', '{\n', '', '', '', 'serial.println("fwdbump");\n', '', '', '', 'delay(1000);\n', '', '', '', 'reversedirection();\n', '', '}\n', '', 'return', 'in;\n}']
Build vocabulary...
('max_document_length...', 2716)
('x shape', (274883, 2716))
Split train/test set...
Vocabulary Size: 161304
Train/Dev split: 273509/1374
Writing to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481637188

2016-12-13T08:53:15.625581: step 1, loss 1.48007, acc 0.609375
2016-12-13T08:53:20.376729: step 2, loss 2.33781, acc 0.5
2016-12-13T08:53:24.972993: step 3, loss 2.02502, acc 0.546875
2016-12-13T08:53:29.769586: step 4, loss 1.97104, acc 0.5625
2016-12-13T08:53:34.553640: step 5, loss 1.69363, acc 0.46875
2016-12-13T08:53:39.583181: step 6, loss 1.74674, acc 0.578125
2016-12-13T08:53:44.405978: step 7, loss 2.04276, acc 0.515625
2016-12-13T08:53:49.000853: step 8, loss 1.71979, acc 0.546875
2016-12-13T08:53:53.540222: step 9, loss 1.91058, acc 0.515625
2016-12-13T08:53:58.324733: step 10, loss 1.63916, acc 0.59375
2016-12-13T08:54:02.781195: step 11, loss 2.41735, acc 0.5
2016-12-13T08:54:07.304396: step 12, loss 1.81771, acc 0.625
2016-12-13T08:54:12.841029: step 13, loss 1.28734, acc 0.625
2016-12-13T08:54:17.870268: step 14, loss 1.64426, acc 0.5
2016-12-13T08:54:22.931947: step 15, loss 2.11062, acc 0.5625
2016-12-13T08:54:27.512141: step 16, loss 2.22477, acc 0.484375
2016-12-13T08:54:31.932843: step 17, loss 1.86213, acc 0.484375
2016-12-13T08:54:36.352228: step 18, loss 1.389, acc 0.59375
2016-12-13T08:54:40.891277: step 19, loss 2.47989, acc 0.4375
2016-12-13T08:54:45.685941: step 20, loss 1.79108, acc 0.53125
2016-12-13T08:54:50.630881: step 21, loss 1.97523, acc 0.546875
2016-12-13T08:54:55.155609: step 22, loss 1.99451, acc 0.515625
2016-12-13T08:54:59.862044: step 23, loss 2.07139, acc 0.546875
2016-12-13T08:55:04.495948: step 24, loss 2.2973, acc 0.5625
2016-12-13T08:55:09.069934: step 25, loss 1.79143, acc 0.578125
2016-12-13T08:55:13.784853: step 26, loss 2.15988, acc 0.546875
2016-12-13T08:55:18.296475: step 27, loss 1.97832, acc 0.53125
2016-12-13T08:55:22.978498: step 28, loss 1.41259, acc 0.65625
2016-12-13T08:55:27.297451: step 29, loss 1.61243, acc 0.5625
2016-12-13T08:55:31.697152: step 30, loss 2.46448, acc 0.40625
2016-12-13T08:55:36.078602: step 31, loss 1.55246, acc 0.609375
2016-12-13T08:55:40.742682: step 32, loss 2.16276, acc 0.5
2016-12-13T08:55:45.358022: step 33, loss 1.42889, acc 0.515625
2016-12-13T08:55:50.380433: step 34, loss 2.03682, acc 0.5
2016-12-13T08:55:55.281166: step 35, loss 2.36337, acc 0.53125
2016-12-13T08:55:59.763329: step 36, loss 1.78345, acc 0.53125
2016-12-13T08:56:04.257269: step 37, loss 1.91085, acc 0.53125
2016-12-13T08:56:08.630363: step 38, loss 1.93723, acc 0.59375
2016-12-13T08:56:13.105029: step 39, loss 1.94003, acc 0.4375
2016-12-13T08:56:17.706584: step 40, loss 1.22785, acc 0.640625
2016-12-13T08:56:22.737863: step 41, loss 1.74681, acc 0.578125
2016-12-13T08:56:27.419024: step 42, loss 1.85444, acc 0.546875
2016-12-13T08:56:32.338645: step 43, loss 1.15237, acc 0.640625
2016-12-13T08:56:37.573071: step 44, loss 1.53813, acc 0.65625
2016-12-13T08:56:42.050729: step 45, loss 2.25671, acc 0.484375
2016-12-13T08:56:46.535484: step 46, loss 1.62317, acc 0.5625
2016-12-13T08:56:51.045398: step 47, loss 1.21205, acc 0.625
2016-12-13T08:56:55.817792: step 48, loss 1.79775, acc 0.5625
2016-12-13T08:57:01.109690: step 49, loss 1.62037, acc 0.53125
2016-12-13T08:57:06.234062: step 50, loss 1.87247, acc 0.53125
2016-12-13T08:57:10.836072: step 51, loss 1.78791, acc 0.5
2016-12-13T08:57:15.345885: step 52, loss 1.91772, acc 0.515625
2016-12-13T08:57:19.885366: step 53, loss 1.51783, acc 0.65625
2016-12-13T08:57:24.711829: step 54, loss 2.07418, acc 0.546875
2016-12-13T08:57:29.404376: step 55, loss 1.67021, acc 0.515625
2016-12-13T08:57:34.438702: step 56, loss 1.44712, acc 0.578125
2016-12-13T08:57:39.002977: step 57, loss 1.50773, acc 0.546875
2016-12-13T08:57:43.647717: step 58, loss 2.06909, acc 0.453125
2016-12-13T08:57:48.256402: step 59, loss 1.32104, acc 0.640625
2016-12-13T08:57:52.716919: step 60, loss 1.27563, acc 0.640625
2016-12-13T08:57:57.159393: step 61, loss 1.76073, acc 0.390625
2016-12-13T08:58:01.636441: step 62, loss 1.42382, acc 0.625
2016-12-13T08:58:06.327021: step 63, loss 1.75677, acc 0.515625
2016-12-13T08:58:10.776804: step 64, loss 2.12328, acc 0.484375
2016-12-13T08:58:15.296258: step 65, loss 1.66075, acc 0.53125
2016-12-13T08:58:20.127825: step 66, loss 2.15963, acc 0.515625
2016-12-13T08:58:24.845429: step 67, loss 1.49653, acc 0.578125
2016-12-13T08:58:29.623056: step 68, loss 1.5343, acc 0.5625
2016-12-13T08:58:34.255510: step 69, loss 1.0789, acc 0.59375
2016-12-13T08:58:39.456656: step 70, loss 1.4144, acc 0.5
2016-12-13T08:58:44.411245: step 71, loss 1.4066, acc 0.5625
2016-12-13T08:58:48.928575: step 72, loss 1.91553, acc 0.515625
2016-12-13T08:58:53.862764: step 73, loss 1.07841, acc 0.59375
2016-12-13T08:58:59.030776: step 74, loss 1.55096, acc 0.609375
2016-12-13T08:59:03.864978: step 75, loss 1.20032, acc 0.640625
2016-12-13T08:59:08.536865: step 76, loss 1.3545, acc 0.609375
2016-12-13T08:59:13.315815: step 77, loss 2.08851, acc 0.453125
2016-12-13T08:59:18.127064: step 78, loss 1.73634, acc 0.46875
2016-12-13T08:59:23.559068: step 79, loss 1.69988, acc 0.46875
2016-12-13T08:59:28.335135: step 80, loss 1.44628, acc 0.578125
2016-12-13T08:59:33.055375: step 81, loss 1.65889, acc 0.53125
2016-12-13T08:59:37.584300: step 82, loss 1.69802, acc 0.484375
2016-12-13T08:59:42.394081: step 83, loss 1.29114, acc 0.5625
2016-12-13T08:59:47.048667: step 84, loss 1.30022, acc 0.609375
2016-12-13T08:59:51.655665: step 85, loss 1.78884, acc 0.453125
2016-12-13T08:59:56.090566: step 86, loss 1.31179, acc 0.625
2016-12-13T09:00:00.638181: step 87, loss 1.5439, acc 0.53125
2016-12-13T09:00:05.282832: step 88, loss 1.87735, acc 0.53125
2016-12-13T09:00:09.750409: step 89, loss 1.34222, acc 0.578125
2016-12-13T09:00:14.528042: step 90, loss 1.57357, acc 0.5
2016-12-13T09:00:19.010469: step 91, loss 1.50603, acc 0.5
2016-12-13T09:00:23.483119: step 92, loss 1.8678, acc 0.46875
2016-12-13T09:00:27.986608: step 93, loss 1.78267, acc 0.515625
2016-12-13T09:00:32.443308: step 94, loss 1.64204, acc 0.5
2016-12-13T09:00:37.067808: step 95, loss 1.66735, acc 0.359375
2016-12-13T09:00:41.871930: step 96, loss 1.45654, acc 0.578125
2016-12-13T09:00:46.919278: step 97, loss 1.62301, acc 0.5625
2016-12-13T09:00:51.910519: step 98, loss 1.47686, acc 0.578125
2016-12-13T09:00:56.636210: step 99, loss 1.5715, acc 0.5
2016-12-13T09:01:01.820940: step 100, loss 1.52217, acc 0.46875

Evaluation:
2016-12-13T09:03:21.948157: step 100, loss 0.729054, acc 0.588064

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481637188/checkpoints/model-100

2016-12-13T09:03:30.820634: step 101, loss 1.79167, acc 0.46875
2016-12-13T09:03:35.951761: step 102, loss 1.82147, acc 0.515625
2016-12-13T09:03:41.492781: step 103, loss 1.17923, acc 0.625
2016-12-13T09:03:47.577636: step 104, loss 1.07262, acc 0.546875
2016-12-13T09:03:53.721494: step 105, loss 1.32787, acc 0.53125
2016-12-13T09:03:58.638292: step 106, loss 1.38651, acc 0.59375
2016-12-13T09:04:03.499783: step 107, loss 1.82229, acc 0.46875
2016-12-13T09:04:08.124573: step 108, loss 1.77442, acc 0.546875
2016-12-13T09:04:13.643178: step 109, loss 1.45059, acc 0.515625
2016-12-13T09:04:19.323758: step 110, loss 1.71052, acc 0.515625
2016-12-13T09:04:24.585101: step 111, loss 1.33773, acc 0.53125
2016-12-13T09:04:29.531298: step 112, loss 1.02978, acc 0.640625
2016-12-13T09:04:34.740706: step 113, loss 1.17047, acc 0.59375
2016-12-13T09:04:39.317780: step 114, loss 1.40768, acc 0.53125
2016-12-13T09:04:43.821599: step 115, loss 1.2515, acc 0.5625
2016-12-13T09:04:48.349443: step 116, loss 2.34623, acc 0.46875
2016-12-13T09:04:53.276926: step 117, loss 1.00658, acc 0.609375
2016-12-13T09:04:58.035626: step 118, loss 1.03084, acc 0.625
2016-12-13T09:05:03.105294: step 119, loss 1.23433, acc 0.546875
2016-12-13T09:05:08.377810: step 120, loss 1.33851, acc 0.4375
2016-12-13T09:05:13.009708: step 121, loss 1.10111, acc 0.578125
2016-12-13T09:05:18.125646: step 122, loss 1.40525, acc 0.59375
2016-12-13T09:05:23.850964: step 123, loss 1.35169, acc 0.484375
2016-12-13T09:05:28.861042: step 124, loss 1.30053, acc 0.609375
2016-12-13T09:05:33.933730: step 125, loss 1.17527, acc 0.65625
2016-12-13T09:05:38.669343: step 126, loss 1.5252, acc 0.546875
2016-12-13T09:05:43.401806: step 127, loss 1.97536, acc 0.390625
2016-12-13T09:05:48.143733: step 128, loss 1.2997, acc 0.578125
2016-12-13T09:05:53.047450: step 129, loss 1.13888, acc 0.484375
2016-12-13T09:05:57.951075: step 130, loss 1.34991, acc 0.625
2016-12-13T09:06:02.823852: step 131, loss 1.59567, acc 0.5
2016-12-13T09:06:07.272980: step 132, loss 1.07954, acc 0.578125
2016-12-13T09:06:11.703323: step 133, loss 1.08628, acc 0.546875
2016-12-13T09:06:16.141613: step 134, loss 1.43439, acc 0.5
2016-12-13T09:06:20.559427: step 135, loss 1.33821, acc 0.46875
2016-12-13T09:06:25.047135: step 136, loss 1.51375, acc 0.515625
2016-12-13T09:06:29.445876: step 137, loss 1.3231, acc 0.578125
2016-12-13T09:06:33.913567: step 138, loss 1.38484, acc 0.546875
2016-12-13T09:06:38.497136: step 139, loss 1.46436, acc 0.4375
2016-12-13T09:06:42.978052: step 140, loss 1.51464, acc 0.453125
2016-12-13T09:06:47.534669: step 141, loss 1.65529, acc 0.390625
2016-12-13T09:06:51.999265: step 142, loss 1.43571, acc 0.515625
2016-12-13T09:06:56.400592: step 143, loss 1.20818, acc 0.484375
2016-12-13T09:07:00.886485: step 144, loss 1.08029, acc 0.53125
2016-12-13T09:07:05.757115: step 145, loss 1.15145, acc 0.5625
2016-12-13T09:07:10.935299: step 146, loss 0.908697, acc 0.625
2016-12-13T09:07:15.424611: step 147, loss 1.28348, acc 0.53125
2016-12-13T09:07:19.886736: step 148, loss 1.32651, acc 0.515625
2016-12-13T09:07:24.352394: step 149, loss 1.24227, acc 0.53125
2016-12-13T09:07:28.687655: step 150, loss 1.09428, acc 0.53125
2016-12-13T09:07:33.004298: step 151, loss 1.18581, acc 0.515625
2016-12-13T09:07:37.321238: step 152, loss 1.5191, acc 0.484375
2016-12-13T09:07:41.835233: step 153, loss 1.47535, acc 0.484375
2016-12-13T09:07:46.111054: step 154, loss 1.61631, acc 0.46875
2016-12-13T09:07:50.517457: step 155, loss 1.33091, acc 0.546875
2016-12-13T09:07:54.891319: step 156, loss 1.29614, acc 0.515625
2016-12-13T09:07:59.225149: step 157, loss 1.00467, acc 0.609375
2016-12-13T09:08:03.574756: step 158, loss 1.26447, acc 0.484375
2016-12-13T09:08:07.942621: step 159, loss 1.02621, acc 0.625
2016-12-13T09:08:12.456684: step 160, loss 1.12788, acc 0.5625
2016-12-13T09:08:16.808396: step 161, loss 1.26946, acc 0.578125
2016-12-13T09:08:21.134749: step 162, loss 0.856156, acc 0.640625
2016-12-13T09:08:25.428921: step 163, loss 1.32762, acc 0.4375
2016-12-13T09:08:29.736735: step 164, loss 1.23237, acc 0.515625
2016-12-13T09:08:34.099416: step 165, loss 1.1528, acc 0.5625
2016-12-13T09:08:38.371804: step 166, loss 1.15972, acc 0.5
2016-12-13T09:08:42.825268: step 167, loss 0.865438, acc 0.609375
2016-12-13T09:08:47.130871: step 168, loss 1.02933, acc 0.59375
2016-12-13T09:08:51.478009: step 169, loss 1.29144, acc 0.515625
2016-12-13T09:08:56.297785: step 170, loss 0.955745, acc 0.5625
2016-12-13T09:09:00.825623: step 171, loss 1.05028, acc 0.640625
2016-12-13T09:09:05.733986: step 172, loss 1.10691, acc 0.5625
2016-12-13T09:09:10.124256: step 173, loss 1.35947, acc 0.46875
2016-12-13T09:09:14.660123: step 174, loss 1.15096, acc 0.53125
2016-12-13T09:09:19.211116: step 175, loss 1.06471, acc 0.59375
2016-12-13T09:09:24.571124: step 176, loss 1.22906, acc 0.5
2016-12-13T09:09:29.137701: step 177, loss 1.03538, acc 0.546875
2016-12-13T09:09:33.570291: step 178, loss 0.984057, acc 0.578125
2016-12-13T09:09:37.975108: step 179, loss 1.25784, acc 0.515625
2016-12-13T09:09:42.484401: step 180, loss 1.46696, acc 0.421875
2016-12-13T09:09:47.081257: step 181, loss 1.17618, acc 0.640625
2016-12-13T09:09:51.541858: step 182, loss 1.41567, acc 0.40625
2016-12-13T09:09:55.927960: step 183, loss 1.20644, acc 0.484375
2016-12-13T09:10:00.302286: step 184, loss 1.53752, acc 0.34375
2016-12-13T09:10:04.673344: step 185, loss 0.833297, acc 0.515625
2016-12-13T09:10:09.025986: step 186, loss 0.879328, acc 0.609375
2016-12-13T09:10:13.409662: step 187, loss 1.13508, acc 0.5625
2016-12-13T09:10:17.827200: step 188, loss 1.08859, acc 0.640625
2016-12-13T09:10:22.327158: step 189, loss 0.775088, acc 0.671875
2016-12-13T09:10:26.713113: step 190, loss 1.22141, acc 0.53125
2016-12-13T09:10:31.135367: step 191, loss 1.2459, acc 0.515625
2016-12-13T09:10:35.476000: step 192, loss 0.763373, acc 0.640625
2016-12-13T09:10:39.869623: step 193, loss 1.01242, acc 0.5625
2016-12-13T09:10:44.287824: step 194, loss 1.01412, acc 0.53125
2016-12-13T09:10:48.666950: step 195, loss 1.16009, acc 0.53125
2016-12-13T09:10:53.282059: step 196, loss 0.892345, acc 0.640625
2016-12-13T09:10:57.670061: step 197, loss 1.26047, acc 0.59375
2016-12-13T09:11:02.069585: step 198, loss 1.19259, acc 0.5625
2016-12-13T09:11:06.489227: step 199, loss 1.24432, acc 0.53125
2016-12-13T09:11:10.894351: step 200, loss 0.894914, acc 0.5625

Evaluation:
2016-12-13T09:13:26.083472: step 200, loss 0.755337, acc 0.607715

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481637188/checkpoints/model-200

2016-12-13T09:13:34.238421: step 201, loss 0.994541, acc 0.640625
2016-12-13T09:13:38.599183: step 202, loss 0.703122, acc 0.703125
2016-12-13T09:13:42.979648: step 203, loss 1.12774, acc 0.5625
2016-12-13T09:13:47.368406: step 204, loss 1.0007, acc 0.609375
2016-12-13T09:13:51.706583: step 205, loss 1.28022, acc 0.515625
2016-12-13T09:13:56.207305: step 206, loss 0.752882, acc 0.640625
2016-12-13T09:14:00.581625: step 207, loss 1.10038, acc 0.4375
2016-12-13T09:14:04.956561: step 208, loss 0.901075, acc 0.59375
2016-12-13T09:14:09.357801: step 209, loss 1.23472, acc 0.4375
2016-12-13T09:14:13.725051: step 210, loss 0.897517, acc 0.546875
2016-12-13T09:14:18.186330: step 211, loss 1.02397, acc 0.5625
2016-12-13T09:14:22.560466: step 212, loss 1.17325, acc 0.53125
2016-12-13T09:14:27.439230: step 213, loss 1.27766, acc 0.53125
2016-12-13T09:14:31.951610: step 214, loss 1.01857, acc 0.546875
2016-12-13T09:14:36.438484: step 215, loss 1.57162, acc 0.359375
2016-12-13T09:14:40.880807: step 216, loss 1.10483, acc 0.515625
2016-12-13T09:14:45.696383: step 217, loss 0.746543, acc 0.640625
2016-12-13T09:14:50.124352: step 218, loss 1.20781, acc 0.53125
2016-12-13T09:14:54.839747: step 219, loss 1.44415, acc 0.546875
2016-12-13T09:14:59.428867: step 220, loss 1.36548, acc 0.515625
2016-12-13T09:15:03.827097: step 221, loss 0.814394, acc 0.5625
2016-12-13T09:15:08.274839: step 222, loss 0.870942, acc 0.5625
2016-12-13T09:15:12.735963: step 223, loss 1.25097, acc 0.53125
2016-12-13T09:15:17.274442: step 224, loss 0.850737, acc 0.5625
2016-12-13T09:15:21.733816: step 225, loss 0.966638, acc 0.421875
2016-12-13T09:15:26.145768: step 226, loss 0.820382, acc 0.609375
2016-12-13T09:15:30.623627: step 227, loss 0.955893, acc 0.53125
2016-12-13T09:15:35.202707: step 228, loss 1.18619, acc 0.453125
2016-12-13T09:15:39.528503: step 229, loss 0.945343, acc 0.515625
2016-12-13T09:15:43.915194: step 230, loss 0.768714, acc 0.671875
2016-12-13T09:15:48.350159: step 231, loss 0.930057, acc 0.6875
2016-12-13T09:15:52.691944: step 232, loss 0.944573, acc 0.53125
2016-12-13T09:15:57.102957: step 233, loss 0.930611, acc 0.59375
2016-12-13T09:16:01.501790: step 234, loss 1.04445, acc 0.53125
2016-12-13T09:16:06.129263: step 235, loss 1.06609, acc 0.5
2016-12-13T09:16:10.533553: step 236, loss 0.821676, acc 0.609375
2016-12-13T09:16:14.905747: step 237, loss 0.837704, acc 0.640625
2016-12-13T09:16:19.336525: step 238, loss 1.16526, acc 0.515625
2016-12-13T09:16:23.755749: step 239, loss 0.956432, acc 0.5625
2016-12-13T09:16:28.165942: step 240, loss 1.04266, acc 0.53125
2016-12-13T09:16:32.553691: step 241, loss 0.992967, acc 0.53125
2016-12-13T09:16:37.126893: step 242, loss 1.01117, acc 0.5
2016-12-13T09:16:41.566937: step 243, loss 1.11959, acc 0.515625
2016-12-13T09:16:45.962013: step 244, loss 0.820865, acc 0.6875
2016-12-13T09:16:50.387831: step 245, loss 0.907995, acc 0.546875
2016-12-13T09:16:54.751929: step 246, loss 1.05799, acc 0.5
2016-12-13T09:16:59.148219: step 247, loss 0.971025, acc 0.5
2016-12-13T09:17:03.514259: step 248, loss 1.03594, acc 0.53125
2016-12-13T09:17:08.048454: step 249, loss 0.920897, acc 0.625
2016-12-13T09:17:12.423237: step 250, loss 0.883724, acc 0.546875
2016-12-13T09:17:16.871840: step 251, loss 0.774199, acc 0.640625
2016-12-13T09:17:21.294949: step 252, loss 1.06023, acc 0.53125
2016-12-13T09:17:25.731157: step 253, loss 0.815522, acc 0.578125
2016-12-13T09:17:30.207902: step 254, loss 0.996875, acc 0.515625
2016-12-13T09:17:34.561950: step 255, loss 1.06663, acc 0.53125
2016-12-13T09:17:38.988276: step 256, loss 0.85452, acc 0.546875
2016-12-13T09:17:43.456676: step 257, loss 0.951091, acc 0.609375
2016-12-13T09:17:47.873393: step 258, loss 0.894355, acc 0.59375
2016-12-13T09:17:52.236298: step 259, loss 0.987631, acc 0.453125
2016-12-13T09:17:56.698830: step 260, loss 0.926217, acc 0.5625
2016-12-13T09:18:01.396507: step 261, loss 0.881772, acc 0.578125
2016-12-13T09:18:06.388838: step 262, loss 0.969812, acc 0.484375
2016-12-13T09:18:10.806405: step 263, loss 0.921874, acc 0.515625
2016-12-13T09:18:15.258716: step 264, loss 0.900684, acc 0.5625
2016-12-13T09:18:19.620857: step 265, loss 0.788608, acc 0.625
2016-12-13T09:18:23.993416: step 266, loss 0.931756, acc 0.453125
2016-12-13T09:18:28.398037: step 267, loss 0.683186, acc 0.640625
2016-12-13T09:18:32.772327: step 268, loss 0.834168, acc 0.609375
2016-12-13T09:18:37.135146: step 269, loss 0.887211, acc 0.578125
2016-12-13T09:18:41.532494: step 270, loss 0.775255, acc 0.578125
2016-12-13T09:18:46.075615: step 271, loss 0.88295, acc 0.546875
2016-12-13T09:18:50.522568: step 272, loss 0.820955, acc 0.671875
2016-12-13T09:18:54.903773: step 273, loss 0.845782, acc 0.546875
2016-12-13T09:18:59.298697: step 274, loss 0.912151, acc 0.515625
2016-12-13T09:19:03.672495: step 275, loss 0.769381, acc 0.65625
2016-12-13T09:19:08.052438: step 276, loss 0.875157, acc 0.546875
2016-12-13T09:19:12.425518: step 277, loss 0.78215, acc 0.640625
2016-12-13T09:19:16.956038: step 278, loss 0.713014, acc 0.5625
2016-12-13T09:19:21.388838: step 279, loss 0.857379, acc 0.59375
2016-12-13T09:19:25.811270: step 280, loss 0.71154, acc 0.59375
2016-12-13T09:19:30.170358: step 281, loss 0.969131, acc 0.5
2016-12-13T09:19:34.569390: step 282, loss 0.926947, acc 0.546875
2016-12-13T09:19:38.939995: step 283, loss 0.769573, acc 0.59375
2016-12-13T09:19:43.329299: step 284, loss 0.726127, acc 0.546875
2016-12-13T09:19:47.882179: step 285, loss 0.848646, acc 0.546875
2016-12-13T09:19:52.273714: step 286, loss 0.919371, acc 0.515625
2016-12-13T09:19:56.672800: step 287, loss 0.809341, acc 0.609375
2016-12-13T09:20:01.071108: step 288, loss 0.857558, acc 0.5625
2016-12-13T09:20:05.415838: step 289, loss 0.719831, acc 0.609375
2016-12-13T09:20:09.823377: step 290, loss 1.04877, acc 0.4375
2016-12-13T09:20:14.216057: step 291, loss 0.886599, acc 0.5
2016-12-13T09:20:18.710346: step 292, loss 0.986692, acc 0.484375
2016-12-13T09:20:23.189912: step 293, loss 1.03661, acc 0.46875
2016-12-13T09:20:27.550380: step 294, loss 0.698292, acc 0.640625
2016-12-13T09:20:31.977433: step 295, loss 0.73516, acc 0.703125
2016-12-13T09:20:36.357104: step 296, loss 0.904258, acc 0.625
2016-12-13T09:20:40.765605: step 297, loss 0.771906, acc 0.59375
2016-12-13T09:20:45.094461: step 298, loss 0.941214, acc 0.53125
2016-12-13T09:20:49.509306: step 299, loss 0.867844, acc 0.59375
2016-12-13T09:20:54.082649: step 300, loss 0.871526, acc 0.5625

Evaluation:
2016-12-13T09:22:34.635448: step 300, loss 0.6648, acc 0.609898

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481637188/checkpoints/model-300

2016-12-13T09:22:42.418730: step 301, loss 0.934113, acc 0.484375
2016-12-13T09:22:46.823817: step 302, loss 0.84361, acc 0.578125
2016-12-13T09:22:51.220062: step 303, loss 0.776222, acc 0.59375
2016-12-13T09:22:55.678570: step 304, loss 0.857224, acc 0.59375
2016-12-13T09:23:00.160958: step 305, loss 1.05479, acc 0.4375
2016-12-13T09:23:05.224022: step 306, loss 0.812973, acc 0.53125
2016-12-13T09:23:09.715014: step 307, loss 0.82516, acc 0.53125
2016-12-13T09:23:14.326356: step 308, loss 0.82405, acc 0.578125
2016-12-13T09:23:18.731265: step 309, loss 0.698565, acc 0.65625
2016-12-13T09:23:23.140729: step 310, loss 0.924666, acc 0.5
2016-12-13T09:23:27.478704: step 311, loss 1.00079, acc 0.546875
2016-12-13T09:23:31.846935: step 312, loss 0.568954, acc 0.6875
2016-12-13T09:23:36.219773: step 313, loss 0.786726, acc 0.625
2016-12-13T09:23:40.574574: step 314, loss 1.30432, acc 0.40625
2016-12-13T09:23:45.010090: step 315, loss 0.786842, acc 0.5
2016-12-13T09:23:49.521806: step 316, loss 0.902367, acc 0.546875
2016-12-13T09:23:53.904939: step 317, loss 0.80064, acc 0.625
2016-12-13T09:23:58.338766: step 318, loss 0.922512, acc 0.46875
2016-12-13T09:24:02.995877: step 319, loss 0.905522, acc 0.5
2016-12-13T09:24:07.500359: step 320, loss 0.869275, acc 0.53125
2016-12-13T09:24:11.881161: step 321, loss 0.868856, acc 0.53125
2016-12-13T09:24:16.464711: step 322, loss 0.913954, acc 0.625
2016-12-13T09:24:21.272675: step 323, loss 0.853805, acc 0.59375
2016-12-13T09:24:25.727361: step 324, loss 0.96481, acc 0.578125
2016-12-13T09:24:30.128180: step 325, loss 1.00743, acc 0.625
2016-12-13T09:24:34.517124: step 326, loss 0.735213, acc 0.625
2016-12-13T09:24:38.906147: step 327, loss 0.805885, acc 0.59375
2016-12-13T09:24:43.330448: step 328, loss 0.929426, acc 0.5
2016-12-13T09:24:47.726448: step 329, loss 0.714553, acc 0.5625
2016-12-13T09:24:52.278618: step 330, loss 0.840994, acc 0.53125
2016-12-13T09:24:56.688159: step 331, loss 0.664355, acc 0.640625
2016-12-13T09:25:01.026873: step 332, loss 0.969446, acc 0.46875
2016-12-13T09:25:05.397603: step 333, loss 0.898316, acc 0.46875
2016-12-13T09:25:09.784222: step 334, loss 0.871029, acc 0.546875
2016-12-13T09:25:14.288343: step 335, loss 0.672027, acc 0.65625
2016-12-13T09:25:18.745113: step 336, loss 0.994338, acc 0.5
2016-12-13T09:25:23.326152: step 337, loss 0.837064, acc 0.59375
2016-12-13T09:25:27.791564: step 338, loss 0.940645, acc 0.5
2016-12-13T09:25:32.334161: step 339, loss 0.78765, acc 0.53125
2016-12-13T09:25:36.747616: step 340, loss 0.760596, acc 0.546875
2016-12-13T09:25:41.116689: step 341, loss 0.791423, acc 0.5
2016-12-13T09:25:45.455574: step 342, loss 0.723915, acc 0.59375
2016-12-13T09:25:49.848067: step 343, loss 0.740073, acc 0.625
2016-12-13T09:25:54.423490: step 344, loss 0.779114, acc 0.5625
2016-12-13T09:25:58.791841: step 345, loss 0.849068, acc 0.5
2016-12-13T09:26:03.188460: step 346, loss 0.714651, acc 0.640625
2016-12-13T09:26:07.531242: step 347, loss 0.906286, acc 0.515625
2016-12-13T09:26:11.908424: step 348, loss 0.887699, acc 0.578125
2016-12-13T09:26:16.283408: step 349, loss 0.7904, acc 0.5625
2016-12-13T09:26:20.650275: step 350, loss 0.784151, acc 0.578125
2016-12-13T09:26:25.102543: step 351, loss 0.778898, acc 0.578125
2016-12-13T09:26:29.562600: step 352, loss 0.805198, acc 0.546875
2016-12-13T09:26:33.958461: step 353, loss 0.906962, acc 0.453125
2016-12-13T09:26:38.356293: step 354, loss 0.795478, acc 0.484375
2016-12-13T09:26:42.783411: step 355, loss 0.671723, acc 0.609375
2016-12-13T09:26:47.174720: step 356, loss 0.572381, acc 0.71875
2016-12-13T09:26:51.586710: step 357, loss 0.926013, acc 0.5
2016-12-13T09:26:55.956730: step 358, loss 0.768389, acc 0.5625
2016-12-13T09:27:00.498718: step 359, loss 0.839211, acc 0.546875
2016-12-13T09:27:04.913658: step 360, loss 0.78317, acc 0.5625
2016-12-13T09:27:09.310434: step 361, loss 0.765524, acc 0.59375
2016-12-13T09:27:13.700641: step 362, loss 0.723746, acc 0.59375
2016-12-13T09:27:18.087094: step 363, loss 0.862189, acc 0.46875
2016-12-13T09:27:22.424883: step 364, loss 0.746298, acc 0.484375
2016-12-13T09:27:26.812332: step 365, loss 0.825044, acc 0.515625
2016-12-13T09:27:31.353544: step 366, loss 0.825114, acc 0.53125
2016-12-13T09:27:35.774550: step 367, loss 0.684488, acc 0.5625
2016-12-13T09:27:40.166774: step 368, loss 0.708174, acc 0.59375
2016-12-13T09:27:44.624336: step 369, loss 0.769491, acc 0.640625
2016-12-13T09:27:49.031414: step 370, loss 0.652045, acc 0.6875
2016-12-13T09:27:53.427349: step 371, loss 0.79601, acc 0.53125
2016-12-13T09:27:57.843125: step 372, loss 0.734748, acc 0.609375
2016-12-13T09:28:02.895108: step 373, loss 0.704225, acc 0.609375
2016-12-13T09:28:07.367792: step 374, loss 0.744532, acc 0.53125
2016-12-13T09:28:12.009299: step 375, loss 0.704293, acc 0.625
2016-12-13T09:28:16.524463: step 376, loss 0.77704, acc 0.640625
2016-12-13T09:28:20.913074: step 377, loss 0.826985, acc 0.546875
2016-12-13T09:28:25.375501: step 378, loss 0.804966, acc 0.53125
2016-12-13T09:28:29.757376: step 379, loss 0.874442, acc 0.5
2016-12-13T09:28:34.312817: step 380, loss 0.780152, acc 0.59375
2016-12-13T09:28:38.689765: step 381, loss 0.786097, acc 0.421875
2016-12-13T09:28:43.085615: step 382, loss 0.757207, acc 0.59375
2016-12-13T09:28:47.506250: step 383, loss 0.81479, acc 0.5625
2016-12-13T09:28:51.809698: step 384, loss 0.713981, acc 0.59375
2016-12-13T09:28:56.214400: step 385, loss 0.706899, acc 0.609375
2016-12-13T09:29:00.553425: step 386, loss 0.755714, acc 0.609375
2016-12-13T09:29:04.978715: step 387, loss 0.805895, acc 0.5
2016-12-13T09:29:09.485186: step 388, loss 0.769105, acc 0.5
2016-12-13T09:29:13.877933: step 389, loss 0.707944, acc 0.546875
2016-12-13T09:29:18.215238: step 390, loss 0.693099, acc 0.625
2016-12-13T09:29:22.617952: step 391, loss 0.820144, acc 0.5
2016-12-13T09:29:26.991597: step 392, loss 0.70194, acc 0.515625
2016-12-13T09:29:31.350382: step 393, loss 0.878857, acc 0.5
2016-12-13T09:29:35.723263: step 394, loss 0.65776, acc 0.625
2016-12-13T09:29:40.226081: step 395, loss 0.697826, acc 0.625
2016-12-13T09:29:44.654064: step 396, loss 0.708293, acc 0.65625
2016-12-13T09:29:49.040171: step 397, loss 0.73193, acc 0.625
2016-12-13T09:29:53.466318: step 398, loss 0.748262, acc 0.59375
2016-12-13T09:29:57.841230: step 399, loss 0.71313, acc 0.5625
2016-12-13T09:30:02.208260: step 400, loss 0.784571, acc 0.5625

Evaluation:
2016-12-13T09:31:45.850402: step 400, loss 0.650972, acc 0.613537

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481637188/checkpoints/model-400

2016-12-13T09:31:53.430825: step 401, loss 0.681842, acc 0.609375
2016-12-13T09:31:57.782378: step 402, loss 0.73755, acc 0.515625
2016-12-13T09:32:02.197062: step 403, loss 0.774485, acc 0.5
2016-12-13T09:32:06.880800: step 404, loss 0.753432, acc 0.59375
2016-12-13T09:32:11.328580: step 405, loss 0.746501, acc 0.5
2016-12-13T09:32:15.684331: step 406, loss 0.846826, acc 0.484375
2016-12-13T09:32:20.062762: step 407, loss 0.69929, acc 0.578125
2016-12-13T09:32:24.501269: step 408, loss 0.656745, acc 0.640625
2016-12-13T09:32:28.848442: step 409, loss 0.711622, acc 0.546875
2016-12-13T09:32:33.207621: step 410, loss 0.699887, acc 0.65625
2016-12-13T09:32:37.801688: step 411, loss 0.685634, acc 0.59375
2016-12-13T09:32:42.156052: step 412, loss 0.826647, acc 0.5625
2016-12-13T09:32:46.558081: step 413, loss 0.666112, acc 0.609375
2016-12-13T09:32:50.986308: step 414, loss 0.688789, acc 0.578125
2016-12-13T09:32:55.482322: step 415, loss 0.667125, acc 0.59375
2016-12-13T09:32:59.949593: step 416, loss 0.776443, acc 0.546875
2016-12-13T09:33:05.149443: step 417, loss 0.658544, acc 0.609375
2016-12-13T09:33:09.755113: step 418, loss 0.686541, acc 0.609375
2016-12-13T09:33:14.194307: step 419, loss 0.736472, acc 0.5625
2016-12-13T09:33:18.609869: step 420, loss 0.809227, acc 0.59375
2016-12-13T09:33:22.999254: step 421, loss 0.770655, acc 0.578125
2016-12-13T09:33:27.398980: step 422, loss 0.777657, acc 0.546875
2016-12-13T09:33:31.814668: step 423, loss 0.725609, acc 0.5625
2016-12-13T09:33:36.227321: step 424, loss 0.696416, acc 0.578125
2016-12-13T09:33:40.713970: step 425, loss 0.775015, acc 0.484375
2016-12-13T09:33:45.132044: step 426, loss 0.728919, acc 0.546875
2016-12-13T09:33:49.531055: step 427, loss 0.781597, acc 0.578125
2016-12-13T09:33:53.911367: step 428, loss 0.740624, acc 0.484375
2016-12-13T09:33:58.352668: step 429, loss 0.685232, acc 0.59375
2016-12-13T09:34:02.722153: step 430, loss 0.620015, acc 0.6875
2016-12-13T09:34:07.160545: step 431, loss 0.667839, acc 0.59375
2016-12-13T09:34:11.543951: step 432, loss 0.913909, acc 0.578125
2016-12-13T09:34:16.077062: step 433, loss 0.738887, acc 0.609375
2016-12-13T09:34:20.502969: step 434, loss 0.661169, acc 0.609375
2016-12-13T09:34:24.894102: step 435, loss 0.767496, acc 0.546875
2016-12-13T09:34:29.269174: step 436, loss 0.628024, acc 0.5625
2016-12-13T09:34:33.693237: step 437, loss 0.637796, acc 0.625
2016-12-13T09:34:38.097180: step 438, loss 0.692364, acc 0.5625
2016-12-13T09:34:42.472240: step 439, loss 0.774904, acc 0.5625
2016-12-13T09:34:47.056183: step 440, loss 0.746046, acc 0.5
2016-12-13T09:34:51.446036: step 441, loss 0.697507, acc 0.5625
2016-12-13T09:34:55.852021: step 442, loss 0.666829, acc 0.59375
2016-12-13T09:35:00.213201: step 443, loss 0.723344, acc 0.53125
2016-12-13T09:35:04.599334: step 444, loss 0.637898, acc 0.625
2016-12-13T09:35:08.959203: step 445, loss 0.63707, acc 0.65625
2016-12-13T09:35:13.364856: step 446, loss 0.673981, acc 0.65625
2016-12-13T09:35:17.946154: step 447, loss 0.823848, acc 0.578125
2016-12-13T09:35:22.373280: step 448, loss 0.691977, acc 0.640625
2016-12-13T09:35:26.774896: step 449, loss 0.789003, acc 0.515625
2016-12-13T09:35:31.155123: step 450, loss 0.828667, acc 0.359375
2016-12-13T09:35:35.518844: step 451, loss 0.745273, acc 0.546875
2016-12-13T09:35:39.918275: step 452, loss 0.646061, acc 0.625
2016-12-13T09:35:44.286912: step 453, loss 0.602828, acc 0.640625
2016-12-13T09:35:48.753015: step 454, loss 0.67908, acc 0.578125
2016-12-13T09:35:53.225871: step 455, loss 0.568958, acc 0.625
2016-12-13T09:35:57.624184: step 456, loss 0.770371, acc 0.546875
2016-12-13T09:36:02.015752: step 457, loss 0.739337, acc 0.5625
2016-12-13T09:36:06.380654: step 458, loss 0.749772, acc 0.53125
2016-12-13T09:36:10.755578: step 459, loss 0.713755, acc 0.640625
2016-12-13T09:36:15.169994: step 460, loss 0.719206, acc 0.46875
2016-12-13T09:36:19.577409: step 461, loss 0.778178, acc 0.5
2016-12-13T09:36:24.119490: step 462, loss 0.698348, acc 0.546875
2016-12-13T09:36:28.513815: step 463, loss 0.687053, acc 0.515625
2016-12-13T09:36:32.887433: step 464, loss 0.726812, acc 0.5625
2016-12-13T09:36:37.293597: step 465, loss 0.765718, acc 0.53125
2016-12-13T09:36:41.683440: step 466, loss 0.713998, acc 0.578125
2016-12-13T09:36:46.060902: step 467, loss 0.724333, acc 0.546875
2016-12-13T09:36:50.389721: step 468, loss 0.660432, acc 0.65625
2016-12-13T09:36:54.970276: step 469, loss 0.790986, acc 0.4375
2016-12-13T09:36:59.321041: step 470, loss 0.654331, acc 0.625
2016-12-13T09:37:03.692363: step 471, loss 0.660404, acc 0.59375
2016-12-13T09:37:08.080218: step 472, loss 0.684278, acc 0.5625
2016-12-13T09:37:12.510379: step 473, loss 0.723346, acc 0.578125
2016-12-13T09:37:16.893724: step 474, loss 0.681838, acc 0.609375
2016-12-13T09:37:21.291720: step 475, loss 0.777104, acc 0.5
2016-12-13T09:37:25.908382: step 476, loss 0.707414, acc 0.609375
2016-12-13T09:37:30.616007: step 477, loss 0.665851, acc 0.65625
2016-12-13T09:37:35.030041: step 478, loss 0.681203, acc 0.515625
2016-12-13T09:37:39.416312: step 479, loss 0.716197, acc 0.53125
2016-12-13T09:37:43.715255: step 480, loss 0.785046, acc 0.5
2016-12-13T09:37:48.050396: step 481, loss 0.779729, acc 0.515625
2016-12-13T09:37:52.332306: step 482, loss 0.727239, acc 0.5
2016-12-13T09:37:56.684210: step 483, loss 0.711377, acc 0.578125
2016-12-13T09:38:01.295401: step 484, loss 0.656666, acc 0.59375
2016-12-13T10:37:58.505174: step 485, loss 0.715305, acc 0.546875
2016-12-13T10:38:04.213073: step 486, loss 0.724584, acc 0.578125
2016-12-13T10:38:18.085710: step 487, loss 0.687488, acc 0.59375
2016-12-13T10:38:36.179179: step 488, loss 0.660207, acc 0.5625
2016-12-13T11:38:44.404403: step 489, loss 0.595071, acc 0.65625
2016-12-13T11:38:49.510129: step 490, loss 0.670543, acc 0.625
2016-12-13T11:38:55.281511: step 491, loss 0.576214, acc 0.6875
2016-12-13T11:39:12.746601: step 492, loss 0.68986, acc 0.65625
2016-12-13T11:39:29.900723: step 493, loss 0.858006, acc 0.53125
2016-12-13T11:39:46.572826: step 494, loss 0.831988, acc 0.515625
2016-12-13T11:50:58.584582: step 495, loss 0.647699, acc 0.640625
2016-12-13T11:51:05.263599: step 496, loss 0.670905, acc 0.5625
2016-12-13T11:51:09.886912: step 497, loss 0.688708, acc 0.53125
2016-12-13T11:51:14.411014: step 498, loss 0.823946, acc 0.375
2016-12-13T11:51:18.581030: step 499, loss 0.686864, acc 0.59375
2016-12-13T11:51:22.702899: step 500, loss 0.677325, acc 0.5

Evaluation:
2016-12-13T11:53:44.756659: step 500, loss 0.658842, acc 0.634643

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481637188/checkpoints/model-500

2016-12-13T11:53:52.974412: step 501, loss 0.6534, acc 0.578125
2016-12-13T11:53:57.410824: step 502, loss 0.702764, acc 0.53125
2016-12-13T11:54:01.911933: step 503, loss 0.706869, acc 0.578125
2016-12-13T11:54:06.633601: step 504, loss 0.748296, acc 0.578125
2016-12-13T11:54:11.694882: step 505, loss 0.635908, acc 0.65625
2016-12-13T11:54:16.381636: step 506, loss 0.721192, acc 0.5
2016-12-13T11:54:20.944539: step 507, loss 0.693665, acc 0.515625
2016-12-13T11:54:25.416589: step 508, loss 0.737998, acc 0.4375
2016-12-13T11:54:29.682362: step 509, loss 0.726885, acc 0.515625
2016-12-13T11:54:33.932821: step 510, loss 0.736438, acc 0.453125
2016-12-13T11:54:38.297852: step 511, loss 0.75254, acc 0.453125
2016-12-13T11:54:42.958496: step 512, loss 0.699599, acc 0.484375
2016-12-13T11:54:47.209183: step 513, loss 0.619565, acc 0.609375
2016-12-13T11:54:51.438458: step 514, loss 0.765853, acc 0.453125
2016-12-13T11:54:55.673949: step 515, loss 0.678598, acc 0.625
2016-12-13T11:55:00.290247: step 516, loss 0.648994, acc 0.609375
2016-12-13T11:55:04.674825: step 517, loss 0.718479, acc 0.578125
2016-12-13T11:55:09.094836: step 518, loss 0.744142, acc 0.546875
2016-12-13T11:55:13.620387: step 519, loss 0.686257, acc 0.640625
2016-12-13T11:55:18.185373: step 520, loss 0.646917, acc 0.578125
2016-12-13T11:55:23.205236: step 521, loss 0.682723, acc 0.515625
2016-12-13T11:55:27.521916: step 522, loss 0.697414, acc 0.515625
2016-12-13T11:55:31.812981: step 523, loss 0.713143, acc 0.546875
2016-12-13T11:55:36.204312: step 524, loss 0.71525, acc 0.578125
2016-12-13T11:55:41.041031: step 525, loss 0.676794, acc 0.609375
2016-12-13T11:55:45.785721: step 526, loss 0.668156, acc 0.640625
2016-12-13T11:55:50.527447: step 527, loss 0.640448, acc 0.640625
2016-12-13T11:55:55.693606: step 528, loss 0.763569, acc 0.5625
2016-12-13T11:56:01.235616: step 529, loss 0.706085, acc 0.609375
2016-12-13T11:56:06.193138: step 530, loss 0.733347, acc 0.609375
2016-12-13T11:56:11.139112: step 531, loss 0.677225, acc 0.609375
2016-12-13T11:56:15.980620: step 532, loss 0.682781, acc 0.53125
2016-12-13T11:56:20.456908: step 533, loss 0.687571, acc 0.609375
2016-12-13T11:56:24.996200: step 534, loss 0.698293, acc 0.5625
2016-12-13T11:56:30.743268: step 535, loss 0.707323, acc 0.53125
2016-12-13T11:56:35.166584: step 536, loss 0.707957, acc 0.46875
2016-12-13T11:56:39.687004: step 537, loss 0.717544, acc 0.5625
2016-12-13T11:56:44.668467: step 538, loss 0.679629, acc 0.5
2016-12-13T11:56:49.238025: step 539, loss 0.688739, acc 0.625
2016-12-13T11:56:53.792625: step 540, loss 0.677614, acc 0.59375
2016-12-13T11:56:58.165175: step 541, loss 0.692133, acc 0.546875
2016-12-13T11:57:02.743739: step 542, loss 0.652296, acc 0.640625
2016-12-13T11:57:07.211535: step 543, loss 0.797572, acc 0.5
2016-12-13T11:57:11.668584: step 544, loss 0.747145, acc 0.5625
2016-12-13T11:57:16.058891: step 545, loss 0.689149, acc 0.515625
2016-12-13T11:57:21.098173: step 546, loss 0.671968, acc 0.5625
2016-12-13T11:57:25.558846: step 547, loss 0.674082, acc 0.609375
2016-12-13T11:57:29.981010: step 548, loss 0.653937, acc 0.59375
2016-12-13T11:57:34.499000: step 549, loss 0.662321, acc 0.609375
2016-12-13T11:57:38.864695: step 550, loss 0.702927, acc 0.65625
2016-12-13T11:57:43.188616: step 551, loss 0.588262, acc 0.703125
2016-12-13T11:57:47.586251: step 552, loss 0.717512, acc 0.59375
2016-12-13T11:57:51.875476: step 553, loss 0.68392, acc 0.609375
2016-12-13T11:57:56.432925: step 554, loss 0.726851, acc 0.546875
2016-12-13T11:58:01.039067: step 555, loss 0.754254, acc 0.515625
2016-12-13T11:58:05.473494: step 556, loss 0.681034, acc 0.5625
2016-12-13T11:58:09.870411: step 557, loss 0.680178, acc 0.53125
2016-12-13T11:58:14.183054: step 558, loss 0.656405, acc 0.578125
2016-12-13T11:58:18.474164: step 559, loss 0.656435, acc 0.640625
2016-12-13T11:58:22.754886: step 560, loss 0.648807, acc 0.65625
2016-12-13T11:58:27.207528: step 561, loss 0.637568, acc 0.625
2016-12-13T11:58:31.485660: step 562, loss 0.652987, acc 0.5625
2016-12-13T11:58:35.792954: step 563, loss 0.629386, acc 0.625
2016-12-13T11:58:40.057893: step 564, loss 0.725104, acc 0.515625
2016-12-13T11:58:44.410510: step 565, loss 0.598745, acc 0.71875
2016-12-13T11:58:48.669265: step 566, loss 0.724284, acc 0.515625
2016-12-13T11:58:52.941636: step 567, loss 0.713096, acc 0.515625
2016-12-13T11:58:57.830949: step 568, loss 0.771901, acc 0.53125
2016-12-13T11:59:02.558569: step 569, loss 0.725108, acc 0.484375
2016-12-13T11:59:07.265431: step 570, loss 0.717834, acc 0.546875
2016-12-13T11:59:12.089361: step 571, loss 0.71066, acc 0.5
2016-12-13T11:59:16.972168: step 572, loss 0.668907, acc 0.578125
2016-12-13T11:59:21.711962: step 573, loss 0.641755, acc 0.625
2016-12-13T11:59:26.894814: step 574, loss 0.761168, acc 0.515625
2016-12-13T11:59:32.051441: step 575, loss 0.605694, acc 0.703125
2016-12-13T11:59:36.881925: step 576, loss 0.826169, acc 0.453125
2016-12-13T11:59:41.775125: step 577, loss 0.654294, acc 0.5625
2016-12-13T11:59:46.511693: step 578, loss 0.68438, acc 0.578125
2016-12-13T11:59:51.342472: step 579, loss 0.697798, acc 0.625
2016-12-13T11:59:56.028842: step 580, loss 0.682316, acc 0.546875
2016-12-13T12:00:01.285413: step 581, loss 0.669083, acc 0.609375
2016-12-13T12:00:07.408439: step 582, loss 0.723798, acc 0.578125
2016-12-13T12:00:12.371069: step 583, loss 0.684766, acc 0.59375
2016-12-13T12:00:17.224151: step 584, loss 0.65432, acc 0.65625
2016-12-13T12:00:22.329926: step 585, loss 0.648114, acc 0.546875
2016-12-13T12:00:27.726487: step 586, loss 0.653899, acc 0.578125
2016-12-13T12:00:33.369906: step 587, loss 0.623334, acc 0.796875
2016-12-13T12:00:38.286410: step 588, loss 0.70969, acc 0.5625
2016-12-13T12:00:43.495024: step 589, loss 0.731202, acc 0.46875
2016-12-13T12:00:48.619962: step 590, loss 0.715556, acc 0.546875
2016-12-13T12:00:53.353904: step 591, loss 0.710543, acc 0.5
2016-12-13T12:00:58.163804: step 592, loss 0.716055, acc 0.5625
2016-12-13T12:01:03.191911: step 593, loss 0.707903, acc 0.5
2016-12-13T12:01:08.232304: step 594, loss 0.625834, acc 0.609375
2016-12-13T12:01:13.657581: step 595, loss 0.607219, acc 0.625
2016-12-13T12:01:18.481985: step 596, loss 0.695632, acc 0.609375
2016-12-13T12:01:23.229026: step 597, loss 0.803153, acc 0.5625
2016-12-13T12:01:28.039518: step 598, loss 0.770841, acc 0.578125
2016-12-13T12:01:32.940149: step 599, loss 0.652988, acc 0.640625
2016-12-13T12:01:37.748473: step 600, loss 0.662265, acc 0.609375

Evaluation:
2016-12-13T12:03:53.136494: step 600, loss 0.654616, acc 0.609898

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481637188/checkpoints/model-600

2016-12-13T12:04:01.184334: step 601, loss 0.700966, acc 0.53125
2016-12-13T12:04:05.793521: step 602, loss 0.71782, acc 0.546875
2016-12-13T12:04:10.389587: step 603, loss 0.622627, acc 0.6875
2016-12-13T12:04:14.791052: step 604, loss 0.684487, acc 0.46875
2016-12-13T12:04:19.210538: step 605, loss 0.730666, acc 0.484375
2016-12-13T12:04:23.663677: step 606, loss 0.683314, acc 0.484375
2016-12-13T12:04:28.029536: step 607, loss 0.629968, acc 0.671875
2016-12-13T12:04:32.551433: step 608, loss 0.665963, acc 0.578125
2016-12-13T12:04:37.121406: step 609, loss 0.654106, acc 0.5625
2016-12-13T12:04:41.644548: step 610, loss 0.637171, acc 0.640625
2016-12-13T12:04:46.110102: step 611, loss 0.735477, acc 0.546875
2016-12-13T12:04:50.593372: step 612, loss 0.680236, acc 0.578125
2016-12-13T12:04:55.135043: step 613, loss 0.667526, acc 0.640625
2016-12-13T12:04:59.881486: step 614, loss 0.715485, acc 0.609375
2016-12-13T12:05:04.524453: step 615, loss 0.719099, acc 0.53125
2016-12-13T12:05:09.135888: step 616, loss 0.656007, acc 0.625
2016-12-13T12:05:13.539115: step 617, loss 0.756147, acc 0.546875
2016-12-13T12:05:17.986835: step 618, loss 0.70293, acc 0.5625
2016-12-13T12:05:22.640794: step 619, loss 0.727927, acc 0.5
2016-12-13T12:05:27.187272: step 620, loss 0.649453, acc 0.625
2016-12-13T12:05:31.530401: step 621, loss 0.655746, acc 0.59375
2016-12-13T12:05:35.875294: step 622, loss 0.691335, acc 0.59375
2016-12-13T12:05:40.719209: step 623, loss 0.677476, acc 0.59375
2016-12-13T12:05:45.378352: step 624, loss 0.941918, acc 0.515625
2016-12-13T12:05:50.050865: step 625, loss 0.932001, acc 0.484375
2016-12-13T12:05:54.556580: step 626, loss 0.669865, acc 0.609375
2016-12-13T12:05:59.275156: step 627, loss 0.656493, acc 0.59375
2016-12-13T12:06:03.813094: step 628, loss 0.745382, acc 0.5
2016-12-13T12:06:08.330760: step 629, loss 0.748641, acc 0.5
2016-12-13T12:06:12.885297: step 630, loss 0.770636, acc 0.453125
2016-12-13T12:06:17.464003: step 631, loss 0.743831, acc 0.453125
2016-12-13T12:06:21.911918: step 632, loss 0.671108, acc 0.515625
2016-12-13T12:06:26.337093: step 633, loss 0.694178, acc 0.609375
2016-12-13T12:06:30.838933: step 634, loss 0.741272, acc 0.578125
2016-12-13T12:06:35.306997: step 635, loss 0.727219, acc 0.59375
2016-12-13T12:06:39.782442: step 636, loss 0.843686, acc 0.46875
2016-12-13T12:06:44.526275: step 637, loss 0.820812, acc 0.5
2016-12-13T12:06:49.092377: step 638, loss 0.670658, acc 0.578125
2016-12-13T12:06:53.599658: step 639, loss 0.744399, acc 0.484375
2016-12-13T12:06:58.111188: step 640, loss 0.762503, acc 0.40625
2016-12-13T12:07:02.624004: step 641, loss 0.740191, acc 0.5
2016-12-13T12:07:07.146269: step 642, loss 0.742289, acc 0.515625
2016-12-13T12:07:11.900502: step 643, loss 0.695401, acc 0.515625
2016-12-13T12:07:16.462591: step 644, loss 0.621057, acc 0.625
2016-12-13T12:07:20.973266: step 645, loss 0.70074, acc 0.59375
2016-12-13T12:07:25.386489: step 646, loss 0.870815, acc 0.5625
2016-12-13T12:07:29.808369: step 647, loss 0.776017, acc 0.5625
2016-12-13T12:07:34.299573: step 648, loss 0.689038, acc 0.59375
2016-12-13T12:07:39.160394: step 649, loss 0.653649, acc 0.640625
2016-12-13T12:07:43.766409: step 650, loss 0.643416, acc 0.5625
2016-12-13T12:07:48.196926: step 651, loss 0.676155, acc 0.609375
2016-12-13T12:07:52.804097: step 652, loss 0.684231, acc 0.5625
2016-12-13T12:07:57.378036: step 653, loss 0.665929, acc 0.640625
2016-12-13T12:08:02.239327: step 654, loss 0.678188, acc 0.546875
2016-12-13T12:08:07.354495: step 655, loss 0.680542, acc 0.59375
2016-12-13T12:08:12.300539: step 656, loss 0.751634, acc 0.53125
2016-12-13T12:08:16.965319: step 657, loss 0.619935, acc 0.65625
2016-12-13T12:08:21.576045: step 658, loss 0.682817, acc 0.609375
2016-12-13T12:08:26.155165: step 659, loss 0.755254, acc 0.484375
2016-12-13T12:08:30.928240: step 660, loss 0.615681, acc 0.640625
2016-12-13T12:08:35.326702: step 661, loss 0.686234, acc 0.59375
2016-12-13T12:08:40.601525: step 662, loss 0.702229, acc 0.59375
2016-12-13T12:08:45.380365: step 663, loss 0.66655, acc 0.625
2016-12-13T12:08:50.159641: step 664, loss 0.738491, acc 0.46875
2016-12-13T12:08:55.332309: step 665, loss 0.696478, acc 0.53125
2016-12-13T12:09:00.046561: step 666, loss 0.721314, acc 0.421875
2016-12-13T12:09:04.440593: step 667, loss 0.656274, acc 0.59375
2016-12-13T12:09:09.003112: step 668, loss 0.747772, acc 0.578125
2016-12-13T12:09:13.605690: step 669, loss 0.591724, acc 0.671875
2016-12-13T12:09:18.103420: step 670, loss 0.691618, acc 0.546875
2016-12-13T12:09:22.591394: step 671, loss 0.65631, acc 0.640625
2016-12-13T12:09:27.172859: step 672, loss 0.65434, acc 0.640625
2016-12-13T12:09:31.613856: step 673, loss 0.62209, acc 0.640625
2016-12-13T12:09:36.168312: step 674, loss 0.680079, acc 0.59375
2016-12-13T12:09:40.797712: step 675, loss 0.636045, acc 0.578125
2016-12-13T12:09:45.241686: step 676, loss 0.661441, acc 0.609375
2016-12-13T12:09:49.727623: step 677, loss 0.663588, acc 0.546875
2016-12-13T12:09:54.217603: step 678, loss 0.705452, acc 0.5625
2016-12-13T12:09:58.942985: step 679, loss 0.65469, acc 0.625
2016-12-13T12:10:03.285324: step 680, loss 0.695849, acc 0.546875
2016-12-13T12:10:07.667672: step 681, loss 0.673041, acc 0.65625
2016-12-13T12:10:12.002466: step 682, loss 0.684592, acc 0.625
2016-12-13T12:10:16.447051: step 683, loss 0.666904, acc 0.625
2016-12-13T12:10:20.759715: step 684, loss 0.71912, acc 0.53125
2016-12-13T12:10:25.093961: step 685, loss 0.686257, acc 0.625
2016-12-13T12:10:29.519561: step 686, loss 0.699299, acc 0.484375
2016-12-13T12:10:33.989154: step 687, loss 0.697504, acc 0.578125
2016-12-13T12:10:38.471428: step 688, loss 0.73474, acc 0.53125
2016-12-13T12:10:43.121369: step 689, loss 0.621677, acc 0.671875
2016-12-13T12:10:47.626195: step 690, loss 0.66867, acc 0.5625
2016-12-13T12:10:52.142515: step 691, loss 0.658187, acc 0.609375
2016-12-13T12:10:56.699782: step 692, loss 0.673117, acc 0.625
2016-12-13T12:11:01.156794: step 693, loss 0.71435, acc 0.59375
2016-12-13T12:11:05.632094: step 694, loss 0.650538, acc 0.625
2016-12-13T12:11:10.142327: step 695, loss 0.658628, acc 0.625
2016-12-13T12:11:14.689966: step 696, loss 0.747014, acc 0.546875
2016-12-13T12:11:19.574475: step 697, loss 0.673318, acc 0.59375
2016-12-13T12:11:24.183319: step 698, loss 0.650439, acc 0.640625
2016-12-13T12:11:28.517086: step 699, loss 0.658982, acc 0.546875
2016-12-13T12:11:32.965920: step 700, loss 0.712902, acc 0.546875

Evaluation:
2016-12-13T12:14:00.613204: step 700, loss 0.667349, acc 0.60917

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481637188/checkpoints/model-700

2016-12-13T12:14:09.006400: step 701, loss 0.701431, acc 0.5625
2016-12-13T12:14:13.460500: step 702, loss 0.689143, acc 0.59375
2016-12-13T12:14:18.562960: step 703, loss 0.712007, acc 0.5
2016-12-13T12:14:23.259535: step 704, loss 0.763142, acc 0.46875
2016-12-13T12:14:27.779975: step 705, loss 0.670298, acc 0.609375
2016-12-13T12:14:32.332546: step 706, loss 0.751796, acc 0.5625
2016-12-13T12:14:37.043050: step 707, loss 0.692461, acc 0.625
2016-12-13T12:14:41.501380: step 708, loss 0.694735, acc 0.53125
2016-12-13T12:14:46.301155: step 709, loss 0.714441, acc 0.515625
2016-12-13T12:14:51.051996: step 710, loss 0.658817, acc 0.609375
2016-12-13T12:14:55.586769: step 711, loss 0.700728, acc 0.53125
2016-12-13T12:14:59.982299: step 712, loss 0.656425, acc 0.6875
2016-12-13T12:15:04.467826: step 713, loss 0.597871, acc 0.65625
2016-12-13T12:15:09.903741: step 714, loss 0.597561, acc 0.734375
2016-12-13T12:15:15.127656: step 715, loss 0.654098, acc 0.59375
2016-12-13T12:15:20.212067: step 716, loss 0.820772, acc 0.53125
2016-12-13T12:15:25.262299: step 717, loss 0.820907, acc 0.5
2016-12-13T12:15:30.218986: step 718, loss 0.688609, acc 0.546875
2016-12-13T12:15:35.418689: step 719, loss 0.636941, acc 0.6875
2016-12-13T12:15:41.422652: step 720, loss 0.712559, acc 0.53125
2016-12-13T12:15:46.729991: step 721, loss 0.69404, acc 0.5
2016-12-13T12:15:52.402113: step 722, loss 0.748017, acc 0.4375
2016-12-13T12:15:57.397094: step 723, loss 0.704644, acc 0.578125
2016-12-13T12:16:02.421276: step 724, loss 0.666979, acc 0.65625
2016-12-13T12:16:07.760826: step 725, loss 0.669923, acc 0.578125
2016-12-13T12:16:13.467106: step 726, loss 0.74913, acc 0.5
2016-12-13T12:16:18.870477: step 727, loss 0.642733, acc 0.703125
2016-12-13T12:16:23.834336: step 728, loss 0.664649, acc 0.625
2016-12-13T12:16:29.288520: step 729, loss 0.695272, acc 0.578125
2016-12-13T12:16:34.518941: step 730, loss 0.683194, acc 0.640625
2016-12-13T12:16:39.292795: step 731, loss 0.663207, acc 0.640625
2016-12-13T12:16:44.088529: step 732, loss 0.647482, acc 0.6875
2016-12-13T12:16:49.170051: step 733, loss 0.635074, acc 0.609375
2016-12-13T12:16:54.132100: step 734, loss 0.715868, acc 0.546875
2016-12-13T12:16:58.991542: step 735, loss 0.624426, acc 0.640625
2016-12-13T12:17:03.780415: step 736, loss 0.655262, acc 0.609375
2016-12-13T12:17:08.613865: step 737, loss 0.65594, acc 0.625
2016-12-13T12:17:13.479279: step 738, loss 0.692078, acc 0.578125
2016-12-13T12:17:18.711741: step 739, loss 0.656708, acc 0.625
2016-12-13T12:17:23.681009: step 740, loss 0.704496, acc 0.625
2016-12-13T12:17:28.661236: step 741, loss 0.687195, acc 0.59375
2016-12-13T12:17:33.743067: step 742, loss 0.654467, acc 0.546875
2016-12-13T12:17:38.989260: step 743, loss 0.739646, acc 0.515625
2016-12-13T12:17:43.805508: step 744, loss 0.678922, acc 0.625
2016-12-13T12:17:48.799365: step 745, loss 0.742105, acc 0.515625
2016-12-13T12:17:53.660141: step 746, loss 0.712728, acc 0.65625
2016-12-13T12:17:58.467752: step 747, loss 0.720385, acc 0.609375
2016-12-13T12:18:03.305835: step 748, loss 0.720113, acc 0.578125
2016-12-13T12:18:08.133473: step 749, loss 0.721244, acc 0.5
2016-12-13T12:18:12.954177: step 750, loss 0.743213, acc 0.453125
2016-12-13T12:18:17.627478: step 751, loss 0.717961, acc 0.421875
2016-12-13T12:18:22.588124: step 752, loss 0.662799, acc 0.59375
2016-12-13T12:18:27.373688: step 753, loss 0.666503, acc 0.59375
2016-12-13T12:18:32.194053: step 754, loss 0.708427, acc 0.53125
2016-12-13T12:18:36.973138: step 755, loss 0.734874, acc 0.5625
2016-12-13T12:18:41.788738: step 756, loss 0.72638, acc 0.578125
2016-12-13T12:18:46.432003: step 757, loss 0.690789, acc 0.59375
2016-12-13T12:18:51.022938: step 758, loss 0.739787, acc 0.5625
2016-12-13T12:18:55.863873: step 759, loss 0.633037, acc 0.640625
2016-12-13T12:19:00.583302: step 760, loss 0.614667, acc 0.671875
2016-12-13T12:19:05.277561: step 761, loss 0.699223, acc 0.5
2016-12-13T12:19:10.090053: step 762, loss 0.611639, acc 0.625
2016-12-13T12:19:14.950102: step 763, loss 0.668272, acc 0.609375
2016-12-13T12:19:19.904355: step 764, loss 0.643091, acc 0.59375
2016-12-13T12:19:25.024413: step 765, loss 0.657262, acc 0.5625
2016-12-13T12:19:30.205664: step 766, loss 0.73733, acc 0.484375
2016-12-13T12:19:34.940153: step 767, loss 0.630065, acc 0.625
2016-12-13T12:19:39.704201: step 768, loss 0.718555, acc 0.546875
2016-12-13T12:19:44.557359: step 769, loss 0.694142, acc 0.546875
2016-12-13T12:19:49.405847: step 770, loss 0.729142, acc 0.578125
2016-12-13T12:19:54.027121: step 771, loss 0.668554, acc 0.578125
2016-12-13T12:19:58.945051: step 772, loss 0.646581, acc 0.59375
2016-12-13T12:20:03.903458: step 773, loss 0.685562, acc 0.59375
2016-12-13T12:20:08.619153: step 774, loss 0.664016, acc 0.6875
2016-12-13T12:20:13.336239: step 775, loss 0.655192, acc 0.640625
2016-12-13T12:20:18.186290: step 776, loss 0.681408, acc 0.5625
2016-12-13T12:20:23.120419: step 777, loss 0.649646, acc 0.640625
2016-12-13T12:20:27.974734: step 778, loss 0.682204, acc 0.5625
2016-12-13T12:20:33.207826: step 779, loss 0.634223, acc 0.609375
2016-12-13T12:20:38.178405: step 780, loss 0.605893, acc 0.6875
2016-12-13T12:20:43.132410: step 781, loss 0.680086, acc 0.609375
2016-12-13T12:20:47.901838: step 782, loss 0.775659, acc 0.515625
2016-12-13T12:20:52.781484: step 783, loss 0.740164, acc 0.546875
2016-12-13T12:20:57.904253: step 784, loss 0.662337, acc 0.625
2016-12-13T12:21:02.959568: step 785, loss 0.626059, acc 0.703125
2016-12-13T12:21:07.763796: step 786, loss 0.64375, acc 0.625
2016-12-13T12:21:12.592120: step 787, loss 0.638277, acc 0.6875
2016-12-13T12:21:17.394702: step 788, loss 0.696892, acc 0.5625
2016-12-13T12:21:22.086134: step 789, loss 0.658888, acc 0.625
2016-12-13T12:21:26.863476: step 790, loss 0.639425, acc 0.671875
2016-12-13T12:21:31.589353: step 791, loss 0.663105, acc 0.640625
2016-12-13T12:21:36.658978: step 792, loss 0.765715, acc 0.484375
2016-12-13T12:21:41.450267: step 793, loss 0.756007, acc 0.453125
2016-12-13T12:21:46.173112: step 794, loss 0.694655, acc 0.546875
2016-12-13T12:21:50.986381: step 795, loss 0.674561, acc 0.578125
2016-12-13T12:21:57.187813: step 796, loss 0.665843, acc 0.65625
2016-12-13T12:22:01.922977: step 797, loss 0.663377, acc 0.609375
2016-12-13T12:22:06.748449: step 798, loss 0.685797, acc 0.578125
2016-12-13T12:22:11.854249: step 799, loss 0.614903, acc 0.640625
2016-12-13T12:22:16.960376: step 800, loss 0.657091, acc 0.59375

Evaluation:
2016-12-13T12:26:24.686825: step 800, loss 0.678599, acc 0.596798

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481637188/checkpoints/model-800

2016-12-13T12:26:33.292784: step 801, loss 0.778786, acc 0.53125
2016-12-13T12:26:38.367486: step 802, loss 0.773572, acc 0.578125
2016-12-13T12:26:43.328574: step 803, loss 0.647028, acc 0.609375
2016-12-13T12:26:48.268151: step 804, loss 0.628889, acc 0.640625
2016-12-13T12:26:53.124258: step 805, loss 0.727467, acc 0.5625
2016-12-13T12:26:58.009085: step 806, loss 0.679007, acc 0.53125
2016-12-13T12:27:02.883863: step 807, loss 0.659852, acc 0.5625
2016-12-13T12:27:07.817726: step 808, loss 0.637978, acc 0.671875
2016-12-13T12:27:12.695609: step 809, loss 0.700056, acc 0.546875
2016-12-13T12:27:17.537141: step 810, loss 0.697638, acc 0.609375
2016-12-13T12:27:22.428076: step 811, loss 0.722677, acc 0.5
2016-12-13T12:27:27.222190: step 812, loss 0.631194, acc 0.671875
2016-12-13T12:27:32.240855: step 813, loss 0.692542, acc 0.53125
2016-12-13T12:27:37.689369: step 814, loss 0.604719, acc 0.671875
2016-12-13T12:27:43.335671: step 815, loss 0.652198, acc 0.625
2016-12-13T12:27:48.930677: step 816, loss 0.645263, acc 0.609375
2016-12-13T12:27:54.320287: step 817, loss 0.721642, acc 0.5625
2016-12-13T12:27:59.082725: step 818, loss 0.641222, acc 0.65625
2016-12-13T12:28:05.130451: step 819, loss 0.58735, acc 0.671875
2016-12-13T12:28:10.391677: step 820, loss 0.680919, acc 0.609375
2016-12-13T12:28:15.712255: step 821, loss 0.691693, acc 0.640625
2016-12-13T12:28:20.643987: step 822, loss 0.573723, acc 0.734375
2016-12-13T12:28:25.939916: step 823, loss 0.697195, acc 0.609375
2016-12-13T12:28:31.241503: step 824, loss 0.64044, acc 0.703125
2016-12-13T12:28:36.399089: step 825, loss 0.639206, acc 0.671875
2016-12-13T12:28:41.121447: step 826, loss 0.653268, acc 0.546875
2016-12-13T12:28:46.189445: step 827, loss 0.697949, acc 0.546875
2016-12-13T12:28:51.299054: step 828, loss 0.642854, acc 0.5625
2016-12-13T12:28:56.032375: step 829, loss 0.697944, acc 0.578125
2016-12-13T12:29:00.737817: step 830, loss 0.672157, acc 0.53125
2016-12-13T12:29:05.471291: step 831, loss 0.736374, acc 0.4375
2016-12-13T12:29:10.285499: step 832, loss 0.658263, acc 0.640625
2016-12-13T12:29:15.071664: step 833, loss 0.629594, acc 0.59375
2016-12-13T12:29:19.948651: step 834, loss 0.67509, acc 0.640625
2016-12-13T12:29:24.731514: step 835, loss 0.665089, acc 0.59375
2016-12-13T12:29:29.660920: step 836, loss 0.566519, acc 0.75
2016-12-13T12:29:34.831241: step 837, loss 0.698413, acc 0.59375
2016-12-13T12:29:40.225943: step 838, loss 0.706527, acc 0.59375
2016-12-13T12:29:44.973705: step 839, loss 0.635628, acc 0.640625
2016-12-13T12:29:49.776881: step 840, loss 0.667888, acc 0.578125
2016-12-13T12:29:54.614940: step 841, loss 0.60572, acc 0.65625
2016-12-13T12:29:59.389168: step 842, loss 0.675182, acc 0.625
2016-12-13T12:30:04.089608: step 843, loss 0.679928, acc 0.578125
2016-12-13T12:30:08.874155: step 844, loss 0.658059, acc 0.59375
2016-12-13T12:30:13.595947: step 845, loss 0.681709, acc 0.59375
2016-12-13T12:30:18.375828: step 846, loss 0.714381, acc 0.5625
2016-12-13T12:30:23.391703: step 847, loss 0.671968, acc 0.609375
2016-12-13T12:30:28.104226: step 848, loss 0.707564, acc 0.5625
2016-12-13T12:30:32.731881: step 849, loss 0.701631, acc 0.625
2016-12-13T12:30:37.339029: step 850, loss 0.769962, acc 0.53125
2016-12-13T12:30:42.049785: step 851, loss 0.697086, acc 0.578125
2016-12-13T12:30:46.823343: step 852, loss 0.668829, acc 0.53125
2016-12-13T12:30:51.563159: step 853, loss 0.748062, acc 0.5625
2016-12-13T12:30:56.604019: step 854, loss 0.641029, acc 0.609375
2016-12-13T12:31:01.234258: step 855, loss 0.688884, acc 0.5625
2016-12-13T12:31:05.992609: step 856, loss 0.708779, acc 0.578125
2016-12-13T12:31:10.763170: step 857, loss 0.664238, acc 0.609375
2016-12-13T12:31:15.525567: step 858, loss 0.650455, acc 0.59375
2016-12-13T12:31:20.365078: step 859, loss 0.673572, acc 0.65625
2016-12-13T12:31:25.095101: step 860, loss 0.68561, acc 0.609375
2016-12-13T12:31:30.419254: step 861, loss 0.624802, acc 0.65625
2016-12-13T12:31:35.504507: step 862, loss 0.675879, acc 0.578125
2016-12-13T12:31:40.342415: step 863, loss 0.644436, acc 0.640625
2016-12-13T12:31:45.233911: step 864, loss 0.596384, acc 0.640625
2016-12-13T12:31:50.013038: step 865, loss 0.681172, acc 0.59375
2016-12-13T12:31:54.834127: step 866, loss 0.600545, acc 0.703125
2016-12-13T12:31:59.831446: step 867, loss 0.686256, acc 0.609375
2016-12-13T12:32:04.530700: step 868, loss 0.622067, acc 0.71875
2016-12-13T12:32:09.298959: step 869, loss 0.592136, acc 0.703125
2016-12-13T12:32:14.150099: step 870, loss 0.662938, acc 0.515625
2016-12-13T12:32:18.968741: step 871, loss 0.621701, acc 0.640625
2016-12-13T12:32:23.792472: step 872, loss 0.646496, acc 0.640625
2016-12-13T12:32:28.642598: step 873, loss 0.688972, acc 0.609375
2016-12-13T12:32:33.498431: step 874, loss 0.676768, acc 0.640625
2016-12-13T12:32:38.308161: step 875, loss 0.681162, acc 0.625
2016-12-13T12:32:43.119965: step 876, loss 0.590871, acc 0.671875
2016-12-13T12:32:47.871944: step 877, loss 0.619484, acc 0.71875
2016-12-13T12:32:52.732366: step 878, loss 0.681703, acc 0.53125
2016-12-13T12:32:57.564493: step 879, loss 0.612561, acc 0.703125
2016-12-13T12:33:02.763375: step 880, loss 0.647298, acc 0.65625
2016-12-13T12:33:08.122371: step 881, loss 0.748941, acc 0.46875
2016-12-13T12:33:12.932448: step 882, loss 0.664485, acc 0.59375
2016-12-13T12:33:17.666094: step 883, loss 0.693958, acc 0.546875
2016-12-13T12:33:22.483700: step 884, loss 0.686309, acc 0.578125
2016-12-13T12:33:27.171415: step 885, loss 0.707988, acc 0.515625
2016-12-13T12:33:31.845260: step 886, loss 0.657077, acc 0.671875
2016-12-13T12:33:36.856281: step 887, loss 0.610185, acc 0.640625
2016-12-13T12:33:41.612766: step 888, loss 0.618419, acc 0.703125
2016-12-13T12:33:46.396755: step 889, loss 0.739614, acc 0.453125
2016-12-13T12:33:51.170273: step 890, loss 0.778063, acc 0.484375
2016-12-13T12:33:55.906092: step 891, loss 0.730436, acc 0.5625
2016-12-13T12:34:00.676716: step 892, loss 0.729654, acc 0.578125
2016-12-13T12:34:05.510860: step 893, loss 0.739569, acc 0.453125
2016-12-13T12:34:10.563838: step 894, loss 0.687887, acc 0.609375
2016-12-13T12:34:15.392497: step 895, loss 0.754153, acc 0.484375
2016-12-13T12:34:20.294723: step 896, loss 0.716262, acc 0.515625
2016-12-13T12:34:24.973390: step 897, loss 0.640849, acc 0.6875
2016-12-13T12:34:29.779835: step 898, loss 0.671358, acc 0.5625
2016-12-13T12:34:34.495066: step 899, loss 0.678932, acc 0.59375
2016-12-13T12:34:39.609312: step 900, loss 0.676671, acc 0.578125

Evaluation:
2016-12-13T12:35:57.740296: step 900, loss 0.681007, acc 0.592431

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481637188/checkpoints/model-900

2016-12-13T12:36:05.257956: step 901, loss 0.688275, acc 0.578125
2016-12-13T12:36:10.181272: step 902, loss 0.675589, acc 0.5625
2016-12-13T12:36:15.207868: step 903, loss 0.60889, acc 0.671875
2016-12-13T12:36:20.044984: step 904, loss 0.65179, acc 0.640625
2016-12-13T12:36:25.174211: step 905, loss 0.662678, acc 0.609375
2016-12-13T12:36:30.505528: step 906, loss 0.689386, acc 0.5625
2016-12-13T12:36:35.406455: step 907, loss 0.69351, acc 0.578125
2016-12-13T12:36:40.438984: step 908, loss 0.622729, acc 0.59375
2016-12-13T12:36:45.314944: step 909, loss 0.780194, acc 0.53125
2016-12-13T12:36:50.211022: step 910, loss 0.702816, acc 0.515625
2016-12-13T12:36:55.075523: step 911, loss 0.6675, acc 0.625
2016-12-13T12:37:00.028665: step 912, loss 0.652374, acc 0.625
2016-12-13T12:37:04.836531: step 913, loss 0.695724, acc 0.578125
2016-12-13T12:37:09.645673: step 914, loss 0.700623, acc 0.578125
2016-12-13T12:37:14.525464: step 915, loss 0.662709, acc 0.5625
2016-12-13T12:37:19.392403: step 916, loss 0.714496, acc 0.53125
2016-12-13T12:37:24.133313: step 917, loss 0.71086, acc 0.609375
2016-12-13T12:37:28.871775: step 918, loss 0.692215, acc 0.53125
2016-12-13T12:37:33.803822: step 919, loss 0.728986, acc 0.59375
2016-12-13T12:37:38.641539: step 920, loss 0.633818, acc 0.71875
2016-12-13T12:37:43.348450: step 921, loss 0.720358, acc 0.578125
2016-12-13T12:37:48.119822: step 922, loss 0.678077, acc 0.515625
2016-12-13T12:37:52.977396: step 923, loss 0.736526, acc 0.515625
2016-12-13T12:37:57.788254: step 924, loss 0.674633, acc 0.609375
2016-12-13T12:38:03.522667: step 925, loss 0.642749, acc 0.65625
2016-12-13T12:38:08.823407: step 926, loss 0.66732, acc 0.609375
2016-12-13T12:38:13.538934: step 927, loss 0.68028, acc 0.53125
2016-12-13T12:38:18.401498: step 928, loss 0.673704, acc 0.625
2016-12-13T12:38:23.235757: step 929, loss 0.627744, acc 0.671875
2016-12-13T12:38:28.073069: step 930, loss 0.626826, acc 0.6875
2016-12-13T12:38:32.842287: step 931, loss 0.740813, acc 0.546875
2016-12-13T12:38:37.830016: step 932, loss 0.695427, acc 0.546875
2016-12-13T12:38:42.548649: step 933, loss 0.665021, acc 0.578125
2016-12-13T12:38:47.328403: step 934, loss 0.690557, acc 0.5625
2016-12-13T12:38:52.168863: step 935, loss 0.700305, acc 0.546875
2016-12-13T12:38:56.902389: step 936, loss 0.653113, acc 0.59375
2016-12-13T12:39:01.731189: step 937, loss 0.704395, acc 0.515625
2016-12-13T12:39:06.666044: step 938, loss 0.672397, acc 0.5625
2016-12-13T12:39:12.298990: step 939, loss 0.777094, acc 0.5
2016-12-13T12:39:17.211010: step 940, loss 0.665493, acc 0.6875
2016-12-13T12:39:21.968395: step 941, loss 0.635005, acc 0.703125
2016-12-13T12:39:26.722065: step 942, loss 0.696572, acc 0.625
2016-12-13T12:39:31.442588: step 943, loss 0.698513, acc 0.546875
2016-12-13T12:39:36.267938: step 944, loss 0.711405, acc 0.546875
2016-12-13T12:39:41.245376: step 945, loss 0.698768, acc 0.53125
2016-12-13T12:39:46.069229: step 946, loss 0.672967, acc 0.625
2016-12-13T12:39:50.830544: step 947, loss 0.735331, acc 0.515625
2016-12-13T12:39:55.663939: step 948, loss 0.634919, acc 0.625
2016-12-13T12:40:00.403888: step 949, loss 0.703565, acc 0.5625
2016-12-13T12:40:05.182340: step 950, loss 0.683012, acc 0.59375
2016-12-13T12:40:10.269965: step 951, loss 0.730651, acc 0.53125
2016-12-13T12:40:15.196775: step 952, loss 0.822103, acc 0.4375
2016-12-13T12:40:20.029707: step 953, loss 0.678857, acc 0.5625
2016-12-13T12:40:24.754299: step 954, loss 0.731322, acc 0.421875
2016-12-13T12:40:29.533740: step 955, loss 0.781978, acc 0.40625
2016-12-13T12:40:34.320374: step 956, loss 0.66572, acc 0.59375
2016-12-13T12:40:39.193500: step 957, loss 0.689485, acc 0.484375
2016-12-13T12:40:44.134963: step 958, loss 0.691178, acc 0.625
2016-12-13T12:40:49.143849: step 959, loss 0.654401, acc 0.6875
2016-12-13T12:40:54.047234: step 960, loss 0.771605, acc 0.515625
2016-12-13T12:40:58.960214: step 961, loss 0.782084, acc 0.5625
2016-12-13T12:41:03.774026: step 962, loss 0.64655, acc 0.65625
2016-12-13T12:41:08.586301: step 963, loss 0.684283, acc 0.671875
2016-12-13T12:41:13.011142: step 964, loss 0.68208, acc 0.5625
2016-12-13T12:41:17.629747: step 965, loss 0.653056, acc 0.578125
2016-12-13T12:41:22.067871: step 966, loss 0.668444, acc 0.65625
2016-12-13T12:41:26.448458: step 967, loss 0.649637, acc 0.609375
2016-12-13T12:41:30.856302: step 968, loss 0.701055, acc 0.484375
2016-12-13T12:41:35.287467: step 969, loss 0.606654, acc 0.6875
2016-12-13T12:41:39.651039: step 970, loss 0.729368, acc 0.53125
2016-12-13T12:41:44.046585: step 971, loss 0.660101, acc 0.5625
2016-12-13T12:41:48.510173: step 972, loss 0.749787, acc 0.546875
2016-12-13T12:41:53.084665: step 973, loss 0.72397, acc 0.53125
2016-12-13T12:41:57.469814: step 974, loss 0.751658, acc 0.484375
2016-12-13T12:42:01.869001: step 975, loss 0.686955, acc 0.546875
2016-12-13T12:42:06.272223: step 976, loss 0.657642, acc 0.6875
2016-12-13T12:42:10.651427: step 977, loss 0.694213, acc 0.53125
2016-12-13T12:42:15.049664: step 978, loss 0.646772, acc 0.609375
2016-12-13T12:42:19.436663: step 979, loss 0.646727, acc 0.59375
2016-12-13T12:42:24.056764: step 980, loss 0.620617, acc 0.65625
2016-12-13T12:42:28.456222: step 981, loss 0.638295, acc 0.625
2016-12-13T12:42:32.852929: step 982, loss 0.608957, acc 0.625
2016-12-13T12:42:37.265718: step 983, loss 0.720857, acc 0.59375
2016-12-13T12:42:41.681543: step 984, loss 0.716959, acc 0.5625
2016-12-13T12:42:46.063437: step 985, loss 0.61769, acc 0.65625
2016-12-13T12:42:50.484055: step 986, loss 0.588522, acc 0.671875
2016-12-13T12:42:55.061807: step 987, loss 0.73241, acc 0.578125
2016-12-13T12:42:59.446415: step 988, loss 0.695234, acc 0.5
2016-12-13T12:43:04.698019: step 989, loss 0.660458, acc 0.625
2016-12-13T12:43:09.158923: step 990, loss 0.663204, acc 0.59375
2016-12-13T12:43:13.574135: step 991, loss 0.723919, acc 0.484375
2016-12-13T12:43:17.973904: step 992, loss 0.734889, acc 0.453125
2016-12-13T12:43:22.396109: step 993, loss 0.681974, acc 0.5
2016-12-13T12:43:26.976683: step 994, loss 0.690773, acc 0.578125
2016-12-13T12:43:31.404439: step 995, loss 0.631331, acc 0.703125
2016-12-13T12:43:35.834161: step 996, loss 0.754728, acc 0.53125
2016-12-13T12:43:40.184796: step 997, loss 0.783427, acc 0.515625
2016-12-13T12:43:44.608824: step 998, loss 0.644107, acc 0.625
2016-12-13T12:43:49.020159: step 999, loss 0.658014, acc 0.5625
2016-12-13T12:43:53.423561: step 1000, loss 0.700867, acc 0.578125

Evaluation:
2016-12-13T12:45:20.489973: step 1000, loss 0.674101, acc 0.586608

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481637188/checkpoints/model-1000

2016-12-13T12:45:27.921888: step 1001, loss 0.744433, acc 0.4375
2016-12-13T12:45:32.342726: step 1002, loss 0.648079, acc 0.625
2016-12-13T12:45:36.727879: step 1003, loss 0.739545, acc 0.46875
2016-12-13T12:45:41.095342: step 1004, loss 0.703705, acc 0.5
2016-12-13T12:45:45.662421: step 1005, loss 0.688803, acc 0.609375
2016-12-13T12:45:50.319793: step 1006, loss 0.71635, acc 0.546875
2016-12-13T12:45:54.813910: step 1007, loss 0.707635, acc 0.625
2016-12-13T12:45:59.243063: step 1008, loss 0.568334, acc 0.71875
2016-12-13T12:46:03.661750: step 1009, loss 0.763578, acc 0.484375
2016-12-13T12:46:08.097244: step 1010, loss 0.629416, acc 0.703125
2016-12-13T12:46:12.535275: step 1011, loss 0.681945, acc 0.65625
2016-12-13T12:46:16.938309: step 1012, loss 0.744728, acc 0.515625
2016-12-13T12:46:21.515903: step 1013, loss 0.72481, acc 0.515625
2016-12-13T12:46:25.946450: step 1014, loss 0.72868, acc 0.46875
2016-12-13T12:46:30.343498: step 1015, loss 0.688052, acc 0.59375
2016-12-13T12:46:34.731876: step 1016, loss 0.667344, acc 0.609375
2016-12-13T12:46:39.166513: step 1017, loss 0.604368, acc 0.703125
2016-12-13T12:46:43.832280: step 1018, loss 0.63448, acc 0.609375
2016-12-13T12:46:48.879798: step 1019, loss 0.659865, acc 0.59375
2016-12-13T12:46:53.774404: step 1020, loss 0.764521, acc 0.515625
2016-12-13T12:46:59.033367: step 1021, loss 0.656818, acc 0.578125
2016-12-13T12:47:03.870752: step 1022, loss 0.671902, acc 0.625
2016-12-13T12:47:08.727634: step 1023, loss 0.662514, acc 0.59375
2016-12-13T12:47:13.301496: step 1024, loss 0.685176, acc 0.5625
2016-12-13T12:47:18.252025: step 1025, loss 0.691872, acc 0.546875
2016-12-13T12:47:23.429606: step 1026, loss 0.73456, acc 0.515625
2016-12-13T12:47:29.189949: step 1027, loss 0.679291, acc 0.625
2016-12-13T12:47:34.042267: step 1028, loss 0.691214, acc 0.5
2016-12-13T12:47:39.130773: step 1029, loss 0.651462, acc 0.625
2016-12-13T12:47:44.291199: step 1030, loss 0.625899, acc 0.6875
2016-12-13T12:47:49.028922: step 1031, loss 0.698863, acc 0.59375
2016-12-13T12:47:54.083891: step 1032, loss 0.670125, acc 0.609375
2016-12-13T12:47:58.967021: step 1033, loss 0.730971, acc 0.625
2016-12-13T12:48:05.052103: step 1034, loss 0.662215, acc 0.53125
2016-12-13T12:48:09.778206: step 1035, loss 0.674608, acc 0.546875
2016-12-13T12:48:14.221826: step 1036, loss 0.679844, acc 0.578125
2016-12-13T12:48:18.662330: step 1037, loss 0.761108, acc 0.453125
2016-12-13T12:48:23.102803: step 1038, loss 0.646453, acc 0.625
2016-12-13T12:48:27.606927: step 1039, loss 0.660665, acc 0.59375
2016-12-13T12:48:32.128316: step 1040, loss 0.69025, acc 0.578125
2016-12-13T12:48:36.572218: step 1041, loss 0.650976, acc 0.59375
2016-12-13T12:48:41.014254: step 1042, loss 0.738673, acc 0.59375
2016-12-13T12:48:45.463500: step 1043, loss 0.575998, acc 0.734375
2016-12-13T12:48:49.937698: step 1044, loss 0.676617, acc 0.578125
2016-12-13T12:48:54.396849: step 1045, loss 0.662328, acc 0.609375
2016-12-13T12:48:59.078951: step 1046, loss 0.588839, acc 0.578125
2016-12-13T12:49:03.757594: step 1047, loss 0.72364, acc 0.59375
2016-12-13T12:49:08.186862: step 1048, loss 0.694398, acc 0.515625
2016-12-13T12:49:12.667110: step 1049, loss 0.701558, acc 0.546875
2016-12-13T12:49:17.115675: step 1050, loss 0.678594, acc 0.578125
2016-12-13T12:49:21.484919: step 1051, loss 0.630898, acc 0.671875
2016-12-13T12:49:25.966496: step 1052, loss 0.704381, acc 0.53125
2016-12-13T12:49:30.438208: step 1053, loss 0.66277, acc 0.625
2016-12-13T12:49:35.044490: step 1054, loss 0.682785, acc 0.59375
2016-12-13T12:49:39.466208: step 1055, loss 0.70147, acc 0.640625
2016-12-13T12:49:43.934559: step 1056, loss 0.696096, acc 0.625
2016-12-13T12:49:48.688535: step 1057, loss 0.742589, acc 0.640625
2016-12-13T12:49:53.393699: step 1058, loss 0.624184, acc 0.671875
2016-12-13T12:49:57.992513: step 1059, loss 0.705374, acc 0.53125
2016-12-13T12:50:02.592649: step 1060, loss 0.712266, acc 0.515625
2016-12-13T12:50:07.328847: step 1061, loss 0.73747, acc 0.484375
2016-12-13T12:50:11.810319: step 1062, loss 0.669482, acc 0.609375
2016-12-13T12:50:16.322225: step 1063, loss 0.652472, acc 0.609375
2016-12-13T12:50:20.922170: step 1064, loss 0.698239, acc 0.578125
2016-12-13T12:50:26.047854: step 1065, loss 0.557874, acc 0.671875
2016-12-13T12:50:31.731241: step 1066, loss 0.752652, acc 0.59375
2016-12-13T12:50:36.879857: step 1067, loss 0.600399, acc 0.640625
2016-12-13T12:50:42.797676: step 1068, loss 0.637086, acc 0.625
2016-12-13T12:50:47.679975: step 1069, loss 0.647229, acc 0.625
2016-12-13T12:50:52.306915: step 1070, loss 0.654455, acc 0.53125
2016-12-13T12:50:56.867530: step 1071, loss 0.652768, acc 0.609375
2016-12-13T12:51:01.444023: step 1072, loss 0.688147, acc 0.59375
2016-12-13T12:51:06.779918: step 1073, loss 0.646463, acc 0.640625
2016-12-13T12:51:12.337225: step 1074, loss 0.65848, acc 0.53125
2016-12-13T12:51:17.311029: step 1075, loss 0.725076, acc 0.515625
2016-12-13T12:51:21.938176: step 1076, loss 0.749619, acc 0.421875
2016-12-13T12:51:26.559267: step 1077, loss 0.692726, acc 0.515625
2016-12-13T12:51:31.128206: step 1078, loss 0.665232, acc 0.609375
2016-12-13T12:51:35.835615: step 1079, loss 0.63329, acc 0.640625
2016-12-13T12:51:40.783087: step 1080, loss 0.61911, acc 0.65625
2016-12-13T12:51:45.483617: step 1081, loss 0.680551, acc 0.546875
2016-12-13T12:51:50.012772: step 1082, loss 0.754381, acc 0.5625
2016-12-13T12:51:55.350858: step 1083, loss 0.693449, acc 0.640625
2016-12-13T12:52:00.784056: step 1084, loss 0.658216, acc 0.609375
2016-12-13T12:52:06.361706: step 1085, loss 0.674801, acc 0.5625
2016-12-13T12:52:11.218297: step 1086, loss 0.704869, acc 0.515625
2016-12-13T12:52:15.734422: step 1087, loss 0.754791, acc 0.46875
2016-12-13T12:52:20.847110: step 1088, loss 0.79596, acc 0.421875
2016-12-13T12:52:26.139840: step 1089, loss 0.711164, acc 0.46875
2016-12-13T12:52:30.723305: step 1090, loss 0.695052, acc 0.515625
2016-12-13T12:52:35.310060: step 1091, loss 0.764517, acc 0.46875
2016-12-13T12:52:39.894938: step 1092, loss 0.722642, acc 0.578125
2016-12-13T12:52:44.940364: step 1093, loss 0.601137, acc 0.671875
2016-12-13T12:52:50.013969: step 1094, loss 0.708486, acc 0.609375
2016-12-13T12:52:54.455784: step 1095, loss 0.739804, acc 0.5625
2016-12-13T12:52:58.999514: step 1096, loss 0.748748, acc 0.546875
2016-12-13T12:53:04.184936: step 1097, loss 0.658931, acc 0.625
2016-12-13T12:53:08.985186: step 1098, loss 0.620115, acc 0.71875
2016-12-13T12:53:13.827112: step 1099, loss 0.654156, acc 0.609375
2016-12-13T12:53:18.589695: step 1100, loss 0.638196, acc 0.65625

Evaluation:
2016-12-13T12:55:19.783829: step 1100, loss 0.651128, acc 0.611354

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481637188/checkpoints/model-1100

2016-12-13T12:55:27.560788: step 1101, loss 0.717671, acc 0.515625
2016-12-13T12:55:32.484108: step 1102, loss 0.653235, acc 0.578125
2016-12-13T12:55:37.282299: step 1103, loss 0.645696, acc 0.59375
2016-12-13T12:55:41.895097: step 1104, loss 0.650733, acc 0.59375
2016-12-13T12:55:46.733779: step 1105, loss 0.642936, acc 0.59375
2016-12-13T12:55:51.229649: step 1106, loss 0.649054, acc 0.609375
2016-12-13T12:55:55.706993: step 1107, loss 0.695206, acc 0.515625
2016-12-13T12:56:00.230206: step 1108, loss 0.654119, acc 0.6875
2016-12-13T12:56:04.716469: step 1109, loss 0.617966, acc 0.671875
2016-12-13T12:56:09.208597: step 1110, loss 0.737434, acc 0.515625
2016-12-13T12:56:13.705275: step 1111, loss 0.725056, acc 0.46875
2016-12-13T12:56:18.322112: step 1112, loss 0.656449, acc 0.578125
2016-12-13T12:56:22.798906: step 1113, loss 0.706327, acc 0.453125
2016-12-13T12:56:27.250425: step 1114, loss 0.734312, acc 0.484375
2016-12-13T12:56:31.647368: step 1115, loss 0.710891, acc 0.5
2016-12-13T12:56:36.264126: step 1116, loss 0.706183, acc 0.546875
2016-12-13T12:56:40.940902: step 1117, loss 0.677325, acc 0.59375
2016-12-13T12:56:46.004357: step 1118, loss 0.647157, acc 0.640625
2016-12-13T12:56:51.529730: step 1119, loss 0.625831, acc 0.65625
2016-12-13T12:56:56.524801: step 1120, loss 0.687535, acc 0.546875
2016-12-13T12:57:01.395659: step 1121, loss 0.625752, acc 0.671875
2016-12-13T12:57:05.888121: step 1122, loss 0.6486, acc 0.625
2016-12-13T12:57:10.318663: step 1123, loss 0.615258, acc 0.6875
2016-12-13T12:57:14.809923: step 1124, loss 0.732047, acc 0.46875
2016-12-13T12:57:19.232737: step 1125, loss 0.684159, acc 0.59375
2016-12-13T12:57:24.073958: step 1126, loss 0.673094, acc 0.59375
2016-12-13T12:57:28.794956: step 1127, loss 0.720744, acc 0.515625
2016-12-13T12:57:33.424671: step 1128, loss 0.62866, acc 0.65625
2016-12-13T12:57:38.023105: step 1129, loss 0.679018, acc 0.546875
2016-12-13T12:57:42.618165: step 1130, loss 0.689846, acc 0.53125
2016-12-13T12:57:47.190916: step 1131, loss 0.694415, acc 0.5625
2016-12-13T12:57:51.764657: step 1132, loss 0.660852, acc 0.625
2016-12-13T12:57:56.690861: step 1133, loss 0.671259, acc 0.53125
2016-12-13T12:58:01.755420: step 1134, loss 0.704795, acc 0.515625
2016-12-13T12:58:06.838999: step 1135, loss 0.725264, acc 0.5
2016-12-13T12:58:11.404827: step 1136, loss 0.612092, acc 0.734375
2016-12-13T12:58:16.004699: step 1137, loss 0.671841, acc 0.53125
2016-12-13T12:58:20.609310: step 1138, loss 0.698153, acc 0.53125
2016-12-13T12:58:25.233008: step 1139, loss 0.68329, acc 0.59375
2016-12-13T12:58:29.939952: step 1140, loss 0.723073, acc 0.546875
2016-12-13T12:58:34.630432: step 1141, loss 0.661333, acc 0.609375
2016-12-13T12:58:39.302676: step 1142, loss 0.685471, acc 0.625
2016-12-13T12:58:44.046867: step 1143, loss 0.715616, acc 0.5625
2016-12-13T12:58:48.642645: step 1144, loss 0.654813, acc 0.578125
2016-12-13T12:58:53.225174: step 1145, loss 0.632936, acc 0.65625
2016-12-13T12:58:57.909025: step 1146, loss 0.650915, acc 0.640625
2016-12-13T12:59:02.706726: step 1147, loss 0.684421, acc 0.578125
2016-12-13T12:59:08.055089: step 1148, loss 0.699807, acc 0.640625
2016-12-13T12:59:13.128965: step 1149, loss 0.600736, acc 0.6875
2016-12-13T12:59:17.650094: step 1150, loss 0.614401, acc 0.703125
2016-12-13T12:59:22.135353: step 1151, loss 0.682305, acc 0.53125
2016-12-13T12:59:26.672676: step 1152, loss 0.74349, acc 0.578125
2016-12-13T12:59:31.166417: step 1153, loss 0.662377, acc 0.59375
2016-12-13T12:59:35.702974: step 1154, loss 0.701062, acc 0.5625
2016-12-13T12:59:40.188196: step 1155, loss 0.694933, acc 0.59375
2016-12-13T12:59:44.644718: step 1156, loss 0.674392, acc 0.53125
2016-12-13T12:59:49.121549: step 1157, loss 0.66406, acc 0.625
2016-12-13T12:59:53.541231: step 1158, loss 0.704383, acc 0.53125
2016-12-13T12:59:58.093062: step 1159, loss 0.670543, acc 0.625
2016-12-13T13:00:02.810508: step 1160, loss 0.66489, acc 0.65625
2016-12-13T13:00:07.460748: step 1161, loss 0.646396, acc 0.71875
2016-12-13T13:00:11.882597: step 1162, loss 0.788928, acc 0.453125
2016-12-13T13:00:16.342277: step 1163, loss 0.698294, acc 0.671875
2016-12-13T13:00:20.858312: step 1164, loss 0.681166, acc 0.5625
2016-12-13T13:00:25.314351: step 1165, loss 0.60845, acc 0.625
2016-12-13T13:00:29.766192: step 1166, loss 0.65711, acc 0.578125
2016-12-13T13:00:34.374279: step 1167, loss 0.678371, acc 0.578125
2016-12-13T13:00:39.089263: step 1168, loss 0.752102, acc 0.453125
2016-12-13T13:00:43.639654: step 1169, loss 0.711432, acc 0.578125
2016-12-13T13:00:48.123682: step 1170, loss 0.67412, acc 0.59375
2016-12-13T13:00:52.605831: step 1171, loss 0.70026, acc 0.578125
2016-12-13T13:00:57.042786: step 1172, loss 0.645526, acc 0.609375
2016-12-13T13:01:01.477178: step 1173, loss 0.70918, acc 0.53125
2016-12-13T13:01:05.930661: step 1174, loss 0.670287, acc 0.625
2016-12-13T13:01:10.561743: step 1175, loss 0.61761, acc 0.6875
2016-12-13T13:01:14.973330: step 1176, loss 0.7184, acc 0.625
2016-12-13T13:01:19.406205: step 1177, loss 0.623464, acc 0.671875
2016-12-13T13:01:23.896397: step 1178, loss 0.658989, acc 0.578125
2016-12-13T13:01:28.351340: step 1179, loss 0.589187, acc 0.703125
2016-12-13T13:01:32.797973: step 1180, loss 0.590705, acc 0.703125
2016-12-13T13:01:37.247548: step 1181, loss 0.592049, acc 0.671875
2016-12-13T13:01:41.812920: step 1182, loss 0.6787, acc 0.59375
2016-12-13T13:01:46.237525: step 1183, loss 0.709791, acc 0.625
2016-12-13T13:01:50.613772: step 1184, loss 0.777336, acc 0.546875
2016-12-13T13:01:55.075980: step 1185, loss 0.705361, acc 0.546875
2016-12-13T13:01:59.517998: step 1186, loss 0.664488, acc 0.59375
2016-12-13T13:02:03.968462: step 1187, loss 0.664178, acc 0.546875
2016-12-13T13:02:08.376366: step 1188, loss 0.659585, acc 0.546875
2016-12-13T13:02:13.031048: step 1189, loss 0.735466, acc 0.5
2016-12-13T13:02:17.451836: step 1190, loss 0.688077, acc 0.53125
2016-12-13T13:02:21.937996: step 1191, loss 0.638327, acc 0.65625
2016-12-13T13:02:26.360613: step 1192, loss 0.580013, acc 0.71875
2016-12-13T13:02:30.780434: step 1193, loss 0.679317, acc 0.609375
2016-12-13T13:02:35.192519: step 1194, loss 0.914162, acc 0.484375
2016-12-13T13:02:39.676126: step 1195, loss 0.727302, acc 0.59375
2016-12-13T13:02:44.601844: step 1196, loss 0.601525, acc 0.6875
2016-12-13T13:02:49.315771: step 1197, loss 0.666337, acc 0.640625
2016-12-13T13:02:53.886257: step 1198, loss 0.647138, acc 0.640625
2016-12-13T13:02:58.365864: step 1199, loss 0.697761, acc 0.5625
2016-12-13T13:03:03.560402: step 1200, loss 0.745601, acc 0.5

Evaluation:
2016-12-13T13:05:20.217030: step 1200, loss 0.663641, acc 0.613537

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481637188/checkpoints/model-1200

2016-12-13T13:05:28.269861: step 1201, loss 0.720143, acc 0.53125
2016-12-13T13:05:32.696489: step 1202, loss 0.694941, acc 0.546875
2016-12-13T13:05:37.175104: step 1203, loss 0.626678, acc 0.6875
2016-12-13T13:05:41.604235: step 1204, loss 0.624787, acc 0.671875
2016-12-13T13:05:46.069080: step 1205, loss 0.759861, acc 0.515625
2016-12-13T13:05:50.854520: step 1206, loss 0.685896, acc 0.640625
2016-12-13T13:05:55.299040: step 1207, loss 0.714879, acc 0.625
2016-12-13T13:05:59.982581: step 1208, loss 0.640498, acc 0.6875
2016-12-13T13:06:04.544204: step 1209, loss 0.66064, acc 0.671875
2016-12-13T13:06:09.118857: step 1210, loss 0.672208, acc 0.578125
2016-12-13T13:06:13.916362: step 1211, loss 0.628256, acc 0.640625
2016-12-13T13:06:18.912400: step 1212, loss 0.698846, acc 0.59375
2016-12-13T13:06:24.195735: step 1213, loss 0.664421, acc 0.609375
2016-12-13T13:06:28.767378: step 1214, loss 0.709544, acc 0.625
2016-12-13T13:06:33.184356: step 1215, loss 0.681282, acc 0.625
2016-12-13T13:06:37.595920: step 1216, loss 0.668208, acc 0.65625
2016-12-13T13:06:42.021797: step 1217, loss 0.668663, acc 0.609375
2016-12-13T13:06:46.604367: step 1218, loss 0.665169, acc 0.671875
2016-12-13T13:06:51.033429: step 1219, loss 0.638285, acc 0.59375
2016-12-13T13:06:55.824030: step 1220, loss 0.703339, acc 0.5
2016-12-13T13:07:00.580953: step 1221, loss 0.732876, acc 0.46875
2016-12-13T13:07:05.087857: step 1222, loss 0.698864, acc 0.5
2016-12-13T13:07:09.561415: step 1223, loss 0.661696, acc 0.578125
2016-12-13T13:07:14.089679: step 1224, loss 0.681542, acc 0.609375
2016-12-13T13:07:18.518387: step 1225, loss 0.71895, acc 0.53125
2016-12-13T13:07:23.025473: step 1226, loss 0.707309, acc 0.5625
2016-12-13T13:07:27.617974: step 1227, loss 0.597414, acc 0.703125
2016-12-13T13:07:31.976487: step 1228, loss 0.580631, acc 0.71875
2016-12-13T13:07:36.476269: step 1229, loss 0.72532, acc 0.578125
2016-12-13T13:07:40.916473: step 1230, loss 0.690577, acc 0.609375
2016-12-13T13:07:45.357594: step 1231, loss 0.637612, acc 0.671875
2016-12-13T13:07:49.861517: step 1232, loss 0.671317, acc 0.578125
2016-12-13T13:07:54.342256: step 1233, loss 0.650168, acc 0.59375
2016-12-13T13:07:58.930010: step 1234, loss 0.698404, acc 0.578125
2016-12-13T13:08:03.480639: step 1235, loss 0.697637, acc 0.578125
2016-12-13T13:08:08.048684: step 1236, loss 0.658635, acc 0.625
2016-12-13T13:08:12.493900: step 1237, loss 0.63979, acc 0.640625
2016-12-13T13:08:17.173857: step 1238, loss 0.666917, acc 0.640625
2016-12-13T13:08:22.012478: step 1239, loss 0.734487, acc 0.609375
2016-12-13T13:08:27.072964: step 1240, loss 0.736718, acc 0.53125
2016-12-13T13:08:31.949139: step 1241, loss 0.678925, acc 0.5625
2016-12-13T13:08:36.517156: step 1242, loss 0.663386, acc 0.59375
2016-12-13T13:08:40.891634: step 1243, loss 0.710522, acc 0.53125
2016-12-13T13:08:45.553529: step 1244, loss 0.804878, acc 0.390625
2016-12-13T13:08:50.025114: step 1245, loss 0.669734, acc 0.5625
2016-12-13T13:08:54.422164: step 1246, loss 0.653518, acc 0.640625
2016-12-13T13:08:58.997582: step 1247, loss 0.593483, acc 0.625
2016-12-13T13:09:03.640954: step 1248, loss 0.820107, acc 0.5625
2016-12-13T13:09:08.244011: step 1249, loss 0.856373, acc 0.5
2016-12-13T13:09:13.034488: step 1250, loss 0.633976, acc 0.640625
2016-12-13T13:09:17.562194: step 1251, loss 0.638556, acc 0.59375
2016-12-13T13:09:22.121722: step 1252, loss 0.733233, acc 0.515625
2016-12-13T13:09:26.634719: step 1253, loss 0.613059, acc 0.671875
2016-12-13T13:09:31.026526: step 1254, loss 0.752015, acc 0.515625
2016-12-13T13:09:35.710880: step 1255, loss 0.643752, acc 0.609375
2016-12-13T13:09:40.193229: step 1256, loss 0.669748, acc 0.609375
2016-12-13T13:09:44.831499: step 1257, loss 0.615994, acc 0.640625
2016-12-13T13:09:49.467533: step 1258, loss 0.783824, acc 0.421875
2016-12-13T13:09:53.782022: step 1259, loss 0.705937, acc 0.578125
2016-12-13T13:09:58.141660: step 1260, loss 0.703644, acc 0.59375
2016-12-13T13:10:02.517887: step 1261, loss 0.75373, acc 0.5
2016-12-13T13:10:07.011608: step 1262, loss 0.677449, acc 0.578125
2016-12-13T13:10:11.442089: step 1263, loss 0.698334, acc 0.5625
2016-12-13T13:10:15.765707: step 1264, loss 0.613505, acc 0.640625
2016-12-13T13:10:20.169008: step 1265, loss 0.715777, acc 0.53125
2016-12-13T13:10:24.544617: step 1266, loss 0.685808, acc 0.5625
2016-12-13T13:10:28.912479: step 1267, loss 0.704496, acc 0.53125
2016-12-13T13:10:33.326221: step 1268, loss 0.695631, acc 0.640625
2016-12-13T13:10:37.737350: step 1269, loss 0.629649, acc 0.671875
2016-12-13T13:10:42.387904: step 1270, loss 0.717854, acc 0.5625
2016-12-13T13:10:46.762588: step 1271, loss 0.633895, acc 0.671875
2016-12-13T13:10:51.156355: step 1272, loss 0.670668, acc 0.625
2016-12-13T13:10:55.497019: step 1273, loss 0.642015, acc 0.640625
2016-12-13T13:10:59.931667: step 1274, loss 0.652975, acc 0.6875
2016-12-13T13:11:04.270784: step 1275, loss 0.720137, acc 0.453125
2016-12-13T13:11:08.674796: step 1276, loss 0.646739, acc 0.609375
2016-12-13T13:11:13.217605: step 1277, loss 0.745146, acc 0.46875
2016-12-13T13:11:17.648040: step 1278, loss 0.713331, acc 0.578125
2016-12-13T13:11:21.995121: step 1279, loss 0.601292, acc 0.75
2016-12-13T13:11:26.398021: step 1280, loss 0.75487, acc 0.515625
2016-12-13T13:11:30.844850: step 1281, loss 0.75052, acc 0.53125
2016-12-13T13:11:35.534576: step 1282, loss 0.654307, acc 0.609375
2016-12-13T13:11:40.081137: step 1283, loss 0.644637, acc 0.640625
2016-12-13T13:11:44.766870: step 1284, loss 0.694775, acc 0.609375
2016-12-13T13:11:49.239212: step 1285, loss 0.654378, acc 0.578125
2016-12-13T13:11:53.706572: step 1286, loss 0.649144, acc 0.671875
2016-12-13T13:11:58.167204: step 1287, loss 0.642397, acc 0.65625
2016-12-13T13:12:02.632103: step 1288, loss 0.685581, acc 0.578125
2016-12-13T13:12:07.135182: step 1289, loss 0.568215, acc 0.78125
2016-12-13T13:12:11.634310: step 1290, loss 0.628118, acc 0.625
2016-12-13T13:12:16.311342: step 1291, loss 0.698628, acc 0.640625
2016-12-13T13:12:20.779487: step 1292, loss 0.777001, acc 0.5
2016-12-13T13:12:25.314015: step 1293, loss 0.65707, acc 0.609375
2016-12-13T13:12:29.746922: step 1294, loss 0.571414, acc 0.71875
2016-12-13T13:12:34.182669: step 1295, loss 0.693644, acc 0.546875
2016-12-13T13:12:38.632498: step 1296, loss 0.709412, acc 0.484375
2016-12-13T13:12:43.071225: step 1297, loss 0.647678, acc 0.59375
2016-12-13T13:12:47.586805: step 1298, loss 0.75421, acc 0.390625
2016-12-13T13:12:52.123429: step 1299, loss 0.752092, acc 0.453125
2016-12-13T13:12:56.420392: step 1300, loss 0.693865, acc 0.484375

Evaluation:
2016-12-13T13:14:55.159794: step 1300, loss 0.650935, acc 0.61936

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481637188/checkpoints/model-1300

2016-12-13T13:15:02.890669: step 1301, loss 0.726404, acc 0.484375
2016-12-13T13:15:07.271861: step 1302, loss 0.734695, acc 0.484375
2016-12-13T13:15:11.532156: step 1303, loss 0.720939, acc 0.578125
2016-12-13T13:15:15.995441: step 1304, loss 0.661391, acc 0.625
2016-12-13T13:15:20.314070: step 1305, loss 0.690046, acc 0.5625
2016-12-13T13:15:24.683848: step 1306, loss 0.693213, acc 0.515625
2016-12-13T13:15:29.029759: step 1307, loss 0.625196, acc 0.65625
2016-12-13T13:15:33.320852: step 1308, loss 0.697094, acc 0.578125
2016-12-13T13:15:37.677683: step 1309, loss 0.64821, acc 0.671875
2016-12-13T13:15:41.983440: step 1310, loss 0.718802, acc 0.5625
2016-12-13T13:15:46.470454: step 1311, loss 0.696006, acc 0.546875
2016-12-13T13:15:51.017937: step 1312, loss 0.714761, acc 0.625
2016-12-13T13:15:55.705576: step 1313, loss 0.670634, acc 0.578125
2016-12-13T13:16:00.278437: step 1314, loss 0.676344, acc 0.609375
2016-12-13T13:16:04.885968: step 1315, loss 0.646937, acc 0.59375
2016-12-13T13:16:09.549842: step 1316, loss 0.601512, acc 0.734375
2016-12-13T13:16:14.866866: step 1317, loss 0.691056, acc 0.5625
2016-12-13T13:16:19.633740: step 1318, loss 0.683329, acc 0.625
2016-12-13T13:16:24.177399: step 1319, loss 0.698162, acc 0.53125
2016-12-13T13:16:28.845581: step 1320, loss 0.70135, acc 0.5625
2016-12-13T13:16:33.522384: step 1321, loss 0.610862, acc 0.609375
2016-12-13T13:16:37.946232: step 1322, loss 0.74356, acc 0.53125
2016-12-13T13:16:42.390662: step 1323, loss 0.692139, acc 0.609375
2016-12-13T13:16:46.835637: step 1324, loss 0.673934, acc 0.5625
2016-12-13T13:16:51.522594: step 1325, loss 0.679644, acc 0.53125
2016-12-13T13:16:56.033012: step 1326, loss 0.737659, acc 0.5625
2016-12-13T13:17:00.459633: step 1327, loss 0.750025, acc 0.484375
2016-12-13T13:17:04.916970: step 1328, loss 0.716826, acc 0.515625
2016-12-13T13:17:09.377481: step 1329, loss 0.698058, acc 0.453125
2016-12-13T13:17:13.822351: step 1330, loss 0.683092, acc 0.59375
2016-12-13T13:17:18.325679: step 1331, loss 0.780847, acc 0.53125
2016-12-13T13:17:23.017331: step 1332, loss 0.618669, acc 0.59375
2016-12-13T13:17:27.483597: step 1333, loss 0.709659, acc 0.609375
2016-12-13T13:17:31.913637: step 1334, loss 0.640759, acc 0.65625
2016-12-13T13:17:36.389842: step 1335, loss 0.623464, acc 0.578125
2016-12-13T13:17:40.842606: step 1336, loss 0.638081, acc 0.671875
2016-12-13T13:17:45.313512: step 1337, loss 0.585823, acc 0.6875
2016-12-13T13:17:49.774364: step 1338, loss 0.724991, acc 0.484375
2016-12-13T13:17:54.263621: step 1339, loss 0.64264, acc 0.65625
2016-12-13T13:17:58.791553: step 1340, loss 0.67911, acc 0.59375
2016-12-13T13:18:03.456278: step 1341, loss 0.683028, acc 0.5625
2016-12-13T13:18:08.658925: step 1342, loss 0.655474, acc 0.625
2016-12-13T13:18:13.144482: step 1343, loss 0.648117, acc 0.625
2016-12-13T13:18:17.624877: step 1344, loss 0.688345, acc 0.5625
2016-12-13T13:18:22.055867: step 1345, loss 0.725856, acc 0.59375
2016-12-13T13:18:26.658579: step 1346, loss 0.706873, acc 0.546875
2016-12-13T13:18:31.185358: step 1347, loss 0.666367, acc 0.65625
2016-12-13T13:18:35.756976: step 1348, loss 0.635189, acc 0.640625
2016-12-13T13:18:40.245018: step 1349, loss 0.695032, acc 0.546875
2016-12-13T13:18:44.718036: step 1350, loss 0.662072, acc 0.609375
2016-12-13T13:18:49.173446: step 1351, loss 0.721476, acc 0.515625
2016-12-13T13:18:53.647742: step 1352, loss 0.710075, acc 0.515625
2016-12-13T13:18:58.174705: step 1353, loss 0.657947, acc 0.625
2016-12-13T13:19:02.718776: step 1354, loss 0.647419, acc 0.578125
2016-12-13T13:19:07.131193: step 1355, loss 0.698296, acc 0.578125
2016-12-13T13:19:11.644324: step 1356, loss 0.675413, acc 0.578125
2016-12-13T13:19:16.287089: step 1357, loss 0.73306, acc 0.4375
2016-12-13T13:19:20.940368: step 1358, loss 0.635683, acc 0.671875
2016-12-13T13:19:25.363048: step 1359, loss 0.634874, acc 0.625
2016-12-13T13:19:29.963414: step 1360, loss 0.67027, acc 0.5625
2016-12-13T13:19:34.546383: step 1361, loss 0.688918, acc 0.5625
2016-12-13T13:19:38.933461: step 1362, loss 0.670031, acc 0.65625
2016-12-13T13:19:43.559018: step 1363, loss 0.707196, acc 0.65625
2016-12-13T13:19:48.143043: step 1364, loss 0.670645, acc 0.578125
2016-12-13T13:19:52.719591: step 1365, loss 0.701763, acc 0.578125
2016-12-13T13:19:57.221408: step 1366, loss 0.683382, acc 0.625
2016-12-13T13:20:01.997656: step 1367, loss 0.697986, acc 0.546875
2016-12-13T13:20:06.596495: step 1368, loss 0.690226, acc 0.546875
2016-12-13T13:20:11.127866: step 1369, loss 0.645784, acc 0.65625
2016-12-13T13:20:15.710184: step 1370, loss 0.658607, acc 0.59375
2016-12-13T13:20:20.270896: step 1371, loss 0.654758, acc 0.59375
2016-12-13T13:20:24.791595: step 1372, loss 0.617956, acc 0.625
2016-12-13T13:20:29.298929: step 1373, loss 0.753214, acc 0.484375
2016-12-13T13:20:33.977659: step 1374, loss 0.750428, acc 0.578125
2016-12-13T13:20:38.523533: step 1375, loss 0.650951, acc 0.59375
2016-12-13T13:20:42.927765: step 1376, loss 0.69328, acc 0.515625
2016-12-13T13:20:47.410918: step 1377, loss 0.644015, acc 0.640625
2016-12-13T13:20:51.835862: step 1378, loss 0.75681, acc 0.546875
2016-12-13T13:20:56.300752: step 1379, loss 0.640478, acc 0.65625
2016-12-13T13:21:00.840476: step 1380, loss 0.595077, acc 0.703125
2016-12-13T13:21:05.226370: step 1381, loss 0.598675, acc 0.671875
2016-12-13T13:21:09.695461: step 1382, loss 0.602612, acc 0.65625
2016-12-13T13:21:14.079834: step 1383, loss 0.696769, acc 0.625
2016-12-13T13:21:18.476670: step 1384, loss 0.735494, acc 0.5625
2016-12-13T13:21:22.876000: step 1385, loss 0.725867, acc 0.53125
2016-12-13T13:21:27.262479: step 1386, loss 0.695751, acc 0.515625
2016-12-13T13:21:31.612083: step 1387, loss 0.658467, acc 0.578125
2016-12-13T13:21:36.004276: step 1388, loss 0.656703, acc 0.640625
2016-12-13T13:21:40.534912: step 1389, loss 0.63565, acc 0.671875
2016-12-13T13:21:44.935687: step 1390, loss 0.657533, acc 0.6875
2016-12-13T13:21:49.280355: step 1391, loss 0.669266, acc 0.625
2016-12-13T13:21:53.661095: step 1392, loss 0.720444, acc 0.546875
2016-12-13T13:21:58.022202: step 1393, loss 0.588346, acc 0.703125
2016-12-13T13:22:02.590319: step 1394, loss 0.611417, acc 0.671875
2016-12-13T13:22:07.122974: step 1395, loss 0.680386, acc 0.59375
2016-12-13T13:22:11.753035: step 1396, loss 0.609039, acc 0.703125
2016-12-13T13:22:16.254045: step 1397, loss 0.731875, acc 0.453125
2016-12-13T13:22:20.703765: step 1398, loss 0.694061, acc 0.515625
2016-12-13T13:22:25.153679: step 1399, loss 0.662517, acc 0.609375
2016-12-13T13:22:29.504147: step 1400, loss 0.7276, acc 0.46875

Evaluation:
2016-12-13T13:24:02.310263: step 1400, loss 0.653103, acc 0.622999

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481637188/checkpoints/model-1400

2016-12-13T13:24:10.140680: step 1401, loss 0.686295, acc 0.5625
2016-12-13T13:24:14.653762: step 1402, loss 0.687978, acc 0.59375
2016-12-13T13:24:19.292561: step 1403, loss 0.658191, acc 0.640625
2016-12-13T13:24:23.805364: step 1404, loss 0.674929, acc 0.671875
2016-12-13T13:24:28.325340: step 1405, loss 0.616126, acc 0.71875
2016-12-13T13:24:32.796059: step 1406, loss 0.673706, acc 0.609375
2016-12-13T13:24:37.234677: step 1407, loss 0.625712, acc 0.6875
2016-12-13T13:24:41.574891: step 1408, loss 0.659638, acc 0.640625
2016-12-13T13:24:46.083987: step 1409, loss 0.720884, acc 0.546875
2016-12-13T13:24:50.529340: step 1410, loss 0.730098, acc 0.5625
2016-12-13T13:24:55.227155: step 1411, loss 0.739778, acc 0.515625
2016-12-13T13:24:59.742037: step 1412, loss 0.643379, acc 0.640625
2016-12-13T13:25:04.117573: step 1413, loss 0.684259, acc 0.5
2016-12-13T13:25:08.584805: step 1414, loss 0.672372, acc 0.609375
2016-12-13T13:25:12.945081: step 1415, loss 0.658141, acc 0.609375
2016-12-13T13:25:17.255114: step 1416, loss 0.600345, acc 0.65625
2016-12-13T13:25:21.549990: step 1417, loss 0.687554, acc 0.609375
2016-12-13T13:25:25.902399: step 1418, loss 0.778975, acc 0.578125
2016-12-13T13:25:30.200471: step 1419, loss 0.854896, acc 0.515625
2016-12-13T13:25:34.512317: step 1420, loss 0.646414, acc 0.625
2016-12-13T13:25:38.994547: step 1421, loss 0.715612, acc 0.671875
2016-12-13T13:25:43.362509: step 1422, loss 0.71389, acc 0.625
2016-12-13T13:25:47.684055: step 1423, loss 0.648266, acc 0.59375
2016-12-13T13:25:52.009938: step 1424, loss 0.693864, acc 0.625
2016-12-13T13:25:56.325377: step 1425, loss 0.727871, acc 0.5
2016-12-13T13:26:00.704326: step 1426, loss 0.762602, acc 0.421875
2016-12-13T13:26:05.101511: step 1427, loss 0.753804, acc 0.5
2016-12-13T13:26:09.611991: step 1428, loss 0.696357, acc 0.5625
2016-12-13T13:26:13.951602: step 1429, loss 0.584387, acc 0.65625
2016-12-13T13:26:18.298779: step 1430, loss 0.792782, acc 0.515625
2016-12-13T13:26:22.612049: step 1431, loss 0.70009, acc 0.609375
2016-12-13T13:26:27.097293: step 1432, loss 0.672258, acc 0.671875
2016-12-13T13:26:31.575797: step 1433, loss 0.659732, acc 0.609375
2016-12-13T13:26:35.848697: step 1434, loss 0.597352, acc 0.6875
2016-12-13T13:26:40.118583: step 1435, loss 0.679618, acc 0.53125
2016-12-13T13:26:44.611477: step 1436, loss 0.646781, acc 0.609375
2016-12-13T13:26:48.917316: step 1437, loss 0.639019, acc 0.65625
2016-12-13T13:26:53.159503: step 1438, loss 0.647192, acc 0.640625
2016-12-13T13:26:57.483169: step 1439, loss 0.628536, acc 0.671875
2016-12-13T13:27:01.942619: step 1440, loss 0.677926, acc 0.5625
2016-12-13T13:27:06.352050: step 1441, loss 0.632368, acc 0.640625
2016-12-13T13:27:10.759377: step 1442, loss 0.664697, acc 0.609375
2016-12-13T13:27:15.348515: step 1443, loss 0.639818, acc 0.671875
2016-12-13T13:27:19.771589: step 1444, loss 0.688915, acc 0.578125
2016-12-13T13:27:24.169587: step 1445, loss 0.694569, acc 0.609375
2016-12-13T13:27:28.544960: step 1446, loss 0.58869, acc 0.65625
2016-12-13T13:27:32.867308: step 1447, loss 0.720813, acc 0.546875
2016-12-13T13:27:37.229584: step 1448, loss 0.656998, acc 0.59375
2016-12-13T13:27:41.557574: step 1449, loss 0.70269, acc 0.59375
2016-12-13T13:27:45.968677: step 1450, loss 0.687074, acc 0.609375
2016-12-13T13:27:50.316204: step 1451, loss 0.707286, acc 0.546875
2016-12-13T13:27:54.645507: step 1452, loss 0.672985, acc 0.546875
2016-12-13T13:27:58.938810: step 1453, loss 0.711646, acc 0.5
2016-12-13T13:28:03.295808: step 1454, loss 0.690133, acc 0.53125
2016-12-13T13:28:07.663770: step 1455, loss 0.646855, acc 0.609375
2016-12-13T13:28:11.947974: step 1456, loss 0.639888, acc 0.578125
2016-12-13T13:28:16.263590: step 1457, loss 0.698277, acc 0.625
2016-12-13T13:28:20.740592: step 1458, loss 0.633924, acc 0.65625
2016-12-13T13:28:25.083544: step 1459, loss 0.78091, acc 0.53125
2016-12-13T13:28:29.387505: step 1460, loss 0.653206, acc 0.625
2016-12-13T13:28:33.703976: step 1461, loss 0.648412, acc 0.625
2016-12-13T13:28:37.982883: step 1462, loss 0.655453, acc 0.625
2016-12-13T13:28:42.304548: step 1463, loss 0.65628, acc 0.59375
2016-12-13T13:28:46.591086: step 1464, loss 0.638928, acc 0.6875
2016-12-13T13:28:51.072542: step 1465, loss 0.603078, acc 0.65625
2016-12-13T13:28:55.365173: step 1466, loss 0.733861, acc 0.546875
2016-12-13T13:28:59.714741: step 1467, loss 0.712025, acc 0.640625
2016-12-13T13:29:04.025082: step 1468, loss 0.787445, acc 0.546875
2016-12-13T13:29:08.354649: step 1469, loss 0.633989, acc 0.609375
2016-12-13T13:29:12.694715: step 1470, loss 0.641589, acc 0.640625
2016-12-13T13:29:17.008310: step 1471, loss 0.645529, acc 0.609375
2016-12-13T13:29:21.291809: step 1472, loss 0.708866, acc 0.5
2016-12-13T13:29:25.743419: step 1473, loss 0.723495, acc 0.546875
2016-12-13T13:29:30.030937: step 1474, loss 0.709799, acc 0.546875
2016-12-13T13:29:34.284337: step 1475, loss 0.636937, acc 0.671875
2016-12-13T13:29:38.557997: step 1476, loss 0.642515, acc 0.609375
2016-12-13T13:29:42.860019: step 1477, loss 0.65963, acc 0.59375
2016-12-13T13:29:47.213114: step 1478, loss 0.83823, acc 0.53125
2016-12-13T13:29:51.493545: step 1479, loss 0.64814, acc 0.671875
2016-12-13T13:29:56.010690: step 1480, loss 0.84784, acc 0.484375
2016-12-13T13:30:00.365082: step 1481, loss 0.72984, acc 0.578125
2016-12-13T13:30:04.801480: step 1482, loss 0.630583, acc 0.609375
2016-12-13T13:30:09.186315: step 1483, loss 0.675192, acc 0.59375
2016-12-13T13:30:13.608108: step 1484, loss 0.756751, acc 0.46875
2016-12-13T13:30:17.933542: step 1485, loss 0.806196, acc 0.40625
2016-12-13T13:30:22.280176: step 1486, loss 0.784204, acc 0.453125
2016-12-13T13:30:26.745313: step 1487, loss 0.587907, acc 0.671875
2016-12-13T13:30:31.089722: step 1488, loss 0.699492, acc 0.609375
2016-12-13T13:30:35.397357: step 1489, loss 0.783644, acc 0.59375
2016-12-13T13:30:39.793767: step 1490, loss 0.751507, acc 0.625
2016-12-13T13:30:44.125394: step 1491, loss 0.73615, acc 0.515625
2016-12-13T13:30:48.459494: step 1492, loss 0.672258, acc 0.640625
2016-12-13T13:30:52.766669: step 1493, loss 0.69475, acc 0.5625
2016-12-13T13:30:57.110734: step 1494, loss 0.718247, acc 0.46875
2016-12-13T13:31:01.550303: step 1495, loss 0.711507, acc 0.46875
2016-12-13T13:31:05.924278: step 1496, loss 0.740769, acc 0.578125
2016-12-13T13:31:10.190382: step 1497, loss 0.719249, acc 0.484375
2016-12-13T13:31:14.587741: step 1498, loss 0.691442, acc 0.5625
2016-12-13T13:31:18.921465: step 1499, loss 0.628374, acc 0.703125
2016-12-13T13:31:23.270203: step 1500, loss 0.639514, acc 0.71875

Evaluation:
2016-12-13T13:32:40.823860: step 1500, loss 0.694519, acc 0.59607

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481637188/checkpoints/model-1500

2016-12-13T13:32:48.210936: step 1501, loss 0.696905, acc 0.609375
2016-12-13T13:32:52.519380: step 1502, loss 0.721515, acc 0.59375
2016-12-13T13:32:56.865319: step 1503, loss 0.727153, acc 0.609375
2016-12-13T13:33:01.243509: step 1504, loss 0.695128, acc 0.59375
2016-12-13T13:33:05.659215: step 1505, loss 0.644181, acc 0.640625
2016-12-13T13:33:10.039375: step 1506, loss 0.681209, acc 0.5625
2016-12-13T13:33:14.371634: step 1507, loss 0.731179, acc 0.53125
2016-12-13T13:33:18.826571: step 1508, loss 0.679906, acc 0.578125
2016-12-13T13:33:23.192294: step 1509, loss 0.639065, acc 0.59375
2016-12-13T13:33:27.471815: step 1510, loss 0.703316, acc 0.53125
2016-12-13T13:33:31.813063: step 1511, loss 0.66277, acc 0.640625
2016-12-13T13:33:36.102599: step 1512, loss 0.704367, acc 0.5
2016-12-13T13:33:40.388139: step 1513, loss 0.739493, acc 0.546875
2016-12-13T13:33:44.763972: step 1514, loss 0.790429, acc 0.5625
2016-12-13T13:33:49.064498: step 1515, loss 0.756153, acc 0.453125
2016-12-13T13:33:53.527488: step 1516, loss 0.626473, acc 0.65625
2016-12-13T13:33:57.882094: step 1517, loss 0.689364, acc 0.609375
2016-12-13T13:34:02.172575: step 1518, loss 0.668607, acc 0.65625
2016-12-13T13:34:06.468271: step 1519, loss 0.725342, acc 0.53125
2016-12-13T13:34:10.858464: step 1520, loss 0.761128, acc 0.40625
2016-12-13T13:34:15.130873: step 1521, loss 0.620838, acc 0.71875
2016-12-13T13:34:19.689010: step 1522, loss 0.738978, acc 0.5625
2016-12-13T13:34:24.168137: step 1523, loss 0.581947, acc 0.65625
2016-12-13T13:34:28.452811: step 1524, loss 0.685695, acc 0.671875
2016-12-13T13:34:32.704672: step 1525, loss 0.741368, acc 0.59375
2016-12-13T13:34:37.021859: step 1526, loss 0.706788, acc 0.65625
2016-12-13T13:34:41.348097: step 1527, loss 0.716787, acc 0.578125
2016-12-13T13:34:45.614073: step 1528, loss 0.700442, acc 0.59375
2016-12-13T13:34:49.939483: step 1529, loss 0.69338, acc 0.609375
2016-12-13T13:34:54.254818: step 1530, loss 0.640784, acc 0.609375
2016-12-13T13:34:58.728751: step 1531, loss 0.697609, acc 0.515625
2016-12-13T13:35:03.051432: step 1532, loss 0.682512, acc 0.546875
2016-12-13T13:35:07.325191: step 1533, loss 0.711335, acc 0.5625
2016-12-13T13:35:11.625162: step 1534, loss 0.656096, acc 0.609375
2016-12-13T13:35:15.985619: step 1535, loss 0.613235, acc 0.6875
2016-12-13T13:35:20.277431: step 1536, loss 0.748427, acc 0.5625
2016-12-13T13:35:24.604565: step 1537, loss 0.767802, acc 0.578125
2016-12-13T13:35:29.100519: step 1538, loss 0.79202, acc 0.5
2016-12-13T13:35:33.431195: step 1539, loss 0.666036, acc 0.578125
2016-12-13T13:35:37.752124: step 1540, loss 0.684147, acc 0.53125
2016-12-13T13:35:42.068400: step 1541, loss 0.733907, acc 0.421875
2016-12-13T13:35:46.376887: step 1542, loss 0.682144, acc 0.5625
2016-12-13T13:35:50.616940: step 1543, loss 0.680733, acc 0.53125
2016-12-13T13:35:54.958020: step 1544, loss 0.647836, acc 0.609375
2016-12-13T13:35:59.249757: step 1545, loss 0.670989, acc 0.625
2016-12-13T13:36:03.786502: step 1546, loss 0.707894, acc 0.59375
2016-12-13T13:36:08.067269: step 1547, loss 0.65207, acc 0.703125
2016-12-13T13:36:12.379861: step 1548, loss 0.789691, acc 0.578125
2016-12-13T13:36:16.764382: step 1549, loss 0.728826, acc 0.625
2016-12-13T13:36:21.072048: step 1550, loss 0.773455, acc 0.515625
2016-12-13T13:36:25.370257: step 1551, loss 0.630665, acc 0.640625
2016-12-13T13:36:29.788373: step 1552, loss 0.702505, acc 0.484375
2016-12-13T13:36:34.342840: step 1553, loss 0.695676, acc 0.515625
2016-12-13T13:36:38.664021: step 1554, loss 0.659562, acc 0.59375
2016-12-13T13:36:43.017388: step 1555, loss 0.684997, acc 0.59375
2016-12-13T13:36:47.324910: step 1556, loss 0.695974, acc 0.59375
2016-12-13T13:36:51.663151: step 1557, loss 0.759449, acc 0.546875
2016-12-13T13:36:55.967753: step 1558, loss 0.660116, acc 0.59375
2016-12-13T13:37:00.240015: step 1559, loss 0.679365, acc 0.640625
2016-12-13T13:37:04.769473: step 1560, loss 0.654848, acc 0.640625
2016-12-13T13:37:09.267510: step 1561, loss 0.635306, acc 0.625
2016-12-13T13:37:13.650834: step 1562, loss 0.724289, acc 0.5
2016-12-13T13:37:18.058554: step 1563, loss 0.678873, acc 0.609375
2016-12-13T13:37:22.493916: step 1564, loss 0.703258, acc 0.625
2016-12-13T13:37:26.939798: step 1565, loss 0.698571, acc 0.5625
2016-12-13T13:37:31.639358: step 1566, loss 0.661118, acc 0.609375
2016-12-13T13:37:36.045643: step 1567, loss 0.621455, acc 0.6875
2016-12-13T13:37:40.466352: step 1568, loss 0.699934, acc 0.578125
2016-12-13T13:37:44.715164: step 1569, loss 0.654569, acc 0.671875
2016-12-13T13:37:49.014378: step 1570, loss 0.695215, acc 0.671875
2016-12-13T13:37:53.343313: step 1571, loss 0.668909, acc 0.59375
2016-12-13T13:37:57.650824: step 1572, loss 0.545644, acc 0.75
2016-12-13T13:38:01.912356: step 1573, loss 0.632597, acc 0.65625
2016-12-13T13:38:06.229900: step 1574, loss 0.629532, acc 0.640625
2016-12-13T13:38:10.730408: step 1575, loss 0.659755, acc 0.640625
2016-12-13T13:38:15.067856: step 1576, loss 0.70628, acc 0.5625
2016-12-13T13:38:19.367825: step 1577, loss 0.69964, acc 0.546875
2016-12-13T13:38:23.799140: step 1578, loss 0.693093, acc 0.578125
2016-12-13T13:38:28.206813: step 1579, loss 0.626252, acc 0.671875
2016-12-13T13:38:32.507742: step 1580, loss 0.699268, acc 0.5625
2016-12-13T13:38:36.911686: step 1581, loss 0.689635, acc 0.59375
2016-12-13T13:38:41.361403: step 1582, loss 0.651153, acc 0.609375
2016-12-13T13:38:45.688489: step 1583, loss 0.685264, acc 0.578125
2016-12-13T13:38:49.953401: step 1584, loss 0.677957, acc 0.53125
2016-12-13T13:38:54.239332: step 1585, loss 0.630477, acc 0.671875
2016-12-13T13:38:58.561897: step 1586, loss 0.650388, acc 0.640625
2016-12-13T13:39:02.783570: step 1587, loss 0.671293, acc 0.578125
2016-12-13T13:39:07.087947: step 1588, loss 0.654216, acc 0.65625
2016-12-13T13:39:11.340936: step 1589, loss 0.675141, acc 0.578125
2016-12-13T13:39:15.860462: step 1590, loss 0.641257, acc 0.609375
2016-12-13T13:39:20.177460: step 1591, loss 0.672682, acc 0.65625
2016-12-13T13:39:24.425140: step 1592, loss 0.664968, acc 0.640625
2016-12-13T13:39:28.699141: step 1593, loss 0.667349, acc 0.625
2016-12-13T13:39:33.011930: step 1594, loss 0.558721, acc 0.765625
2016-12-13T13:39:37.419575: step 1595, loss 0.630055, acc 0.703125
2016-12-13T13:39:41.754471: step 1596, loss 0.785835, acc 0.59375
2016-12-13T13:39:46.280417: step 1597, loss 0.640607, acc 0.625
2016-12-13T13:39:50.611881: step 1598, loss 0.662558, acc 0.6875
2016-12-13T13:39:54.943818: step 1599, loss 0.667578, acc 0.5625
2016-12-13T13:39:59.313159: step 1600, loss 0.593111, acc 0.640625

Evaluation:
2016-12-13T13:41:38.069038: step 1600, loss 0.654197, acc 0.620087

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481637188/checkpoints/model-1600

2016-12-13T13:41:45.946250: step 1601, loss 0.66458, acc 0.625
2016-12-13T13:41:50.193268: step 1602, loss 0.692373, acc 0.5625
2016-12-13T13:41:54.487624: step 1603, loss 0.624385, acc 0.734375
2016-12-13T13:41:58.787555: step 1604, loss 0.661443, acc 0.578125
2016-12-13T13:42:03.148935: step 1605, loss 0.69684, acc 0.578125
2016-12-13T13:42:07.799126: step 1606, loss 0.648511, acc 0.609375
2016-12-13T13:42:12.418312: step 1607, loss 0.60956, acc 0.6875
2016-12-13T13:42:16.768842: step 1608, loss 0.586279, acc 0.71875
2016-12-13T13:42:21.131494: step 1609, loss 0.73776, acc 0.484375
2016-12-13T13:42:25.505433: step 1610, loss 0.654261, acc 0.71875
2016-12-13T13:42:29.895195: step 1611, loss 0.657275, acc 0.640625
2016-12-13T13:42:34.205773: step 1612, loss 0.643762, acc 0.59375
2016-12-13T13:42:38.563053: step 1613, loss 0.694111, acc 0.59375
2016-12-13T13:42:43.105914: step 1614, loss 0.632899, acc 0.625
2016-12-13T13:42:47.385917: step 1615, loss 0.781283, acc 0.46875
2016-12-13T13:42:51.709263: step 1616, loss 0.658671, acc 0.59375
2016-12-13T13:42:55.986316: step 1617, loss 0.650229, acc 0.640625
2016-12-13T13:43:00.354026: step 1618, loss 0.677122, acc 0.609375
2016-12-13T13:43:04.687882: step 1619, loss 0.671714, acc 0.578125
2016-12-13T13:43:08.970741: step 1620, loss 0.642893, acc 0.65625
2016-12-13T13:43:13.253054: step 1621, loss 0.625331, acc 0.640625
2016-12-13T13:43:17.718807: step 1622, loss 0.671075, acc 0.59375
2016-12-13T13:43:21.984737: step 1623, loss 0.659841, acc 0.640625
2016-12-13T13:43:26.257144: step 1624, loss 0.634738, acc 0.65625
2016-12-13T13:43:30.583814: step 1625, loss 0.71561, acc 0.53125
2016-12-13T13:43:34.922766: step 1626, loss 0.647466, acc 0.671875
2016-12-13T13:43:39.231509: step 1627, loss 0.703744, acc 0.59375
2016-12-13T13:43:43.536077: step 1628, loss 0.645163, acc 0.609375
2016-12-13T13:43:48.026513: step 1629, loss 0.656025, acc 0.65625
2016-12-13T13:43:52.347830: step 1630, loss 0.6744, acc 0.5625
2016-12-13T13:43:56.672899: step 1631, loss 0.586954, acc 0.71875
2016-12-13T13:44:01.054355: step 1632, loss 0.665532, acc 0.640625
2016-12-13T13:44:05.482532: step 1633, loss 0.602733, acc 0.75
2016-12-13T13:44:09.777625: step 1634, loss 0.607875, acc 0.59375
2016-12-13T13:44:14.246116: step 1635, loss 0.700539, acc 0.546875
2016-12-13T13:44:18.600647: step 1636, loss 0.689075, acc 0.59375
2016-12-13T13:44:23.006313: step 1637, loss 0.608192, acc 0.671875
2016-12-13T13:44:27.311970: step 1638, loss 0.689776, acc 0.5625
2016-12-13T13:44:31.636491: step 1639, loss 0.750166, acc 0.484375
2016-12-13T13:44:35.939941: step 1640, loss 0.670574, acc 0.5625
2016-12-13T13:44:40.198161: step 1641, loss 0.690026, acc 0.578125
2016-12-13T13:44:44.511338: step 1642, loss 0.684255, acc 0.625
2016-12-13T13:44:48.872879: step 1643, loss 0.668988, acc 0.609375
2016-12-13T13:44:53.343128: step 1644, loss 0.650718, acc 0.59375
2016-12-13T13:44:57.646270: step 1645, loss 0.652883, acc 0.734375
2016-12-13T13:45:01.940114: step 1646, loss 0.776512, acc 0.546875
2016-12-13T13:45:06.250195: step 1647, loss 0.669633, acc 0.609375
2016-12-13T13:45:10.529533: step 1648, loss 0.656865, acc 0.59375
2016-12-13T13:45:14.927029: step 1649, loss 0.760109, acc 0.46875
2016-12-13T13:45:19.262214: step 1650, loss 0.564283, acc 0.71875
2016-12-13T13:45:23.631239: step 1651, loss 0.606615, acc 0.6875
2016-12-13T13:45:28.085124: step 1652, loss 0.684996, acc 0.5
2016-12-13T13:45:32.522770: step 1653, loss 0.688833, acc 0.546875
2016-12-13T13:45:36.784910: step 1654, loss 0.637698, acc 0.6875
2016-12-13T13:45:41.130898: step 1655, loss 0.641067, acc 0.640625
2016-12-13T13:45:45.956493: step 1656, loss 0.672801, acc 0.625
2016-12-13T13:45:50.334187: step 1657, loss 0.631135, acc 0.65625
2016-12-13T13:45:54.630923: step 1658, loss 0.674353, acc 0.59375
2016-12-13T13:45:59.065176: step 1659, loss 0.689037, acc 0.625
2016-12-13T13:46:03.400332: step 1660, loss 0.679356, acc 0.625
2016-12-13T13:46:07.774034: step 1661, loss 0.687129, acc 0.578125
2016-12-13T13:46:12.075823: step 1662, loss 0.768467, acc 0.5
2016-12-13T13:46:16.326966: step 1663, loss 0.718863, acc 0.46875
2016-12-13T13:46:20.634033: step 1664, loss 0.725896, acc 0.546875
2016-12-13T13:46:24.942254: step 1665, loss 0.775948, acc 0.4375
2016-12-13T13:46:29.593659: step 1666, loss 0.786695, acc 0.421875
2016-12-13T13:46:33.933786: step 1667, loss 0.696141, acc 0.609375
2016-12-13T13:46:38.297069: step 1668, loss 0.644575, acc 0.640625
2016-12-13T13:46:42.593788: step 1669, loss 0.67493, acc 0.59375
2016-12-13T13:46:46.905644: step 1670, loss 0.692986, acc 0.59375
2016-12-13T13:46:51.192996: step 1671, loss 0.65401, acc 0.671875
2016-12-13T13:46:55.550157: step 1672, loss 0.761302, acc 0.65625
2016-12-13T13:46:59.923154: step 1673, loss 0.766068, acc 0.5625
2016-12-13T13:47:04.472956: step 1674, loss 0.773308, acc 0.484375
2016-12-13T13:47:08.786797: step 1675, loss 0.643976, acc 0.609375
2016-12-13T13:47:13.066423: step 1676, loss 0.719249, acc 0.59375
2016-12-13T13:47:17.395213: step 1677, loss 0.771931, acc 0.46875
2016-12-13T13:47:21.801432: step 1678, loss 0.800544, acc 0.421875
2016-12-13T13:47:26.188266: step 1679, loss 0.721456, acc 0.515625
2016-12-13T13:47:30.602767: step 1680, loss 0.708207, acc 0.53125
2016-12-13T13:47:35.210216: step 1681, loss 0.663603, acc 0.59375
2016-12-13T13:47:39.487155: step 1682, loss 0.597541, acc 0.71875
2016-12-13T13:47:43.733983: step 1683, loss 0.716032, acc 0.5625
2016-12-13T13:47:48.075662: step 1684, loss 0.878575, acc 0.546875
2016-12-13T13:47:52.385713: step 1685, loss 0.68108, acc 0.671875
2016-12-13T13:47:56.756069: step 1686, loss 0.73863, acc 0.53125
2016-12-13T13:48:01.089446: step 1687, loss 0.62514, acc 0.671875
2016-12-13T13:48:05.618780: step 1688, loss 0.681243, acc 0.515625
2016-12-13T13:48:10.001458: step 1689, loss 0.72636, acc 0.46875
2016-12-13T13:48:14.360516: step 1690, loss 0.723207, acc 0.515625
2016-12-13T13:48:18.767898: step 1691, loss 0.704866, acc 0.515625
2016-12-13T13:48:23.181998: step 1692, loss 0.70675, acc 0.53125
2016-12-13T13:48:27.516718: step 1693, loss 0.738882, acc 0.546875
2016-12-13T13:48:31.801016: step 1694, loss 0.642468, acc 0.671875
2016-12-13T13:48:36.126215: step 1695, loss 0.642185, acc 0.671875
2016-12-13T13:48:40.554457: step 1696, loss 0.747076, acc 0.53125
2016-12-13T13:48:44.808934: step 1697, loss 0.656721, acc 0.625
2016-12-13T13:48:49.074265: step 1698, loss 0.675782, acc 0.53125
2016-12-13T13:48:53.396246: step 1699, loss 0.713959, acc 0.5625
2016-12-13T13:48:57.718017: step 1700, loss 0.647927, acc 0.5625

Evaluation:
2016-12-13T13:49:54.010876: step 1700, loss 0.671912, acc 0.58952

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481637188/checkpoints/model-1700

2016-12-13T13:50:00.690893: step 1701, loss 0.703692, acc 0.5625
2016-12-13T13:50:04.936515: step 1702, loss 0.670176, acc 0.578125
2016-12-13T13:50:09.286770: step 1703, loss 0.669834, acc 0.59375
2016-12-13T13:50:13.714558: step 1704, loss 0.680175, acc 0.5625
2016-12-13T13:50:18.133504: step 1705, loss 0.65514, acc 0.59375
2016-12-13T13:50:22.570341: step 1706, loss 0.674435, acc 0.609375
2016-12-13T13:50:27.048827: step 1707, loss 0.667711, acc 0.671875
2016-12-13T13:50:31.457640: step 1708, loss 0.701277, acc 0.515625
2016-12-13T13:50:35.727450: step 1709, loss 0.657245, acc 0.671875
2016-12-13T13:50:40.055629: step 1710, loss 0.603586, acc 0.6875
2016-12-13T13:50:44.390240: step 1711, loss 0.594549, acc 0.6875
2016-12-13T13:50:48.760973: step 1712, loss 0.654226, acc 0.65625
2016-12-13T13:50:53.092831: step 1713, loss 0.711174, acc 0.578125
2016-12-13T13:50:57.606375: step 1714, loss 0.572359, acc 0.734375
2016-12-13T13:51:01.978625: step 1715, loss 0.756607, acc 0.53125
2016-12-13T13:51:06.354166: step 1716, loss 0.678078, acc 0.515625
2016-12-13T13:51:10.696818: step 1717, loss 0.765577, acc 0.59375
2016-12-13T13:51:14.979168: step 1718, loss 0.630783, acc 0.625
2016-12-13T13:51:19.269528: step 1719, loss 0.644997, acc 0.578125
2016-12-13T13:51:23.560983: step 1720, loss 0.685547, acc 0.546875
2016-12-13T13:51:28.251317: step 1721, loss 0.67743, acc 0.59375
2016-12-13T13:51:32.610174: step 1722, loss 0.69724, acc 0.578125
2016-12-13T13:51:36.914399: step 1723, loss 0.73778, acc 0.515625
2016-12-13T13:51:41.169741: step 1724, loss 0.658266, acc 0.65625
2016-12-13T13:51:45.472592: step 1725, loss 0.546127, acc 0.765625
2016-12-13T13:51:49.806446: step 1726, loss 0.760297, acc 0.5625
2016-12-13T13:51:54.047863: step 1727, loss 0.761101, acc 0.578125
2016-12-13T13:51:58.390605: step 1728, loss 0.74724, acc 0.578125
2016-12-13T13:52:02.915790: step 1729, loss 0.702241, acc 0.59375
2016-12-13T13:52:07.377101: step 1730, loss 0.621907, acc 0.671875
2016-12-13T13:52:11.721233: step 1731, loss 0.711361, acc 0.5625
2016-12-13T13:52:16.147723: step 1732, loss 0.699458, acc 0.578125
2016-12-13T13:52:20.536900: step 1733, loss 0.660757, acc 0.578125
2016-12-13T13:52:24.972271: step 1734, loss 0.70064, acc 0.5
2016-12-13T13:52:29.301706: step 1735, loss 0.670405, acc 0.59375
2016-12-13T13:52:33.836918: step 1736, loss 0.624243, acc 0.671875
2016-12-13T13:52:38.135308: step 1737, loss 0.590532, acc 0.703125
2016-12-13T13:52:42.477395: step 1738, loss 0.732098, acc 0.53125
2016-12-13T13:52:46.762492: step 1739, loss 0.595018, acc 0.6875
2016-12-13T13:52:51.046273: step 1740, loss 0.676015, acc 0.640625
2016-12-13T13:52:55.352455: step 1741, loss 0.672084, acc 0.625
2016-12-13T13:52:59.678497: step 1742, loss 0.619914, acc 0.6875
2016-12-13T13:53:04.021588: step 1743, loss 0.679611, acc 0.546875
2016-12-13T13:53:08.463962: step 1744, loss 0.646126, acc 0.640625
2016-12-13T13:53:12.790881: step 1745, loss 0.657331, acc 0.640625
2016-12-13T13:53:17.126293: step 1746, loss 0.759593, acc 0.46875
2016-12-13T13:53:21.407419: step 1747, loss 0.672563, acc 0.578125
2016-12-13T13:53:25.689357: step 1748, loss 0.711018, acc 0.453125
2016-12-13T13:53:29.947536: step 1749, loss 0.62367, acc 0.6875
2016-12-13T13:53:34.254705: step 1750, loss 0.598731, acc 0.640625
2016-12-13T13:53:38.748518: step 1751, loss 0.734177, acc 0.578125
2016-12-13T13:53:43.076256: step 1752, loss 0.823879, acc 0.46875
2016-12-13T13:53:47.388705: step 1753, loss 0.696397, acc 0.5625
2016-12-13T13:53:51.729471: step 1754, loss 0.64505, acc 0.671875
2016-12-13T13:53:56.062368: step 1755, loss 0.662899, acc 0.65625
2016-12-13T13:54:00.416663: step 1756, loss 0.696774, acc 0.5625
2016-12-13T13:54:04.709983: step 1757, loss 0.616289, acc 0.6875
2016-12-13T13:54:09.136099: step 1758, loss 0.641231, acc 0.6875
2016-12-13T13:54:13.516878: step 1759, loss 0.658802, acc 0.609375
2016-12-13T13:54:17.911336: step 1760, loss 0.771652, acc 0.40625
2016-12-13T13:54:22.243039: step 1761, loss 0.591149, acc 0.671875
2016-12-13T13:54:26.556400: step 1762, loss 0.62351, acc 0.703125
2016-12-13T13:54:30.876653: step 1763, loss 0.628217, acc 0.65625
2016-12-13T13:54:35.157276: step 1764, loss 0.69575, acc 0.609375
2016-12-13T13:54:39.476605: step 1765, loss 0.705014, acc 0.59375
2016-12-13T13:54:43.965482: step 1766, loss 0.748401, acc 0.5625
2016-12-13T13:54:48.234057: step 1767, loss 0.59878, acc 0.6875
2016-12-13T13:54:52.482553: step 1768, loss 0.706664, acc 0.53125
2016-12-13T13:54:56.745426: step 1769, loss 0.709578, acc 0.578125
2016-12-13T13:55:01.030047: step 1770, loss 0.685839, acc 0.546875
2016-12-13T13:55:05.300487: step 1771, loss 0.668306, acc 0.5625
2016-12-13T13:55:09.669604: step 1772, loss 0.669219, acc 0.625
2016-12-13T13:55:14.215505: step 1773, loss 0.603354, acc 0.703125
2016-12-13T13:55:18.680093: step 1774, loss 0.754432, acc 0.578125
2016-12-13T13:55:22.971641: step 1775, loss 0.724935, acc 0.625
2016-12-13T13:55:27.444748: step 1776, loss 0.633554, acc 0.703125
2016-12-13T13:55:31.782204: step 1777, loss 0.654323, acc 0.65625
2016-12-13T13:55:36.117697: step 1778, loss 0.591813, acc 0.703125
2016-12-13T13:55:40.408953: step 1779, loss 0.606179, acc 0.671875
2016-12-13T13:55:44.719781: step 1780, loss 0.658114, acc 0.578125
2016-12-13T13:55:49.195153: step 1781, loss 0.683167, acc 0.578125
2016-12-13T13:55:53.501083: step 1782, loss 0.68103, acc 0.5625
2016-12-13T13:55:57.795070: step 1783, loss 0.671218, acc 0.515625
2016-12-13T13:56:02.140553: step 1784, loss 0.649883, acc 0.671875
2016-12-13T13:56:06.474264: step 1785, loss 0.66996, acc 0.609375
2016-12-13T13:56:10.781469: step 1786, loss 0.635213, acc 0.640625
2016-12-13T13:56:15.127819: step 1787, loss 0.641374, acc 0.65625
2016-12-13T13:56:19.704140: step 1788, loss 0.634671, acc 0.640625
2016-12-13T13:56:24.058031: step 1789, loss 0.657899, acc 0.640625
2016-12-13T13:56:28.503201: step 1790, loss 0.682944, acc 0.65625
2016-12-13T13:56:32.864713: step 1791, loss 0.752427, acc 0.5625
2016-12-13T13:56:37.198565: step 1792, loss 0.581763, acc 0.6875
2016-12-13T13:56:41.517338: step 1793, loss 0.524604, acc 0.71875
2016-12-13T13:56:45.858784: step 1794, loss 0.620806, acc 0.65625
2016-12-13T13:56:50.250852: step 1795, loss 0.612473, acc 0.65625
2016-12-13T13:56:54.571320: step 1796, loss 0.766071, acc 0.46875
2016-12-13T13:56:58.896676: step 1797, loss 0.651946, acc 0.640625
2016-12-13T13:57:03.442889: step 1798, loss 0.705075, acc 0.546875
2016-12-13T13:57:07.901944: step 1799, loss 0.695367, acc 0.53125
2016-12-13T13:57:12.297399: step 1800, loss 0.641936, acc 0.609375

Evaluation:
2016-12-13T13:59:00.966178: step 1800, loss 0.651591, acc 0.625182

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481637188/checkpoints/model-1800

2016-12-13T13:59:08.782716: step 1801, loss 0.674951, acc 0.578125
2016-12-13T13:59:13.125292: step 1802, loss 0.652147, acc 0.65625
2016-12-13T13:59:17.624558: step 1803, loss 0.646265, acc 0.609375
2016-12-13T13:59:21.977141: step 1804, loss 0.647855, acc 0.671875
2016-12-13T13:59:26.330316: step 1805, loss 0.67412, acc 0.65625
2016-12-13T13:59:30.669043: step 1806, loss 0.62885, acc 0.671875
2016-12-13T13:59:34.981285: step 1807, loss 0.672998, acc 0.625
2016-12-13T13:59:39.318311: step 1808, loss 0.680093, acc 0.546875
2016-12-13T13:59:43.592422: step 1809, loss 0.743992, acc 0.59375
2016-12-13T13:59:47.851978: step 1810, loss 0.656342, acc 0.5625
2016-12-13T13:59:52.303478: step 1811, loss 0.654699, acc 0.59375
2016-12-13T13:59:56.606818: step 1812, loss 0.664047, acc 0.59375
2016-12-13T14:00:01.011140: step 1813, loss 0.662861, acc 0.59375
2016-12-13T14:00:05.487759: step 1814, loss 0.624029, acc 0.625
2016-12-13T14:00:09.952885: step 1815, loss 0.664274, acc 0.59375
2016-12-13T14:00:14.285752: step 1816, loss 0.581844, acc 0.6875
2016-12-13T14:00:18.598179: step 1817, loss 0.650737, acc 0.65625
2016-12-13T14:00:23.059560: step 1818, loss 0.780535, acc 0.546875
2016-12-13T14:00:27.407821: step 1819, loss 0.672672, acc 0.578125
2016-12-13T14:00:31.791059: step 1820, loss 0.655485, acc 0.65625
2016-12-13T14:00:36.077683: step 1821, loss 0.638752, acc 0.734375
2016-12-13T14:00:40.394742: step 1822, loss 0.659507, acc 0.5625
2016-12-13T14:00:44.694256: step 1823, loss 0.638959, acc 0.609375
2016-12-13T14:00:48.980772: step 1824, loss 0.720434, acc 0.515625
2016-12-13T14:00:53.305431: step 1825, loss 0.643424, acc 0.65625
2016-12-13T14:00:57.761049: step 1826, loss 0.683039, acc 0.625
2016-12-13T14:01:02.078282: step 1827, loss 0.702744, acc 0.609375
2016-12-13T14:01:06.424955: step 1828, loss 0.70399, acc 0.625
2016-12-13T14:01:10.771584: step 1829, loss 0.674211, acc 0.5625
2016-12-13T14:01:15.026013: step 1830, loss 0.604386, acc 0.65625
2016-12-13T14:01:19.257433: step 1831, loss 0.625863, acc 0.625
2016-12-13T14:01:23.537750: step 1832, loss 0.702355, acc 0.546875
2016-12-13T14:01:28.064134: step 1833, loss 0.642582, acc 0.578125
2016-12-13T14:01:32.345153: step 1834, loss 0.680543, acc 0.640625
2016-12-13T14:01:36.654618: step 1835, loss 0.666271, acc 0.625
2016-12-13T14:01:40.948535: step 1836, loss 0.696659, acc 0.53125
2016-12-13T14:01:45.258449: step 1837, loss 0.740729, acc 0.53125
2016-12-13T14:01:49.571332: step 1838, loss 0.712258, acc 0.53125
2016-12-13T14:01:53.839955: step 1839, loss 0.647527, acc 0.59375
2016-12-13T14:01:58.154696: step 1840, loss 0.759006, acc 0.46875
2016-12-13T14:02:02.517448: step 1841, loss 0.709228, acc 0.609375
2016-12-13T14:02:06.826717: step 1842, loss 0.658723, acc 0.625
2016-12-13T14:02:11.194732: step 1843, loss 0.682119, acc 0.640625
2016-12-13T14:02:15.452755: step 1844, loss 0.715666, acc 0.546875
2016-12-13T14:02:19.794667: step 1845, loss 0.697535, acc 0.65625
2016-12-13T14:02:24.078341: step 1846, loss 0.732835, acc 0.5625
2016-12-13T14:02:28.417894: step 1847, loss 0.653266, acc 0.609375
2016-12-13T14:02:32.949850: step 1848, loss 0.671653, acc 0.625
2016-12-13T14:02:37.224435: step 1849, loss 0.68764, acc 0.609375
2016-12-13T14:02:41.564629: step 1850, loss 0.687652, acc 0.59375
2016-12-13T14:02:45.870302: step 1851, loss 0.753986, acc 0.5625
2016-12-13T14:02:50.209736: step 1852, loss 0.673346, acc 0.578125
2016-12-13T14:02:54.500711: step 1853, loss 0.691377, acc 0.609375
2016-12-13T14:02:58.952841: step 1854, loss 0.678789, acc 0.546875
2016-12-13T14:03:03.592103: step 1855, loss 0.696673, acc 0.578125
2016-12-13T14:03:08.019131: step 1856, loss 0.677307, acc 0.578125
2016-12-13T14:03:12.436535: step 1857, loss 0.676665, acc 0.625
2016-12-13T14:03:16.854127: step 1858, loss 0.665728, acc 0.515625
2016-12-13T14:03:21.236632: step 1859, loss 0.66986, acc 0.609375
2016-12-13T14:03:25.594738: step 1860, loss 0.768055, acc 0.484375
2016-12-13T14:03:29.993165: step 1861, loss 0.667907, acc 0.578125
2016-12-13T14:03:34.443772: step 1862, loss 0.722643, acc 0.53125
2016-12-13T14:03:38.894672: step 1863, loss 0.690051, acc 0.609375
2016-12-13T14:03:43.288998: step 1864, loss 0.645307, acc 0.65625
2016-12-13T14:03:47.710526: step 1865, loss 0.666983, acc 0.5625
2016-12-13T14:03:52.067924: step 1866, loss 0.683112, acc 0.5625
2016-12-13T14:03:56.499577: step 1867, loss 0.669965, acc 0.578125
2016-12-13T14:04:00.906841: step 1868, loss 0.671488, acc 0.578125
2016-12-13T14:04:05.192118: step 1869, loss 0.80732, acc 0.5
2016-12-13T14:04:09.708516: step 1870, loss 0.682348, acc 0.609375
2016-12-13T14:04:14.046733: step 1871, loss 0.644842, acc 0.609375
2016-12-13T14:04:18.402316: step 1872, loss 0.702828, acc 0.515625
2016-12-13T14:04:22.669987: step 1873, loss 0.656366, acc 0.59375
2016-12-13T14:04:27.042055: step 1874, loss 0.693916, acc 0.5625
2016-12-13T14:04:31.386886: step 1875, loss 0.664145, acc 0.578125
2016-12-13T14:04:35.720988: step 1876, loss 0.688673, acc 0.515625
2016-12-13T14:04:40.193197: step 1877, loss 0.671513, acc 0.5625
2016-12-13T14:04:44.524425: step 1878, loss 0.710414, acc 0.484375
2016-12-13T14:04:48.829271: step 1879, loss 0.674725, acc 0.625
2016-12-13T14:04:53.158637: step 1880, loss 0.658129, acc 0.65625
2016-12-13T14:04:57.465624: step 1881, loss 0.56471, acc 0.75
2016-12-13T14:05:01.763175: step 1882, loss 0.605185, acc 0.65625
2016-12-13T14:05:06.095806: step 1883, loss 0.612457, acc 0.6875
2016-12-13T14:05:10.489070: step 1884, loss 0.70207, acc 0.515625
2016-12-13T14:05:15.028618: step 1885, loss 0.695508, acc 0.578125
2016-12-13T14:05:19.345817: step 1886, loss 0.59933, acc 0.609375
2016-12-13T14:05:23.746596: step 1887, loss 0.737171, acc 0.5
2016-12-13T14:05:28.099734: step 1888, loss 0.61079, acc 0.640625
2016-12-13T14:05:32.473432: step 1889, loss 0.658433, acc 0.609375
2016-12-13T14:05:36.756828: step 1890, loss 0.604707, acc 0.6875
2016-12-13T14:05:41.055196: step 1891, loss 0.702557, acc 0.53125
2016-12-13T14:05:45.565627: step 1892, loss 0.665819, acc 0.6875
2016-12-13T14:05:49.871665: step 1893, loss 0.644454, acc 0.5625
2016-12-13T14:05:54.166185: step 1894, loss 0.660595, acc 0.640625
2016-12-13T14:05:58.480565: step 1895, loss 0.702509, acc 0.53125
2016-12-13T14:06:02.737432: step 1896, loss 0.580598, acc 0.765625
2016-12-13T14:06:07.095422: step 1897, loss 0.636545, acc 0.671875
2016-12-13T14:06:11.428102: step 1898, loss 0.669734, acc 0.5625
2016-12-13T14:06:15.798327: step 1899, loss 0.613232, acc 0.6875
2016-12-13T14:06:20.127215: step 1900, loss 0.703572, acc 0.609375

Evaluation:
2016-12-13T14:07:55.166752: step 1900, loss 0.652897, acc 0.606987

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481637188/checkpoints/model-1900

2016-12-13T14:08:02.482381: step 1901, loss 0.618291, acc 0.65625
2016-12-13T14:08:07.067571: step 1902, loss 0.670348, acc 0.578125
2016-12-13T14:08:11.432078: step 1903, loss 0.697961, acc 0.578125
2016-12-13T14:08:15.748409: step 1904, loss 0.604172, acc 0.671875
2016-12-13T14:08:20.062096: step 1905, loss 0.669799, acc 0.609375
2016-12-13T14:08:24.349819: step 1906, loss 0.624988, acc 0.640625
2016-12-13T14:08:28.617902: step 1907, loss 0.684747, acc 0.546875
2016-12-13T14:08:32.895615: step 1908, loss 0.713235, acc 0.578125
2016-12-13T14:08:37.325884: step 1909, loss 0.665447, acc 0.609375
2016-12-13T14:08:41.888677: step 1910, loss 0.637359, acc 0.65625
2016-12-13T14:08:46.240675: step 1911, loss 0.669573, acc 0.625
2016-12-13T14:08:50.612892: step 1912, loss 0.644164, acc 0.625
2016-12-13T14:08:54.937133: step 1913, loss 0.644705, acc 0.65625
2016-12-13T14:08:59.273492: step 1914, loss 0.677354, acc 0.5625
2016-12-13T14:09:03.659068: step 1915, loss 0.599237, acc 0.6875
2016-12-13T14:09:08.000575: step 1916, loss 0.652627, acc 0.609375
2016-12-13T14:09:12.509319: step 1917, loss 0.659344, acc 0.59375
2016-12-13T14:09:16.835828: step 1918, loss 0.693018, acc 0.625
2016-12-13T14:09:21.132213: step 1919, loss 0.764953, acc 0.53125
2016-12-13T14:09:25.407418: step 1920, loss 0.681947, acc 0.59375
2016-12-13T14:09:29.723241: step 1921, loss 0.684942, acc 0.546875
2016-12-13T14:09:34.003298: step 1922, loss 0.650442, acc 0.640625
2016-12-13T14:09:38.296871: step 1923, loss 0.641535, acc 0.671875
2016-12-13T14:09:42.588616: step 1924, loss 0.693116, acc 0.5625
2016-12-13T14:09:47.121794: step 1925, loss 0.629805, acc 0.625
2016-12-13T14:09:51.461383: step 1926, loss 0.572914, acc 0.71875
2016-12-13T14:09:55.784578: step 1927, loss 0.625404, acc 0.65625
2016-12-13T14:10:00.100014: step 1928, loss 0.743459, acc 0.515625
2016-12-13T14:10:04.378962: step 1929, loss 0.740992, acc 0.53125
2016-12-13T14:10:08.739342: step 1930, loss 0.671358, acc 0.59375
2016-12-13T14:10:13.105314: step 1931, loss 0.675893, acc 0.515625
2016-12-13T14:10:17.621993: step 1932, loss 0.681257, acc 0.53125
2016-12-13T14:10:21.943222: step 1933, loss 0.738542, acc 0.453125
2016-12-13T14:10:26.279291: step 1934, loss 0.661726, acc 0.546875
2016-12-13T14:10:30.537901: step 1935, loss 0.662607, acc 0.65625
2016-12-13T14:10:34.874953: step 1936, loss 0.696217, acc 0.5625
2016-12-13T14:10:39.287978: step 1937, loss 0.709818, acc 0.640625
2016-12-13T14:10:43.649107: step 1938, loss 0.744108, acc 0.5625
2016-12-13T14:10:48.102624: step 1939, loss 0.701582, acc 0.578125
2016-12-13T14:10:52.533172: step 1940, loss 0.69673, acc 0.5625
2016-12-13T14:10:56.898366: step 1941, loss 0.675377, acc 0.53125
2016-12-13T14:11:01.191964: step 1942, loss 0.662061, acc 0.609375
2016-12-13T14:11:05.466501: step 1943, loss 0.694959, acc 0.5
2016-12-13T14:11:09.842579: step 1944, loss 0.690747, acc 0.578125
2016-12-13T14:11:14.076767: step 1945, loss 0.686924, acc 0.578125
2016-12-13T14:11:18.387356: step 1946, loss 0.608123, acc 0.703125
2016-12-13T14:11:23.246991: step 1947, loss 0.645019, acc 0.640625
2016-12-13T14:11:27.714404: step 1948, loss 0.743836, acc 0.515625
2016-12-13T14:11:32.080966: step 1949, loss 0.865685, acc 0.4375
2016-12-13T14:11:36.426372: step 1950, loss 0.72394, acc 0.59375
2016-12-13T14:11:40.786679: step 1951, loss 0.732001, acc 0.609375
2016-12-13T14:11:45.076487: step 1952, loss 0.721132, acc 0.5625
2016-12-13T14:11:49.480216: step 1953, loss 0.654973, acc 0.6875
2016-12-13T14:11:53.944203: step 1954, loss 0.694496, acc 0.578125
2016-12-13T14:11:58.411813: step 1955, loss 0.684089, acc 0.515625
2016-12-13T14:12:02.967220: step 1956, loss 0.704689, acc 0.515625
2016-12-13T14:12:07.284082: step 1957, loss 0.673321, acc 0.5625
2016-12-13T14:12:11.654979: step 1958, loss 0.69764, acc 0.59375
2016-12-13T14:12:15.975181: step 1959, loss 0.694277, acc 0.59375
2016-12-13T14:12:20.308523: step 1960, loss 0.756036, acc 0.53125
2016-12-13T14:12:24.677746: step 1961, loss 0.604809, acc 0.71875
2016-12-13T14:12:29.207624: step 1962, loss 0.656146, acc 0.65625
2016-12-13T14:12:33.462809: step 1963, loss 0.602691, acc 0.703125
2016-12-13T14:12:37.855671: step 1964, loss 0.678459, acc 0.671875
2016-12-13T14:12:42.236931: step 1965, loss 0.70529, acc 0.609375
2016-12-13T14:12:46.561510: step 1966, loss 0.634765, acc 0.609375
2016-12-13T14:12:50.946666: step 1967, loss 0.664766, acc 0.625
2016-12-13T14:12:55.335801: step 1968, loss 0.636844, acc 0.625
2016-12-13T14:12:59.939643: step 1969, loss 0.72099, acc 0.625
2016-12-13T14:13:04.352691: step 1970, loss 0.704426, acc 0.53125
2016-12-13T14:13:08.739742: step 1971, loss 0.654281, acc 0.625
2016-12-13T14:13:13.119215: step 1972, loss 0.651798, acc 0.5625
2016-12-13T14:13:17.395776: step 1973, loss 0.646554, acc 0.5625
2016-12-13T14:13:21.661407: step 1974, loss 0.678757, acc 0.546875
2016-12-13T14:13:25.975762: step 1975, loss 0.666903, acc 0.640625
2016-12-13T14:13:30.417062: step 1976, loss 0.617609, acc 0.640625
2016-12-13T14:13:34.841421: step 1977, loss 0.664888, acc 0.609375
2016-12-13T14:13:39.102713: step 1978, loss 0.690552, acc 0.625
2016-12-13T14:13:43.417673: step 1979, loss 0.676548, acc 0.59375
2016-12-13T14:13:47.713128: step 1980, loss 0.628009, acc 0.671875
2016-12-13T14:13:52.053113: step 1981, loss 0.710665, acc 0.5625
2016-12-13T14:13:56.339859: step 1982, loss 0.684421, acc 0.609375
2016-12-13T14:14:00.615320: step 1983, loss 0.590734, acc 0.65625
2016-12-13T14:14:05.111470: step 1984, loss 0.633684, acc 0.609375
2016-12-13T14:14:09.460283: step 1985, loss 0.71677, acc 0.515625
2016-12-13T14:14:13.743570: step 1986, loss 0.667735, acc 0.578125
2016-12-13T14:14:18.094953: step 1987, loss 0.620428, acc 0.609375
2016-12-13T14:14:22.448116: step 1988, loss 0.672228, acc 0.578125
2016-12-13T14:14:26.730016: step 1989, loss 0.637551, acc 0.671875
2016-12-13T14:14:31.072328: step 1990, loss 0.799411, acc 0.484375
2016-12-13T14:14:35.537578: step 1991, loss 0.652594, acc 0.609375
2016-12-13T14:14:39.869833: step 1992, loss 0.637401, acc 0.609375
2016-12-13T14:14:44.194947: step 1993, loss 0.633934, acc 0.65625
2016-12-13T14:14:48.326977: step 1994, loss 0.704269, acc 0.578125
2016-12-13T14:14:52.694568: step 1995, loss 0.626588, acc 0.671875
2016-12-13T14:14:57.019909: step 1996, loss 0.604224, acc 0.671875
2016-12-13T14:15:01.278431: step 1997, loss 0.624807, acc 0.625
2016-12-13T14:15:05.627426: step 1998, loss 0.672927, acc 0.609375
2016-12-13T14:15:10.071884: step 1999, loss 0.724866, acc 0.578125
2016-12-13T14:15:14.411505: step 2000, loss 0.625383, acc 0.65625

Evaluation:
2016-12-13T14:16:54.606928: step 2000, loss 0.65384, acc 0.612082

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481637188/checkpoints/model-2000

2016-12-13T14:17:02.093934: step 2001, loss 0.732704, acc 0.578125
2016-12-13T14:17:06.466659: step 2002, loss 0.503528, acc 0.765625
2016-12-13T14:17:10.892882: step 2003, loss 0.617203, acc 0.625
2016-12-13T14:17:15.299949: step 2004, loss 0.621614, acc 0.625
2016-12-13T14:17:19.787830: step 2005, loss 0.687335, acc 0.59375
2016-12-13T14:17:24.203833: step 2006, loss 0.711186, acc 0.625
2016-12-13T14:17:28.795916: step 2007, loss 0.584688, acc 0.65625
2016-12-13T14:17:33.502904: step 2008, loss 0.611037, acc 0.6875
2016-12-13T14:17:37.959467: step 2009, loss 0.607764, acc 0.609375
2016-12-13T14:17:42.444464: step 2010, loss 0.717807, acc 0.53125
2016-12-13T14:17:46.730756: step 2011, loss 0.643404, acc 0.59375
2016-12-13T14:17:51.106659: step 2012, loss 0.615226, acc 0.671875
2016-12-13T14:17:55.445133: step 2013, loss 0.673835, acc 0.609375
2016-12-13T14:18:00.168101: step 2014, loss 0.599634, acc 0.75
2016-12-13T14:18:04.539411: step 2015, loss 0.612016, acc 0.59375
2016-12-13T14:18:09.115471: step 2016, loss 0.693645, acc 0.5
2016-12-13T14:18:13.503853: step 2017, loss 0.724488, acc 0.59375
2016-12-13T14:18:17.829809: step 2018, loss 0.602837, acc 0.71875
2016-12-13T14:18:22.146304: step 2019, loss 0.611691, acc 0.640625
2016-12-13T14:18:26.515314: step 2020, loss 0.586745, acc 0.71875
2016-12-13T14:18:30.769817: step 2021, loss 0.80714, acc 0.4375
2016-12-13T14:18:35.242408: step 2022, loss 0.731906, acc 0.484375
2016-12-13T14:18:39.580727: step 2023, loss 0.612862, acc 0.640625
2016-12-13T14:18:43.890271: step 2024, loss 0.661499, acc 0.546875
2016-12-13T14:18:48.242084: step 2025, loss 0.688422, acc 0.578125
2016-12-13T14:18:52.550830: step 2026, loss 0.648758, acc 0.625
2016-12-13T14:18:56.912548: step 2027, loss 0.63238, acc 0.609375
2016-12-13T14:19:01.301386: step 2028, loss 0.652457, acc 0.578125
2016-12-13T14:19:05.896066: step 2029, loss 0.711272, acc 0.546875
2016-12-13T14:19:10.208637: step 2030, loss 0.679423, acc 0.609375
2016-12-13T14:19:14.626732: step 2031, loss 0.67787, acc 0.546875
2016-12-13T14:19:18.893961: step 2032, loss 0.658412, acc 0.640625
2016-12-13T14:19:23.154896: step 2033, loss 0.715715, acc 0.546875
2016-12-13T14:19:27.477532: step 2034, loss 0.727108, acc 0.421875
2016-12-13T14:19:31.777043: step 2035, loss 0.653999, acc 0.546875
2016-12-13T14:19:36.119334: step 2036, loss 0.627793, acc 0.609375
2016-12-13T14:19:40.531824: step 2037, loss 0.677408, acc 0.578125
2016-12-13T14:19:44.843090: step 2038, loss 0.659771, acc 0.578125
2016-12-13T14:19:49.197404: step 2039, loss 0.665392, acc 0.609375
2016-12-13T14:19:53.504008: step 2040, loss 0.637153, acc 0.625
2016-12-13T14:19:57.787419: step 2041, loss 0.615907, acc 0.640625
2016-12-13T14:20:02.056131: step 2042, loss 0.641933, acc 0.6875
2016-12-13T14:20:06.385010: step 2043, loss 0.707384, acc 0.5
2016-12-13T14:20:11.057896: step 2044, loss 0.692977, acc 0.546875
2016-12-13T14:20:15.342277: step 2045, loss 0.622033, acc 0.59375
2016-12-13T14:20:19.772065: step 2046, loss 0.624465, acc 0.640625
2016-12-13T14:20:24.042720: step 2047, loss 0.685295, acc 0.578125
2016-12-13T14:20:28.350584: step 2048, loss 0.657602, acc 0.609375
2016-12-13T14:20:32.587016: step 2049, loss 0.647371, acc 0.640625
2016-12-13T14:20:36.916859: step 2050, loss 0.713332, acc 0.546875
2016-12-13T14:20:41.365620: step 2051, loss 0.680922, acc 0.640625
2016-12-13T14:20:45.699018: step 2052, loss 0.580894, acc 0.6875
2016-12-13T14:20:50.010661: step 2053, loss 0.656479, acc 0.578125
2016-12-13T14:20:54.312666: step 2054, loss 0.655153, acc 0.59375
2016-12-13T14:20:58.615963: step 2055, loss 0.648706, acc 0.59375
2016-12-13T14:21:02.931378: step 2056, loss 0.66882, acc 0.5625
2016-12-13T14:21:07.235194: step 2057, loss 0.686215, acc 0.578125
2016-12-13T14:21:11.875460: step 2058, loss 0.628123, acc 0.6875
2016-12-13T14:21:16.478190: step 2059, loss 0.653784, acc 0.640625
2016-12-13T14:21:20.760768: step 2060, loss 0.640104, acc 0.6875
2016-12-13T14:21:25.256546: step 2061, loss 0.766973, acc 0.53125
2016-12-13T14:21:29.563786: step 2062, loss 0.725647, acc 0.59375
2016-12-13T14:21:33.909068: step 2063, loss 0.708648, acc 0.515625
2016-12-13T14:21:38.155299: step 2064, loss 0.705997, acc 0.609375
2016-12-13T14:21:42.419984: step 2065, loss 0.695569, acc 0.625
2016-12-13T14:21:46.908138: step 2066, loss 0.698826, acc 0.515625
2016-12-13T14:21:51.215934: step 2067, loss 0.675119, acc 0.578125
2016-12-13T14:21:55.447214: step 2068, loss 0.660962, acc 0.609375
2016-12-13T14:21:59.731425: step 2069, loss 0.65625, acc 0.609375
2016-12-13T14:22:04.239533: step 2070, loss 0.654025, acc 0.609375
2016-12-13T14:22:08.651726: step 2071, loss 0.673833, acc 0.546875
2016-12-13T14:22:13.061860: step 2072, loss 0.697701, acc 0.59375
2016-12-13T14:22:17.498752: step 2073, loss 0.692196, acc 0.5625
2016-12-13T14:22:21.944479: step 2074, loss 0.733532, acc 0.546875
2016-12-13T14:22:26.395109: step 2075, loss 0.636027, acc 0.640625
2016-12-13T14:22:30.731728: step 2076, loss 0.669979, acc 0.640625
2016-12-13T14:22:35.066237: step 2077, loss 0.625357, acc 0.71875
2016-12-13T14:22:39.412085: step 2078, loss 0.64707, acc 0.59375
2016-12-13T14:22:43.683185: step 2079, loss 0.680382, acc 0.625
2016-12-13T14:22:47.966056: step 2080, loss 0.553854, acc 0.765625
2016-12-13T14:22:52.420939: step 2081, loss 0.639762, acc 0.71875
2016-12-13T14:22:56.681370: step 2082, loss 0.628729, acc 0.703125
2016-12-13T14:23:00.982992: step 2083, loss 0.566169, acc 0.703125
2016-12-13T14:23:05.319276: step 2084, loss 0.598996, acc 0.671875
2016-12-13T14:23:09.572999: step 2085, loss 0.783627, acc 0.546875
2016-12-13T14:23:13.923046: step 2086, loss 0.707763, acc 0.578125
2016-12-13T14:23:18.224954: step 2087, loss 0.780038, acc 0.578125
2016-12-13T14:23:22.589505: step 2088, loss 0.672967, acc 0.578125
2016-12-13T14:23:26.896269: step 2089, loss 0.706214, acc 0.578125
2016-12-13T14:23:31.184490: step 2090, loss 0.683012, acc 0.578125
2016-12-13T14:23:35.526451: step 2091, loss 0.778018, acc 0.4375
2016-12-13T14:23:39.818036: step 2092, loss 0.670874, acc 0.578125
2016-12-13T14:23:44.103599: step 2093, loss 0.656367, acc 0.609375
2016-12-13T14:23:48.342102: step 2094, loss 0.682212, acc 0.578125
2016-12-13T14:23:52.672553: step 2095, loss 0.613586, acc 0.65625
2016-12-13T14:23:57.180455: step 2096, loss 0.80075, acc 0.484375
2016-12-13T14:24:01.493279: step 2097, loss 0.633494, acc 0.640625
2016-12-13T14:24:05.720646: step 2098, loss 0.6072, acc 0.65625
2016-12-13T14:24:10.028909: step 2099, loss 0.610026, acc 0.734375
2016-12-13T14:24:14.417562: step 2100, loss 0.626056, acc 0.75

Evaluation:
2016-12-13T14:25:48.711178: step 2100, loss 0.652699, acc 0.611354

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481637188/checkpoints/model-2100

2016-12-13T14:25:56.676472: step 2101, loss 0.691851, acc 0.609375
2016-12-13T14:26:01.074376: step 2102, loss 0.694085, acc 0.53125
2016-12-13T14:26:05.508192: step 2103, loss 0.672192, acc 0.546875
2016-12-13T14:26:10.026176: step 2104, loss 0.737033, acc 0.515625
2016-12-13T14:26:14.444078: step 2105, loss 0.708083, acc 0.53125
2016-12-13T14:26:18.816702: step 2106, loss 0.644835, acc 0.578125
2016-12-13T14:26:23.258027: step 2107, loss 0.694156, acc 0.46875
2016-12-13T14:26:27.629353: step 2108, loss 0.637299, acc 0.625
2016-12-13T14:26:31.922414: step 2109, loss 0.674824, acc 0.640625
2016-12-13T14:26:36.221790: step 2110, loss 0.700119, acc 0.53125
2016-12-13T14:26:40.554109: step 2111, loss 0.645303, acc 0.5625
2016-12-13T14:26:44.891811: step 2112, loss 0.63738, acc 0.75
2016-12-13T14:26:49.288455: step 2113, loss 0.68101, acc 0.609375
2016-12-13T14:26:53.834164: step 2114, loss 0.679296, acc 0.625
2016-12-13T14:26:58.178000: step 2115, loss 0.634201, acc 0.671875
2016-12-13T14:27:02.555440: step 2116, loss 0.595794, acc 0.671875
2016-12-13T14:27:07.004623: step 2117, loss 0.677996, acc 0.5625
2016-12-13T14:27:11.365413: step 2118, loss 0.650041, acc 0.625
2016-12-13T14:27:15.718111: step 2119, loss 0.675175, acc 0.578125
2016-12-13T14:27:20.088691: step 2120, loss 0.658486, acc 0.640625
2016-12-13T14:27:24.669726: step 2121, loss 0.681244, acc 0.5
2016-12-13T14:27:29.210873: step 2122, loss 0.649598, acc 0.546875
2016-12-13T14:27:33.711636: step 2123, loss 0.707726, acc 0.578125
2016-12-13T14:27:37.968942: step 2124, loss 0.696034, acc 0.546875
2016-12-13T14:27:42.310691: step 2125, loss 0.666313, acc 0.578125
2016-12-13T14:27:46.652288: step 2126, loss 0.740207, acc 0.5
2016-12-13T14:27:51.000390: step 2127, loss 0.624765, acc 0.671875
2016-12-13T14:27:55.334671: step 2128, loss 0.629044, acc 0.59375
2016-12-13T14:27:59.721766: step 2129, loss 0.725246, acc 0.5
2016-12-13T14:28:03.986086: step 2130, loss 0.634349, acc 0.609375
2016-12-13T14:28:08.364375: step 2131, loss 0.75835, acc 0.53125
2016-12-13T14:28:12.673076: step 2132, loss 0.645339, acc 0.59375
2016-12-13T14:28:17.004850: step 2133, loss 0.738678, acc 0.5
2016-12-13T14:28:21.357353: step 2134, loss 0.701173, acc 0.578125
2016-12-13T14:28:25.774406: step 2135, loss 0.66461, acc 0.5625
2016-12-13T14:28:30.386865: step 2136, loss 0.692802, acc 0.546875
2016-12-13T14:28:34.794115: step 2137, loss 0.721568, acc 0.53125
2016-12-13T14:28:39.164905: step 2138, loss 0.698199, acc 0.484375
2016-12-13T14:28:43.510815: step 2139, loss 0.669728, acc 0.59375
2016-12-13T14:28:47.766120: step 2140, loss 0.674475, acc 0.578125
2016-12-13T14:28:52.050726: step 2141, loss 0.728336, acc 0.5625
2016-12-13T14:28:56.424418: step 2142, loss 0.596322, acc 0.65625
2016-12-13T14:29:00.839095: step 2143, loss 0.775961, acc 0.515625
2016-12-13T14:29:05.263365: step 2144, loss 0.574778, acc 0.75
2016-12-13T14:29:09.565567: step 2145, loss 0.647417, acc 0.609375
2016-12-13T14:29:13.873558: step 2146, loss 0.646383, acc 0.59375
2016-12-13T14:29:18.149238: step 2147, loss 0.654361, acc 0.640625
2016-12-13T14:29:22.419257: step 2148, loss 0.6992, acc 0.609375
2016-12-13T14:29:26.715684: step 2149, loss 0.692787, acc 0.546875
2016-12-13T14:29:31.050252: step 2150, loss 0.713216, acc 0.484375
2016-12-13T14:29:35.517137: step 2151, loss 0.653023, acc 0.640625
2016-12-13T14:29:39.832198: step 2152, loss 0.617309, acc 0.65625
2016-12-13T14:29:44.129039: step 2153, loss 0.700712, acc 0.625
2016-12-13T14:29:48.478933: step 2154, loss 0.637705, acc 0.625
2016-12-13T14:29:52.724256: step 2155, loss 0.546439, acc 0.734375
2016-12-13T14:29:57.084051: step 2156, loss 0.76571, acc 0.53125
2016-12-13T14:30:01.395985: step 2157, loss 0.681064, acc 0.625
2016-12-13T14:30:05.831878: step 2158, loss 0.657748, acc 0.640625
2016-12-13T14:30:10.169377: step 2159, loss 0.607244, acc 0.6875
2016-12-13T14:30:14.502844: step 2160, loss 0.699507, acc 0.625
2016-12-13T14:30:18.907394: step 2161, loss 0.611909, acc 0.671875
2016-12-13T14:30:23.306934: step 2162, loss 0.639182, acc 0.65625
2016-12-13T14:30:27.618751: step 2163, loss 0.68558, acc 0.59375
2016-12-13T14:30:31.890230: step 2164, loss 0.690583, acc 0.59375
2016-12-13T14:30:36.187527: step 2165, loss 0.730017, acc 0.453125
2016-12-13T14:30:40.685220: step 2166, loss 0.601938, acc 0.671875
2016-12-13T14:30:44.979459: step 2167, loss 0.596093, acc 0.71875
2016-12-13T14:30:49.577014: step 2168, loss 0.660039, acc 0.640625
2016-12-13T14:30:53.982689: step 2169, loss 0.657647, acc 0.59375
2016-12-13T14:30:58.288863: step 2170, loss 0.663647, acc 0.625
2016-12-13T14:31:02.554166: step 2171, loss 0.701463, acc 0.515625
2016-12-13T14:31:06.886607: step 2172, loss 0.665051, acc 0.640625
2016-12-13T14:31:11.391449: step 2173, loss 0.738196, acc 0.484375
2016-12-13T14:31:15.728635: step 2174, loss 0.628159, acc 0.609375
2016-12-13T14:31:19.985876: step 2175, loss 0.720394, acc 0.5
2016-12-13T14:31:24.334938: step 2176, loss 0.703931, acc 0.546875
2016-12-13T14:31:28.600219: step 2177, loss 0.658284, acc 0.59375
2016-12-13T14:31:32.891143: step 2178, loss 0.696145, acc 0.625
2016-12-13T14:31:37.239170: step 2179, loss 0.629592, acc 0.65625
2016-12-13T14:31:41.589636: step 2180, loss 0.786877, acc 0.453125
2016-12-13T14:31:45.962392: step 2181, loss 0.643536, acc 0.671875
2016-12-13T14:31:50.266332: step 2182, loss 0.644092, acc 0.59375
2016-12-13T14:31:54.649058: step 2183, loss 0.69085, acc 0.484375
2016-12-13T14:31:58.987909: step 2184, loss 0.678387, acc 0.578125
2016-12-13T14:32:03.452950: step 2185, loss 0.731141, acc 0.5
2016-12-13T14:32:07.838698: step 2186, loss 0.637631, acc 0.640625
2016-12-13T14:32:12.248962: step 2187, loss 0.637794, acc 0.609375
2016-12-13T14:32:16.797330: step 2188, loss 0.678568, acc 0.59375
2016-12-13T14:32:21.234524: step 2189, loss 0.652866, acc 0.65625
2016-12-13T14:32:25.667471: step 2190, loss 0.745144, acc 0.515625
2016-12-13T14:32:30.017531: step 2191, loss 0.602382, acc 0.640625
2016-12-13T14:32:34.356787: step 2192, loss 0.668643, acc 0.59375
2016-12-13T14:32:38.640719: step 2193, loss 0.723046, acc 0.515625
2016-12-13T14:32:42.980247: step 2194, loss 0.694465, acc 0.546875
2016-12-13T14:32:47.519043: step 2195, loss 0.620409, acc 0.6875
2016-12-13T14:32:51.825257: step 2196, loss 0.588356, acc 0.640625
2016-12-13T14:32:56.127289: step 2197, loss 0.728604, acc 0.5625
2016-12-13T14:33:00.463452: step 2198, loss 0.725726, acc 0.578125
2016-12-13T14:33:04.743307: step 2199, loss 0.632229, acc 0.625
2016-12-13T14:33:09.027889: step 2200, loss 0.618587, acc 0.625

Evaluation:
2016-12-13T14:34:52.139832: step 2200, loss 0.651171, acc 0.610626

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481637188/checkpoints/model-2200

2016-12-13T14:35:00.044854: step 2201, loss 0.57286, acc 0.6875
2016-12-13T14:35:04.667340: step 2202, loss 0.687069, acc 0.625
2016-12-13T14:35:09.176955: step 2203, loss 0.617706, acc 0.65625
2016-12-13T14:35:14.014056: step 2204, loss 0.714988, acc 0.59375
2016-12-13T14:35:18.520267: step 2205, loss 0.722043, acc 0.53125
2016-12-13T14:35:23.200831: step 2206, loss 0.648711, acc 0.625
2016-12-13T14:35:27.622055: step 2207, loss 0.637131, acc 0.6875
2016-12-13T14:35:31.995461: step 2208, loss 0.638117, acc 0.59375
2016-12-13T14:35:36.721439: step 2209, loss 0.707533, acc 0.609375
2016-12-13T14:35:41.275962: step 2210, loss 0.720365, acc 0.578125
2016-12-13T14:35:45.857716: step 2211, loss 0.649297, acc 0.65625
2016-12-13T14:35:50.296070: step 2212, loss 0.81994, acc 0.515625
2016-12-13T14:35:54.855235: step 2213, loss 0.648301, acc 0.5
2016-12-13T14:35:59.305784: step 2214, loss 0.693767, acc 0.5625
2016-12-13T14:36:03.805669: step 2215, loss 0.634516, acc 0.640625
2016-12-13T14:36:08.613996: step 2216, loss 0.646504, acc 0.578125
2016-12-13T14:36:13.477049: step 2217, loss 0.715792, acc 0.53125
2016-12-13T14:36:18.281995: step 2218, loss 0.72043, acc 0.5
2016-12-13T14:36:22.690462: step 2219, loss 0.682081, acc 0.578125
2016-12-13T14:36:27.151605: step 2220, loss 0.641559, acc 0.671875
2016-12-13T14:36:31.614028: step 2221, loss 0.657505, acc 0.65625
2016-12-13T14:36:36.029471: step 2222, loss 0.76965, acc 0.59375
2016-12-13T14:36:40.493165: step 2223, loss 0.749097, acc 0.53125
2016-12-13T14:36:44.948726: step 2224, loss 0.561338, acc 0.75
2016-12-13T14:36:49.629172: step 2225, loss 0.690671, acc 0.5625
2016-12-13T14:36:54.149328: step 2226, loss 0.659527, acc 0.625
2016-12-13T14:36:58.593515: step 2227, loss 0.650426, acc 0.625
2016-12-13T14:37:03.118481: step 2228, loss 0.683302, acc 0.53125
2016-12-13T14:37:07.658956: step 2229, loss 0.669616, acc 0.546875
2016-12-13T14:37:12.156221: step 2230, loss 0.66472, acc 0.59375
2016-12-13T14:37:16.672355: step 2231, loss 0.696152, acc 0.484375
2016-12-13T14:37:21.257021: step 2232, loss 0.602072, acc 0.6875
2016-12-13T14:37:25.735966: step 2233, loss 0.648589, acc 0.640625
2016-12-13T14:37:30.146183: step 2234, loss 0.740698, acc 0.546875
2016-12-13T14:37:34.624982: step 2235, loss 0.627652, acc 0.671875
2016-12-13T14:37:39.091478: step 2236, loss 0.742964, acc 0.59375
2016-12-13T14:37:43.532426: step 2237, loss 0.662851, acc 0.578125
2016-12-13T14:37:47.963372: step 2238, loss 0.670454, acc 0.625
2016-12-13T14:37:52.369508: step 2239, loss 0.708954, acc 0.5625
2016-12-13T14:37:56.838026: step 2240, loss 0.709011, acc 0.484375
2016-12-13T14:38:01.260559: step 2241, loss 0.687544, acc 0.5625
2016-12-13T14:38:05.582596: step 2242, loss 0.745936, acc 0.515625
2016-12-13T14:38:10.042937: step 2243, loss 0.665147, acc 0.59375
2016-12-13T14:38:14.542105: step 2244, loss 0.689999, acc 0.625
2016-12-13T14:38:18.938460: step 2245, loss 0.714842, acc 0.59375
2016-12-13T14:38:23.317474: step 2246, loss 0.61463, acc 0.640625
2016-12-13T14:38:27.849895: step 2247, loss 0.778053, acc 0.578125
2016-12-13T14:38:32.242845: step 2248, loss 0.580135, acc 0.671875
2016-12-13T14:38:36.619430: step 2249, loss 0.637992, acc 0.671875
2016-12-13T14:38:41.029501: step 2250, loss 0.5836, acc 0.71875
2016-12-13T14:38:45.422492: step 2251, loss 0.712643, acc 0.609375
2016-12-13T14:38:49.976019: step 2252, loss 0.678425, acc 0.578125
2016-12-13T14:38:54.345919: step 2253, loss 0.682541, acc 0.578125
2016-12-13T14:38:58.907086: step 2254, loss 0.660338, acc 0.5625
2016-12-13T14:39:03.378518: step 2255, loss 0.680558, acc 0.53125
2016-12-13T14:39:07.779205: step 2256, loss 0.657113, acc 0.59375
2016-12-13T14:39:12.244972: step 2257, loss 0.650017, acc 0.65625
2016-12-13T14:39:16.675309: step 2258, loss 0.684392, acc 0.609375
2016-12-13T14:39:21.000952: step 2259, loss 0.646306, acc 0.640625
2016-12-13T14:39:25.344884: step 2260, loss 0.610024, acc 0.6875
2016-12-13T14:39:29.878983: step 2261, loss 0.653367, acc 0.625
2016-12-13T14:39:34.354930: step 2262, loss 0.597556, acc 0.71875
2016-12-13T14:39:38.695548: step 2263, loss 0.643971, acc 0.640625
2016-12-13T14:39:43.068219: step 2264, loss 0.735363, acc 0.578125
2016-12-13T14:39:47.509510: step 2265, loss 0.630078, acc 0.609375
2016-12-13T14:39:51.868558: step 2266, loss 0.745559, acc 0.609375
2016-12-13T14:39:56.265392: step 2267, loss 0.733259, acc 0.5625
2016-12-13T14:40:00.722476: step 2268, loss 0.652064, acc 0.625
2016-12-13T14:40:05.317619: step 2269, loss 0.703273, acc 0.5625
2016-12-13T14:40:10.533711: step 2270, loss 0.640402, acc 0.578125
2016-12-13T14:40:15.029576: step 2271, loss 0.717524, acc 0.4375
2016-12-13T14:40:19.719098: step 2272, loss 0.673463, acc 0.625
2016-12-13T14:40:24.077895: step 2273, loss 0.625981, acc 0.640625
2016-12-13T14:40:28.490281: step 2274, loss 0.62832, acc 0.625
2016-12-13T14:40:32.902197: step 2275, loss 0.685259, acc 0.59375
2016-12-13T14:40:37.460817: step 2276, loss 0.684941, acc 0.59375
2016-12-13T14:40:41.884597: step 2277, loss 0.758825, acc 0.578125
2016-12-13T14:40:46.305528: step 2278, loss 0.715926, acc 0.640625
2016-12-13T14:40:50.759539: step 2279, loss 0.603229, acc 0.71875
2016-12-13T14:40:55.128619: step 2280, loss 0.695431, acc 0.578125
2016-12-13T14:40:59.507238: step 2281, loss 0.697997, acc 0.515625
2016-12-13T14:41:03.886836: step 2282, loss 0.662571, acc 0.59375
2016-12-13T14:41:08.521554: step 2283, loss 0.5953, acc 0.734375
2016-12-13T14:41:12.844562: step 2284, loss 0.696551, acc 0.53125
2016-12-13T14:41:17.275589: step 2285, loss 0.641784, acc 0.640625
2016-12-13T14:41:21.671919: step 2286, loss 0.659807, acc 0.609375
2016-12-13T14:41:26.064531: step 2287, loss 0.696699, acc 0.5625
2016-12-13T14:41:30.417203: step 2288, loss 0.681962, acc 0.5625
2016-12-13T14:41:34.839835: step 2289, loss 0.730867, acc 0.5625
2016-12-13T14:41:39.569499: step 2290, loss 0.680248, acc 0.65625
2016-12-13T14:41:44.017246: step 2291, loss 0.713415, acc 0.578125
2016-12-13T14:41:48.507419: step 2292, loss 0.636993, acc 0.640625
2016-12-13T14:41:52.973984: step 2293, loss 0.66934, acc 0.640625
2016-12-13T14:41:57.378276: step 2294, loss 0.719962, acc 0.546875
2016-12-13T14:42:01.964441: step 2295, loss 0.636435, acc 0.6875
2016-12-13T14:42:06.526129: step 2296, loss 0.612176, acc 0.65625
2016-12-13T14:42:11.188973: step 2297, loss 0.651676, acc 0.609375
2016-12-13T14:42:15.724278: step 2298, loss 0.658446, acc 0.5625
2016-12-13T14:42:20.144496: step 2299, loss 0.730213, acc 0.453125
2016-12-13T14:42:24.726991: step 2300, loss 0.635118, acc 0.640625

Evaluation:
2016-12-13T14:43:49.267204: step 2300, loss 0.647846, acc 0.620087

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481637188/checkpoints/model-2300

2016-12-13T14:43:57.150451: step 2301, loss 0.703299, acc 0.484375
2016-12-13T14:44:01.738332: step 2302, loss 0.661758, acc 0.546875
2016-12-13T14:44:06.400144: step 2303, loss 0.606425, acc 0.6875
2016-12-13T14:44:10.968336: step 2304, loss 0.605314, acc 0.75
2016-12-13T14:44:15.581381: step 2305, loss 0.675715, acc 0.640625
2016-12-13T14:44:20.017209: step 2306, loss 0.636268, acc 0.625
2016-12-13T14:44:24.433766: step 2307, loss 0.836607, acc 0.546875
2016-12-13T14:44:28.887476: step 2308, loss 0.715449, acc 0.546875
2016-12-13T14:44:33.495121: step 2309, loss 0.615811, acc 0.703125
2016-12-13T14:44:37.926114: step 2310, loss 0.661291, acc 0.609375
2016-12-13T14:44:42.387132: step 2311, loss 0.7051, acc 0.53125
2016-12-13T14:44:46.944749: step 2312, loss 0.637233, acc 0.625
2016-12-13T14:44:51.456477: step 2313, loss 0.6437, acc 0.65625
2016-12-13T14:44:55.907520: step 2314, loss 0.670914, acc 0.578125
2016-12-13T14:45:00.372310: step 2315, loss 0.661888, acc 0.5625
2016-12-13T14:45:04.964192: step 2316, loss 0.83983, acc 0.390625
2016-12-13T14:45:09.446514: step 2317, loss 0.822378, acc 0.421875
2016-12-13T14:45:13.948162: step 2318, loss 0.638539, acc 0.703125
2016-12-13T14:45:18.373464: step 2319, loss 0.666306, acc 0.65625
2016-12-13T14:45:22.807589: step 2320, loss 0.651904, acc 0.578125
2016-12-13T14:45:27.325238: step 2321, loss 0.658115, acc 0.640625
2016-12-13T14:45:31.847105: step 2322, loss 0.627248, acc 0.671875
2016-12-13T14:45:36.426045: step 2323, loss 0.623772, acc 0.609375
2016-12-13T14:45:40.896976: step 2324, loss 0.618741, acc 0.625
2016-12-13T14:45:45.193751: step 2325, loss 0.771642, acc 0.609375
2016-12-13T14:45:49.595182: step 2326, loss 0.581501, acc 0.71875
2016-12-13T14:45:54.030319: step 2327, loss 0.674306, acc 0.5625
2016-12-13T14:45:58.483313: step 2328, loss 0.671271, acc 0.546875
2016-12-13T14:46:02.918011: step 2329, loss 0.602134, acc 0.703125
2016-12-13T14:46:07.449174: step 2330, loss 0.693384, acc 0.578125
2016-12-13T14:46:11.811498: step 2331, loss 0.649245, acc 0.59375
2016-12-13T14:46:16.216218: step 2332, loss 0.651204, acc 0.625
2016-12-13T14:46:20.606027: step 2333, loss 0.670279, acc 0.625
2016-12-13T14:46:24.919934: step 2334, loss 0.667273, acc 0.625
2016-12-13T14:46:29.332369: step 2335, loss 0.710573, acc 0.59375
2016-12-13T14:46:33.788636: step 2336, loss 0.68179, acc 0.59375
2016-12-13T14:46:38.264319: step 2337, loss 0.629137, acc 0.625
2016-12-13T14:46:42.774318: step 2338, loss 0.622621, acc 0.671875
2016-12-13T14:46:47.368004: step 2339, loss 0.648413, acc 0.625
2016-12-13T14:46:51.757080: step 2340, loss 0.671768, acc 0.640625
2016-12-13T14:46:56.291857: step 2341, loss 0.652508, acc 0.6875
2016-12-13T14:47:00.778574: step 2342, loss 0.67516, acc 0.609375
2016-12-13T14:47:05.417699: step 2343, loss 0.643347, acc 0.640625
2016-12-13T14:47:09.950367: step 2344, loss 0.61841, acc 0.671875
2016-12-13T14:47:14.741694: step 2345, loss 0.71139, acc 0.5625
2016-12-13T14:47:19.362520: step 2346, loss 0.743496, acc 0.484375
2016-12-13T14:47:23.928276: step 2347, loss 0.618256, acc 0.640625
2016-12-13T14:47:28.381813: step 2348, loss 0.652133, acc 0.609375
2016-12-13T14:47:32.812132: step 2349, loss 0.650309, acc 0.59375
2016-12-13T14:47:37.200544: step 2350, loss 0.612402, acc 0.703125
2016-12-13T14:47:41.688427: step 2351, loss 0.636161, acc 0.640625
2016-12-13T14:47:46.414893: step 2352, loss 0.595288, acc 0.734375
2016-12-13T14:47:51.010935: step 2353, loss 0.72373, acc 0.578125
2016-12-13T14:47:55.496980: step 2354, loss 0.66354, acc 0.640625
2016-12-13T14:47:59.971093: step 2355, loss 0.694749, acc 0.625
2016-12-13T14:48:04.469614: step 2356, loss 0.70065, acc 0.5625
2016-12-13T14:48:08.981745: step 2357, loss 0.637902, acc 0.625
2016-12-13T14:48:13.481823: step 2358, loss 0.641308, acc 0.609375
2016-12-13T14:48:18.307960: step 2359, loss 0.686601, acc 0.5625
2016-12-13T14:48:22.777502: step 2360, loss 0.603371, acc 0.671875
2016-12-13T14:48:27.302490: step 2361, loss 0.648434, acc 0.59375
2016-12-13T14:48:31.808250: step 2362, loss 0.653275, acc 0.609375
2016-12-13T14:48:36.307414: step 2363, loss 0.658592, acc 0.609375
2016-12-13T14:48:40.812228: step 2364, loss 0.66096, acc 0.609375
2016-12-13T14:48:45.364624: step 2365, loss 0.716343, acc 0.578125
2016-12-13T14:48:50.016365: step 2366, loss 0.662239, acc 0.578125
2016-12-13T14:48:54.429869: step 2367, loss 0.67867, acc 0.65625
2016-12-13T14:48:58.882710: step 2368, loss 0.674174, acc 0.609375
2016-12-13T14:49:03.324021: step 2369, loss 0.706731, acc 0.546875
2016-12-13T14:49:07.821722: step 2370, loss 0.63826, acc 0.671875
2016-12-13T14:49:12.328565: step 2371, loss 0.634902, acc 0.65625
2016-12-13T14:49:16.900366: step 2372, loss 0.704578, acc 0.546875
2016-12-13T14:49:21.762608: step 2373, loss 0.661741, acc 0.59375
2016-12-13T14:49:26.211952: step 2374, loss 0.656528, acc 0.65625
2016-12-13T14:49:30.556529: step 2375, loss 0.589491, acc 0.6875
2016-12-13T14:49:34.995531: step 2376, loss 0.675598, acc 0.640625
2016-12-13T14:49:39.487781: step 2377, loss 0.56946, acc 0.671875
2016-12-13T14:49:43.932136: step 2378, loss 0.754685, acc 0.609375
2016-12-13T14:49:48.420725: step 2379, loss 0.833647, acc 0.515625
2016-12-13T14:49:53.032063: step 2380, loss 0.699913, acc 0.5625
2016-12-13T14:49:57.458568: step 2381, loss 0.645039, acc 0.640625
2016-12-13T14:50:01.877438: step 2382, loss 0.636924, acc 0.59375
2016-12-13T14:50:06.269277: step 2383, loss 0.67619, acc 0.59375
2016-12-13T14:50:10.647828: step 2384, loss 0.73216, acc 0.46875
2016-12-13T14:50:15.185128: step 2385, loss 0.708092, acc 0.5625
2016-12-13T14:50:19.582396: step 2386, loss 0.743046, acc 0.515625
2016-12-13T14:50:24.053580: step 2387, loss 0.671551, acc 0.546875
2016-12-13T14:50:28.501590: step 2388, loss 0.673137, acc 0.5625
2016-12-13T14:50:33.008425: step 2389, loss 0.695001, acc 0.640625
2016-12-13T14:50:37.328075: step 2390, loss 0.731836, acc 0.578125
2016-12-13T14:50:41.786282: step 2391, loss 0.675107, acc 0.671875
2016-12-13T14:50:46.306642: step 2392, loss 0.52683, acc 0.765625
2016-12-13T14:50:50.738105: step 2393, loss 0.633677, acc 0.609375
2016-12-13T14:50:56.464930: step 2394, loss 0.677946, acc 0.640625
2016-12-13T14:51:01.231562: step 2395, loss 0.691845, acc 0.578125
2016-12-13T14:51:06.107561: step 2396, loss 0.606538, acc 0.65625
2016-12-13T14:51:11.186239: step 2397, loss 0.713195, acc 0.5
2016-12-13T14:51:15.815460: step 2398, loss 0.663259, acc 0.5625
^CTraceback (most recent call last):
  File "train.py", line 203, in <module>
    train_step(x_batch, y_batch)
  File "train.py", line 169, in train_step
    feed_dict)
  File "/Users/koza/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 717, in run
    run_metadata_ptr)
  File "/Users/koza/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 915, in _run
    feed_dict_string, options, run_metadata)
  File "/Users/koza/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 965, in _do_run
    target_list, options, run_metadata)
  File "/Users/koza/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 972, in _do_call
    return fn(*args)
  File "/Users/koza/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 954, in _run_fn
    status, run_metadata)
KeyboardInterrupt
You have new mail in /var/mail/koza