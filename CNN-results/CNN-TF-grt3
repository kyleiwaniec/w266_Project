python train.py --positive_data_file data/pos_data_train --negative_data_file data/neg_data_train --dev_sample_percentage .005

Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=64
CHECKPOINT_EVERY=100
DEV_SAMPLE_PERCENTAGE=0.005
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=128
EVALUATE_EVERY=100
FILTER_SIZES=3,4,5
L2_REG_LAMBDA=0.0
LOG_DEVICE_PLACEMENT=False
NEGATIVE_DATA_FILE=data/neg_data_train
NUM_EPOCHS=200
NUM_FILTERS=128
POSITIVE_DATA_FILE=data/pos_data_train

Loading data...
('l-positive_examples', 41051)
('l-negative_examples', 239246)
('type', 280297, 280297)
['1:', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '2:', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '3:', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '4:', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '5:', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '6:', '\n', '', '', '1x1=1', '', '', '', '', '', '', '', '', '', '', '', '', '', '2x1=2', '', '', '', '', '', '', '', '', '', '', '', '', '3x1=3', '', '', '', '', '', '', '', '', '', '', '', '', '4x1=4', '', '', '', '', '', '', '', '', '', '', '', '', '5x1=5', '', '', '', '', '', '', '', '', '', '', '', '', '6x1=6', '\n', '', '', '1x2=2', '', '', '', '', '', '', '', '', '', '', '', '', '', '2x2=4', '', '', '', '', '', '', '', '', '', '', '', '', '3x2=6', '', '', '', '', '', '', '', '', '', '', '', '', '4x2=8', '', '', '', '', '', '', '', '', '', '', '', '', '5x2=10', '', '', '', '', '', '', '', '', '', '', '6x2=12', '\n', '', '', '1x3=3', '', '', '', '', '', '', '', '', '', '', '', '', '', '2x3=6', '', '', '', '', '', '', '', '', '', '', '', '', '3x3=9', '', '', '', '', '', '', '', '', '', '', '', '', '4x3=12', '', '', '', '', '', '', '', '', '', '', '5x3=15', '', '', '', '', '', '', '', '', '', '', '6x3=18', '\n', '', '', '1x4=4', '', '', '', '', '', '', '', '', '', '', '', '', '', '2x4=8', '', '', '', '', '', '', '', '', '', '', '', '', '3x4=12', '', '', '', '', '', '', '', '', '', '', '4x4=16', '', '', '', '', '', '', '', '', '', '', '5x4=20', '', '', '', '', '', '', '', '', '', '', '6x4=24', '\n', '', '', '1x5=5', '', '', '', '', '', '', '', '', '', '', '', '', '', '2x5=10', '', '', '', '', '', '', '', '', '', '', '3x5=15', '', '', '', '', '', '', '', '', '', '', '4x5=20', '', '', '', '', '', '', '', '', '', '', '5x5=25', '', '', '', '', '', '', '', '', '', '', '6x5=30', '\n', '', '', '1x6=6', '', '', '', '', '', '', '', '', '', '', '', '', '', '2x6=12', '', '', '', '', '', '', '', '', '', '', '3x6=18', '', '', '', '', '', '', '', '', '', '', '4x6=24', '', '', '', '', '', '', '', '', '', '', '5x6=30', '', '', '', '', '', '', '', '', '', '', '6x6=36', '\n', '', '', '1x7=7', '', '', '', '', '', '', '', '', '', '', '', '', '', '2x7=14', '', '', '', '', '', '', '', '', '', '', '3x7=21', '', '', '', '', '', '', '', '', '', '', '4x7=28', '', '', '', '', '', '', '', '', '', '', '5x7=35', '', '', '', '', '', '', '', '', '', '', '6x7=42', '\n', '', '', '1x8=8', '', '', '', '', '', '', '', '', '', '', '', '', '', '2x8=16', '', '', '', '', '', '', '', '', '', '', '3x8=24', '', '', '', '', '', '', '', '', '', '', '4x8=32', '', '', '', '', '', '', '', '', '', '', '5x8=40', '', '', '', '', '', '', '', '', '', '', '6x8=48', '\n', '', '', '1x9=9', '', '', '', '', '', '', '', '', '', '', '', '', '', '2x9=18', '', '', '', '', '', '', '', '', '', '', '3x9=27', '', '', '', '', '', '', '', '', '', '', '4x9=36', '', '', '', '', '', '', '', '', '', '', '5x9=45', '', '', '', '', '', '', '', '', '', '', '6x9=54', '\n', '', '', '1x10=10', '', '', '', '', '', '', '', '', '2x10=20', '', '', '', '', '', '', '', '', '3x10=30', '', '', '', '', '', '', '', '', '4x10=40', '', '', '', '', '', '', '', '', '5x10=50', '', '', '', '', '', '', '', '', '6x10=60', '\n', '', '', '1x11=11', '', '', '', '', '', '', '', '', '2x11=22', '', '', '', '', '', '', '', '', '3x11=33', '', '', '', '', '', '', '', '', '4x11=44', '', '', '', '', '', '', '', '', '5x11=55', '', '', '', '', '', '', '', '', '6x11=66', '\n', '', '', '1x12=12', '', '', '', '', '', '', '', '', '2x12=24', '', '', '', '', '', '', '', '', '3x12=36', '', '', '', '', '', '', '', '', '4x12=48', '', '', '', '', '', '', '', '', '5x12=60', '', '', '', '', '', '', '', '', '6x12=72\n\n7:', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '8:', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '9:', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '10:', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '11:', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '12:', '\n', '', '7x1=7', '', '', '', '', '', '', '', '', '', '', '', '', '8x1=8', '', '', '', '', '', '', '', '', '', '', '', '', '9x1=9', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '10x1=10', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '11x1=11', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '12x1=12', '\n', '', '', '', '7x2=14', '', '', '', '', '', '', '', '', '', '8x2=16', '', '', '', '', '', '', '', '', '', '', '9x2=18', '', '', '', '', '', '', '', '', '', '', '', '', '', '10x2=20', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '11x2=22', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '12x2=24', '\n', '', '', '', '7x3=21', '', '', '', '', '', '', '', '', '', '8x3=24', '', '', '', '', '', '', '', '', '', '', '9x3=27', '', '', '', '', '', '', '', '', '', '', '', '', '', '10x3=30', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '11x3=33', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '12x3=36', '\n', '', '', '', '7x4=28', '', '', '', '', '', '', '', '', '', '8x4=32', '', '', '', '', '', '', '', '', '', '', '9x4=36', '', '', '', '', '', '', '', '', '', '', '', '', '', '10x4=40', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '11x4=44', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '12x4=48', '\n', '', '', '', '7x5=35', '', '', '', '', '', '', '', '', '', '8x5=40', '', '', '', '', '', '', '', '', '', '', '9x5=45', '', '', '', '', '', '', '', '', '', '', '', '', '', '10x5=50', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '11x5=55', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '12x5=60', '\n', '', '', '', '7x6=42', '', '', '', '', '', '', '', '', '', '8x6=48', '', '', '', '', '', '', '', '', '', '', '9x6=54', '', '', '', '', '', '', '', '', '', '', '', '', '', '10x6=60', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '11x6=66', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '12x6=72', '\n', '', '', '', '7x7=49', '', '', '', '', '', '', '', '', '', '8x7=56', '', '', '', '', '', '', '', '', '', '', '9x7=63', '', '', '', '', '', '', '', '', '', '', '', '', '', '10x7=70', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '11x7=77', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '12x7=84', '\n', '', '', '', '7x8=56', '', '', '', '', '', '', '', '', '', '8x8=64', '', '', '', '', '', '', '', '', '', '', '9x8=72', '', '', '', '', '', '', '', '', '', '', '', '', '', '10x8=80', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '11x8=88', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '12x8=96', '\n', '', '', '', '7x9=63', '', '', '', '', '', '', '', '', '', '8x9=72', '', '', '', '', '', '', '', '', '', '', '9x9=81', '', '', '', '', '', '', '', '', '', '', '', '', '', '10x9=90', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '11x9=99', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '12x9=108', '\n', '', '', '', '7x10=70', '', '', '', '', '', '', '', '8x10=80', '', '', '', '', '', '', '', '', '9x10=90', '', '', '', '', '', '', '', '', '', '', '10x10=100', '', '', '', '', '', '', '', '', '', '', '', '11x10=110', '', '', '', '', '', '', '', '', '', '', '12x10=120', '\n', '', '', '', '7x11=77', '', '', '', '', '', '', '', '8x11=88', '', '', '', '', '', '', '', '', '9x11=99', '', '', '', '', '', '', '', '', '', '', '10x11=110', '', '', '', '', '', '', '', '', '', '', '', '11x11=121', '', '', '', '', '', '', '', '', '', '', '12x11=132', '\n', '', '', '', '7x12=84', '', '', '', '', '', '', '', '8x12=96', '', '', '', '', '', '', '', '', '9x12=108', '', '', '', '', '', '', '', '', '10x12=120', '', '', '', '', '', '', '', '', '', '', '', '11x12=132', '', '', '', '', '', '', '', '', '', '', '12x12=144']
Build vocabulary...
('max_document_length...', 1713)
('x shape', (280297, 1713))
Split train/test set...
Vocabulary Size: 161983
Train/Dev split: 278896/1401
Writing to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481474959

2016-12-11T11:49:25.244362: step 1, loss 4.60461, acc 0.28125
2016-12-11T11:49:28.611283: step 2, loss 2.76251, acc 0.46875
2016-12-11T11:49:32.013929: step 3, loss 1.93336, acc 0.515625
2016-12-11T11:49:35.338215: step 4, loss 1.54114, acc 0.703125
2016-12-11T11:49:38.845172: step 5, loss 0.735939, acc 0.828125
2016-12-11T11:49:42.252553: step 6, loss 1.51094, acc 0.78125
2016-12-11T11:49:45.785539: step 7, loss 1.73562, acc 0.875
2016-12-11T11:49:49.161944: step 8, loss 0.786391, acc 0.9375
2016-12-11T11:49:52.755754: step 9, loss 1.3776, acc 0.859375
2016-12-11T11:49:56.408174: step 10, loss 1.41517, acc 0.890625
2016-12-11T11:49:59.950449: step 11, loss 2.90111, acc 0.78125
2016-12-11T11:50:03.266093: step 12, loss 2.28373, acc 0.828125
2016-12-11T11:50:06.665390: step 13, loss 1.30114, acc 0.90625
2016-12-11T11:50:10.110143: step 14, loss 1.94393, acc 0.875
2016-12-11T11:50:13.521880: step 15, loss 3.1342, acc 0.796875
2016-12-11T11:50:16.944392: step 16, loss 1.23914, acc 0.890625
2016-12-11T11:50:20.509120: step 17, loss 1.92769, acc 0.8125
2016-12-11T11:50:24.032857: step 18, loss 1.40959, acc 0.828125
2016-12-11T11:50:27.393231: step 19, loss 1.62796, acc 0.828125
2016-12-11T11:50:30.696943: step 20, loss 0.359864, acc 0.90625
2016-12-11T11:50:34.004967: step 21, loss 1.58618, acc 0.734375
2016-12-11T11:50:37.353077: step 22, loss 0.965518, acc 0.828125
2016-12-11T11:50:40.671350: step 23, loss 1.29139, acc 0.671875
2016-12-11T11:50:44.089382: step 24, loss 0.771783, acc 0.765625
2016-12-11T11:50:47.780146: step 25, loss 1.59908, acc 0.65625
2016-12-11T11:50:51.368937: step 26, loss 1.52072, acc 0.734375
2016-12-11T11:50:54.825677: step 27, loss 1.50314, acc 0.609375
2016-12-11T11:50:58.128928: step 28, loss 1.1286, acc 0.703125
2016-12-11T11:51:01.584196: step 29, loss 1.65287, acc 0.734375
2016-12-11T11:51:04.979626: step 30, loss 1.06813, acc 0.734375
2016-12-11T11:51:08.747084: step 31, loss 0.859828, acc 0.734375
2016-12-11T11:51:12.622690: step 32, loss 1.49209, acc 0.671875
2016-12-11T11:51:16.192075: step 33, loss 1.17378, acc 0.734375
2016-12-11T11:51:19.984253: step 34, loss 0.861733, acc 0.828125
2016-12-11T11:51:23.691384: step 35, loss 1.51294, acc 0.734375
2016-12-11T11:51:27.257704: step 36, loss 1.72753, acc 0.703125
2016-12-11T11:51:30.806934: step 37, loss 0.941423, acc 0.84375
2016-12-11T11:51:34.437379: step 38, loss 1.59105, acc 0.71875
2016-12-11T11:51:38.348421: step 39, loss 1.37558, acc 0.796875
2016-12-11T11:51:41.959321: step 40, loss 0.874144, acc 0.875
2016-12-11T11:51:45.862043: step 41, loss 1.57139, acc 0.8125
2016-12-11T11:51:49.506223: step 42, loss 0.556617, acc 0.90625
2016-12-11T11:51:52.851122: step 43, loss 1.02784, acc 0.875
2016-12-11T11:51:56.186585: step 44, loss 1.96925, acc 0.6875
2016-12-11T11:51:59.417273: step 45, loss 1.30497, acc 0.734375
2016-12-11T11:52:02.585871: step 46, loss 1.1848, acc 0.765625
2016-12-11T11:52:05.715405: step 47, loss 1.31889, acc 0.75
2016-12-11T11:52:08.875282: step 48, loss 1.952, acc 0.640625
2016-12-11T11:52:11.904173: step 49, loss 1.29354, acc 0.828125
2016-12-11T11:52:15.007933: step 50, loss 1.57908, acc 0.671875
2016-12-11T11:52:18.111974: step 51, loss 1.78266, acc 0.640625
2016-12-11T11:52:21.204355: step 52, loss 1.56509, acc 0.640625
2016-12-11T11:52:24.363745: step 53, loss 1.32111, acc 0.640625
2016-12-11T11:52:27.643299: step 54, loss 1.36657, acc 0.71875
2016-12-11T11:52:30.863991: step 55, loss 1.52239, acc 0.71875
2016-12-11T11:52:34.274143: step 56, loss 1.02709, acc 0.6875
2016-12-11T11:52:37.435433: step 57, loss 1.17516, acc 0.765625
2016-12-11T11:52:40.476698: step 58, loss 1.02463, acc 0.796875
2016-12-11T11:52:43.610553: step 59, loss 0.818266, acc 0.75
2016-12-11T11:52:46.687986: step 60, loss 1.18189, acc 0.765625
2016-12-11T11:52:49.819578: step 61, loss 1.57432, acc 0.71875
2016-12-11T11:52:52.842760: step 62, loss 1.13542, acc 0.796875
2016-12-11T11:52:56.162171: step 63, loss 0.964346, acc 0.796875
2016-12-11T11:52:59.325844: step 64, loss 1.12583, acc 0.78125
2016-12-11T11:53:03.333719: step 65, loss 1.08782, acc 0.8125
2016-12-11T11:53:07.183968: step 66, loss 1.18793, acc 0.796875
2016-12-11T11:53:11.103910: step 67, loss 1.36088, acc 0.71875
2016-12-11T11:53:14.652543: step 68, loss 1.46367, acc 0.75
2016-12-11T11:53:17.992325: step 69, loss 0.878316, acc 0.796875
2016-12-11T11:53:21.189908: step 70, loss 1.11938, acc 0.75
2016-12-11T11:53:24.305567: step 71, loss 0.975619, acc 0.765625
2016-12-11T11:53:27.498046: step 72, loss 1.47509, acc 0.65625
2016-12-11T11:53:30.606778: step 73, loss 1.19963, acc 0.765625
2016-12-11T11:53:34.257490: step 74, loss 1.08234, acc 0.75
2016-12-11T11:53:37.686199: step 75, loss 1.01846, acc 0.75
2016-12-11T11:53:41.030149: step 76, loss 1.02291, acc 0.765625
2016-12-11T11:53:44.145562: step 77, loss 1.03998, acc 0.734375
2016-12-11T11:53:47.272421: step 78, loss 0.865042, acc 0.828125
2016-12-11T11:53:50.325319: step 79, loss 0.789539, acc 0.78125
2016-12-11T11:53:53.437207: step 80, loss 0.974968, acc 0.84375
2016-12-11T11:53:56.587565: step 81, loss 1.18199, acc 0.765625
2016-12-11T11:53:59.753716: step 82, loss 1.29149, acc 0.796875
2016-12-11T11:54:02.968686: step 83, loss 1.49743, acc 0.703125
2016-12-11T11:54:06.413712: step 84, loss 1.19244, acc 0.75
2016-12-11T11:54:09.889531: step 85, loss 1.23246, acc 0.71875
2016-12-11T11:54:13.296363: step 86, loss 1.08959, acc 0.6875
2016-12-11T11:54:16.747910: step 87, loss 1.02378, acc 0.796875
2016-12-11T11:54:19.889531: step 88, loss 1.52584, acc 0.75
2016-12-11T11:54:23.007208: step 89, loss 1.70889, acc 0.671875
2016-12-11T11:54:26.124715: step 90, loss 1.36682, acc 0.765625
2016-12-11T11:54:29.180364: step 91, loss 0.909893, acc 0.78125
2016-12-11T11:54:32.327237: step 92, loss 0.784763, acc 0.78125
2016-12-11T11:54:35.533606: step 93, loss 1.46798, acc 0.71875
2016-12-11T11:54:38.850854: step 94, loss 0.878469, acc 0.78125
2016-12-11T11:54:42.206710: step 95, loss 0.931051, acc 0.734375
2016-12-11T11:54:45.415001: step 96, loss 0.76534, acc 0.828125
2016-12-11T11:54:48.577447: step 97, loss 1.39491, acc 0.703125
2016-12-11T11:54:51.773012: step 98, loss 1.15729, acc 0.703125
2016-12-11T11:54:55.008090: step 99, loss 0.823668, acc 0.84375
2016-12-11T11:54:58.110319: step 100, loss 1.04226, acc 0.71875

Evaluation:
2016-12-11T11:55:36.932019: step 100, loss 0.709449, acc 0.845111

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481474959/checkpoints/model-100

2016-12-11T11:55:42.676438: step 101, loss 1.00843, acc 0.796875
2016-12-11T11:55:45.837017: step 102, loss 1.00641, acc 0.8125
2016-12-11T11:55:49.176517: step 103, loss 1.69576, acc 0.71875
2016-12-11T11:55:52.709492: step 104, loss 1.45004, acc 0.75
2016-12-11T11:55:55.926075: step 105, loss 0.830749, acc 0.75
2016-12-11T11:55:59.139939: step 106, loss 1.08533, acc 0.765625
2016-12-11T11:56:02.425415: step 107, loss 1.63308, acc 0.671875
2016-12-11T11:56:05.623207: step 108, loss 1.68043, acc 0.640625
2016-12-11T11:56:08.793891: step 109, loss 0.989377, acc 0.828125
2016-12-11T11:56:11.865687: step 110, loss 0.949329, acc 0.765625
2016-12-11T11:56:15.126830: step 111, loss 1.48498, acc 0.6875
2016-12-11T11:56:18.344053: step 112, loss 1.12943, acc 0.765625
2016-12-11T11:56:21.530744: step 113, loss 1.25331, acc 0.671875
2016-12-11T11:56:25.007882: step 114, loss 1.3878, acc 0.78125
2016-12-11T11:56:28.285809: step 115, loss 1.62932, acc 0.71875
2016-12-11T11:56:31.370154: step 116, loss 1.71038, acc 0.609375
2016-12-11T11:56:34.444807: step 117, loss 0.676785, acc 0.796875
2016-12-11T11:56:37.620208: step 118, loss 1.28398, acc 0.640625
2016-12-11T11:56:40.946072: step 119, loss 0.825719, acc 0.734375
2016-12-11T11:56:44.400603: step 120, loss 1.29423, acc 0.703125
2016-12-11T11:56:48.003518: step 121, loss 1.36192, acc 0.71875
2016-12-11T11:56:51.307897: step 122, loss 0.691359, acc 0.8125
2016-12-11T11:56:54.630253: step 123, loss 0.992813, acc 0.8125
2016-12-11T11:56:57.947487: step 124, loss 0.777437, acc 0.828125
2016-12-11T11:57:01.088518: step 125, loss 0.965852, acc 0.828125
2016-12-11T11:57:04.245404: step 126, loss 0.963444, acc 0.78125
2016-12-11T11:57:07.435954: step 127, loss 0.710396, acc 0.84375
2016-12-11T11:57:10.589371: step 128, loss 1.20563, acc 0.671875
2016-12-11T11:57:13.880639: step 129, loss 1.07307, acc 0.796875
2016-12-11T11:57:17.169130: step 130, loss 1.02513, acc 0.765625
2016-12-11T11:57:20.411173: step 131, loss 1.12846, acc 0.75
2016-12-11T11:57:23.488640: step 132, loss 1.30031, acc 0.78125
2016-12-11T11:57:26.667473: step 133, loss 1.17605, acc 0.75
2016-12-11T11:57:29.826520: step 134, loss 0.789792, acc 0.796875
2016-12-11T11:57:32.988793: step 135, loss 1.29245, acc 0.71875
2016-12-11T11:57:36.320567: step 136, loss 0.86343, acc 0.796875
2016-12-11T11:57:39.888060: step 137, loss 0.749631, acc 0.75
2016-12-11T11:57:43.292127: step 138, loss 0.799854, acc 0.8125
2016-12-11T11:57:46.860002: step 139, loss 1.08582, acc 0.65625
2016-12-11T11:57:50.529849: step 140, loss 0.599686, acc 0.78125
2016-12-11T11:57:53.994514: step 141, loss 1.01555, acc 0.765625
2016-12-11T11:57:57.323801: step 142, loss 0.98804, acc 0.765625
2016-12-11T11:58:01.045430: step 143, loss 0.80549, acc 0.8125
2016-12-11T11:58:04.773992: step 144, loss 0.904975, acc 0.796875
2016-12-11T11:58:07.941299: step 145, loss 1.39161, acc 0.734375
2016-12-11T11:58:11.545618: step 146, loss 1.39075, acc 0.75
2016-12-11T11:58:15.218757: step 147, loss 0.729607, acc 0.75
2016-12-11T11:58:18.429244: step 148, loss 1.14094, acc 0.796875
2016-12-11T11:58:21.541988: step 149, loss 0.821307, acc 0.84375
2016-12-11T11:58:24.635147: step 150, loss 1.14472, acc 0.78125
2016-12-11T11:58:27.726215: step 151, loss 0.784, acc 0.796875
2016-12-11T11:58:30.988885: step 152, loss 1.10708, acc 0.765625
2016-12-11T11:58:34.514283: step 153, loss 0.88503, acc 0.796875
2016-12-11T11:58:37.903454: step 154, loss 1.48123, acc 0.6875
2016-12-11T11:58:41.491044: step 155, loss 0.796347, acc 0.78125
2016-12-11T11:58:44.954292: step 156, loss 0.775084, acc 0.8125
2016-12-11T11:58:48.367706: step 157, loss 0.619132, acc 0.84375
2016-12-11T11:58:51.705591: step 158, loss 0.686713, acc 0.8125
2016-12-11T11:58:54.836832: step 159, loss 0.710569, acc 0.78125
2016-12-11T11:58:58.089070: step 160, loss 0.724259, acc 0.796875
2016-12-11T11:59:01.349440: step 161, loss 0.717248, acc 0.796875
2016-12-11T11:59:04.759913: step 162, loss 0.946834, acc 0.75
2016-12-11T11:59:08.040936: step 163, loss 0.945332, acc 0.78125
2016-12-11T11:59:11.088973: step 164, loss 1.33691, acc 0.71875
2016-12-11T11:59:14.173374: step 165, loss 0.665556, acc 0.8125
2016-12-11T11:59:17.275069: step 166, loss 1.01324, acc 0.734375
2016-12-11T11:59:20.550741: step 167, loss 1.24031, acc 0.671875
2016-12-11T11:59:23.864843: step 168, loss 0.928945, acc 0.75
2016-12-11T11:59:27.303117: step 169, loss 0.906405, acc 0.71875
2016-12-11T11:59:30.560657: step 170, loss 1.10041, acc 0.640625
2016-12-11T11:59:33.921543: step 171, loss 0.53089, acc 0.84375
2016-12-11T11:59:37.430472: step 172, loss 0.886048, acc 0.765625
2016-12-11T11:59:41.086235: step 173, loss 0.533673, acc 0.859375
2016-12-11T11:59:44.466744: step 174, loss 0.487378, acc 0.828125
2016-12-11T11:59:47.810146: step 175, loss 1.02757, acc 0.734375
2016-12-11T11:59:51.162846: step 176, loss 0.685635, acc 0.796875
2016-12-11T11:59:54.575146: step 177, loss 0.996648, acc 0.796875
2016-12-11T11:59:58.023859: step 178, loss 0.797569, acc 0.8125
2016-12-11T12:00:01.838550: step 179, loss 0.998203, acc 0.75
2016-12-11T12:00:05.082811: step 180, loss 1.14107, acc 0.765625
2016-12-11T12:00:08.186721: step 181, loss 0.747377, acc 0.796875
2016-12-11T12:00:11.465123: step 182, loss 0.427712, acc 0.859375
2016-12-11T12:00:14.550003: step 183, loss 0.786344, acc 0.78125
2016-12-11T12:00:17.680467: step 184, loss 1.33611, acc 0.734375
2016-12-11T12:00:20.714713: step 185, loss 1.1323, acc 0.703125
2016-12-11T12:00:23.778276: step 186, loss 1.1961, acc 0.734375
2016-12-11T12:00:26.922092: step 187, loss 0.8092, acc 0.78125
2016-12-11T12:00:29.963412: step 188, loss 1.19415, acc 0.640625
2016-12-11T12:00:33.111428: step 189, loss 0.527188, acc 0.828125
2016-12-11T12:00:36.175380: step 190, loss 1.39291, acc 0.59375
2016-12-11T12:00:39.266180: step 191, loss 0.678237, acc 0.84375
2016-12-11T12:00:42.387772: step 192, loss 1.15788, acc 0.71875
2016-12-11T12:00:45.492203: step 193, loss 0.61367, acc 0.8125
2016-12-11T12:00:48.695909: step 194, loss 0.885578, acc 0.78125
2016-12-11T12:00:51.798515: step 195, loss 1.16171, acc 0.765625
2016-12-11T12:00:54.998709: step 196, loss 1.16041, acc 0.8125
2016-12-11T12:00:58.115651: step 197, loss 0.879532, acc 0.78125
2016-12-11T12:01:01.458396: step 198, loss 0.908975, acc 0.765625
2016-12-11T12:01:04.985881: step 199, loss 0.955691, acc 0.640625
2016-12-11T12:01:08.351030: step 200, loss 0.970786, acc 0.703125

Evaluation:
2016-12-11T12:01:47.738670: step 200, loss 0.542933, acc 0.845111

Saved model checkpoint to /Users/koza/Documents/UCBerkeley/266/FinalProject/w266_Project/cnn-text-classification-tf/runs/1481474959/checkpoints/model-200

2016-12-11T12:01:53.288986: step 201, loss 1.45941, acc 0.625
2016-12-11T12:01:56.526862: step 202, loss 0.912725, acc 0.71875
2016-12-11T12:01:59.770564: step 203, loss 0.762917, acc 0.75
2016-12-11T12:02:02.927755: step 204, loss 0.5787, acc 0.78125
2016-12-11T12:02:06.126947: step 205, loss 1.2296, acc 0.671875
2016-12-11T12:02:09.313154: step 206, loss 0.864647, acc 0.8125
2016-12-11T12:02:12.519659: step 207, loss 0.92485, acc 0.765625
2016-12-11T12:02:15.719406: step 208, loss 0.808639, acc 0.765625
2016-12-11T12:02:19.098807: step 209, loss 0.755887, acc 0.78125
2016-12-11T12:02:22.338916: step 210, loss 0.747049, acc 0.828125
2016-12-11T12:02:25.550074: step 211, loss 0.883599, acc 0.765625
2016-12-11T12:02:28.601575: step 212, loss 0.549529, acc 0.84375
2016-12-11T12:02:31.645740: step 213, loss 0.996203, acc 0.75
2016-12-11T12:02:34.775123: step 214, loss 0.584447, acc 0.84375
2016-12-11T12:02:37.802709: step 215, loss 0.915784, acc 0.765625
2016-12-11T12:02:40.842344: step 216, loss 0.872368, acc 0.75
2016-12-11T12:02:43.952822: step 217, loss 0.82138, acc 0.78125
2016-12-11T12:02:47.111688: step 218, loss 0.571884, acc 0.78125
2016-12-11T12:02:50.273301: step 219, loss 0.588702, acc 0.84375
2016-12-11T12:02:53.464011: step 220, loss 1.44982, acc 0.8125
2016-12-11T12:02:57.203891: step 221, loss 1.12183, acc 0.828125
2016-12-11T12:03:01.009758: step 222, loss 0.658806, acc 0.859375
2016-12-11T12:03:04.823264: step 223, loss 0.91298, acc 0.828125
2016-12-11T12:03:08.210686: step 224, loss 0.642514, acc 0.8125
2016-12-11T12:03:11.605189: step 225, loss 0.770465, acc 0.8125
2016-12-11T12:03:14.736903: step 226, loss 0.870592, acc 0.78125
2016-12-11T12:03:17.900704: step 227, loss 0.819805, acc 0.71875
2016-12-11T12:03:20.921875: step 228, loss 0.669553, acc 0.84375
2016-12-11T12:03:23.967050: step 229, loss 0.958437, acc 0.703125
2016-12-11T12:03:27.043054: step 230, loss 1.35315, acc 0.625
2016-12-11T12:03:30.387651: step 231, loss 0.990023, acc 0.65625
2016-12-11T12:03:33.461822: step 232, loss 0.656264, acc 0.765625
2016-12-11T12:03:36.571206: step 233, loss 0.810362, acc 0.78125
2016-12-11T12:03:39.667385: step 234, loss 0.760481, acc 0.734375
2016-12-11T12:03:42.766513: step 235, loss 0.618857, acc 0.859375
2016-12-11T12:03:45.953697: step 236, loss 0.619234, acc 0.84375
2016-12-11T12:03:49.278978: step 237, loss 0.70333, acc 0.828125
2016-12-11T12:03:52.631762: step 238, loss 1.6001, acc 0.765625
2016-12-11T12:03:55.880697: step 239, loss 1.23699, acc 0.765625
2016-12-11T12:03:59.058314: step 240, loss 1.2162, acc 0.8125
2016-12-11T12:04:02.428599: step 241, loss 0.759629, acc 0.84375
2016-12-11T12:04:05.657403: step 242, loss 0.710771, acc 0.828125
2016-12-11T12:04:08.737795: step 243, loss 1.24744, acc 0.703125
2016-12-11T12:04:11.905199: step 244, loss 0.662626, acc 0.796875
2016-12-11T12:04:15.088593: step 245, loss 0.726843, acc 0.75
2016-12-11T12:04:18.254139: step 246, loss 0.482009, acc 0.84375
2016-12-11T12:04:21.494114: step 247, loss 0.920758, acc 0.703125
2016-12-11T12:04:24.561203: step 248, loss 0.667479, acc 0.828125
2016-12-11T12:04:27.671423: step 249, loss 0.74033, acc 0.75
2016-12-11T12:04:30.775779: step 250, loss 0.599423, acc 0.78125
2016-12-11T12:04:33.998883: step 251, loss 0.748247, acc 0.78125
2016-12-11T12:04:37.242560: step 252, loss 0.512241, acc 0.765625
2016-12-11T12:04:40.447860: step 253, loss 0.554422, acc 0.8125
2016-12-11T12:04:43.614573: step 254, loss 1.06871, acc 0.78125
2016-12-11T12:04:46.788028: step 255, loss 1.07254, acc 0.734375
2016-12-11T12:04:49.941014: step 256, loss 0.561468, acc 0.84375
2016-12-11T12:04:53.046237: step 257, loss 0.688386, acc 0.859375
2016-12-11T12:04:56.158415: step 258, loss 0.81541, acc 0.875
2016-12-11T12:04:59.224656: step 259, loss 0.905821, acc 0.8125
2016-12-11T12:05:02.288281: step 260, loss 0.995298, acc 0.71875
2016-12-11T12:05:05.328127: step 261, loss 0.893897, acc 0.765625
2016-12-11T12:05:08.567138: step 262, loss 0.755773, acc 0.71875
2016-12-11T12:05:11.894811: step 263, loss 0.745307, acc 0.765625
2016-12-11T12:05:15.205711: step 264, loss 0.769014, acc 0.734375
2016-12-11T12:05:18.366362: step 265, loss 0.959259, acc 0.75
2016-12-11T12:05:21.458414: step 266, loss 0.57396, acc 0.859375
2016-12-11T12:05:24.750363: step 267, loss 0.577411, acc 0.75
2016-12-11T12:05:27.853474: step 268, loss 0.712665, acc 0.8125
2016-12-11T12:05:30.964965: step 269, loss 0.706682, acc 0.78125
2016-12-11T12:05:34.103056: step 270, loss 0.671469, acc 0.75
2016-12-11T12:05:37.289202: step 271, loss 0.60188, acc 0.796875
2016-12-11T12:05:40.611827: step 272, loss 1.01182, acc 0.71875
2016-12-11T12:05:43.721194: step 273, loss 0.559159, acc 0.828125
2016-12-11T12:05:46.916903: step 274, loss 0.630353, acc 0.765625
2016-12-11T12:05:49.954076: step 275, loss 0.415793, acc 0.828125
2016-12-11T12:05:53.010967: step 276, loss 0.650832, acc 0.828125
2016-12-11T12:05:56.100139: step 277, loss 0.58928, acc 0.8125
2016-12-11T12:05:59.207496: step 278, loss 0.497621, acc 0.84375
2016-12-11T12:06:02.518082: step 279, loss 0.602752, acc 0.78125
2016-12-11T12:06:05.745573: step 280, loss 0.867363, acc 0.75
2016-12-11T12:06:09.014304: step 281, loss 0.810574, acc 0.765625
2016-12-11T12:06:12.608032: step 282, loss 0.548359, acc 0.8125
2016-12-11T12:06:16.261929: step 283, loss 0.848734, acc 0.765625
2016-12-11T12:06:19.983350: step 284, loss 0.607863, acc 0.84375
2016-12-11T12:06:23.584130: step 285, loss 0.993078, acc 0.765625
2016-12-11T12:06:26.823136: step 286, loss 1.24156, acc 0.765625
2016-12-11T12:06:29.988106: step 287, loss 0.82005, acc 0.75
2016-12-11T12:06:33.132041: step 288, loss 0.645741, acc 0.84375
2016-12-11T12:06:36.345444: step 289, loss 0.872228, acc 0.78125
2016-12-11T12:06:39.554799: step 290, loss 0.97032, acc 0.71875
2016-12-11T12:06:42.646128: step 291, loss 0.663489, acc 0.703125
2016-12-11T12:06:45.898496: step 292, loss 0.693806, acc 0.78125
2016-12-11T12:06:49.032495: step 293, loss 0.556206, acc 0.84375
2016-12-11T12:06:52.055524: step 294, loss 0.561525, acc 0.796875
2016-12-11T12:06:55.227559: step 295, loss 0.565373, acc 0.84375
2016-12-11T12:06:58.438012: step 296, loss 0.447612, acc 0.859375
2016-12-11T12:07:01.745410: step 297, loss 0.897216, acc 0.71875
2016-12-11T12:07:05.018607: step 298, loss 0.632544, acc 0.8125
2016-12-11T12:07:08.286457: step 299, loss 0.600305, acc 0.796875
2016-12-11T12:07:11.488618: step 300, loss 0.987996, acc 0.71875

Evaluation:
2016-12-11T12:07:46.605973: step 300, loss 0.52248, acc 0.845111