{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# The RNN implementation\n",
    "import processing\n",
    "import rnnlm\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from features import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(corpus, training_data, model_name, model_params, max_time, batch_size,\n",
    "                learning_rate, keep_prob, num_epochs):    \n",
    "    trained_filename = 'tf_saved/rnnlm_%s' % model_name\n",
    "    \n",
    "    # Will print status every this many seconds\n",
    "    print_interval = 5\n",
    "\n",
    "    # Clear old log directory\n",
    "    shutil.rmtree(\"tf_summaries\", ignore_errors=True)\n",
    "\n",
    "    with tf.Graph().as_default(), tf.Session() as session:\n",
    "        # Seed RNG for repeatability\n",
    "        tf.set_random_seed(42)\n",
    "  \n",
    "        with tf.variable_scope(\"model\", reuse=None):\n",
    "            lm = rnnlm.RNNLM(model_params)\n",
    "            lm.BuildCoreGraph()\n",
    "            lm.BuildTrainGraph()\n",
    "  \n",
    "        session.run(tf.initialize_all_variables())\n",
    "        saver = tf.train.Saver()\n",
    "  \n",
    "        for epoch in xrange(1,num_epochs+1):\n",
    "            t0_epoch = time.time()\n",
    "            bi = processing.batch_generator(training_data[\"train_ids\"], batch_size, max_time)\n",
    "            print \"[epoch %d] Starting epoch %d\" % (epoch, epoch)\n",
    "\n",
    "            # Run a training epoch.\n",
    "            lm.RunEpoch(session, bi, train=True, verbose=True, keep_prob=keep_prob)\n",
    "\n",
    "            print \"[epoch %d] Completed in %s\" % (epoch, rnnlm.pretty_timedelta(since=t0_epoch))\n",
    "\n",
    "            print (\"[epoch %d]\" % epoch),\n",
    "            lm.ScoreDataset(session, training_data[\"test_ids\"], name=\"Test set\")\n",
    "            print \"\"\n",
    "\n",
    "            # Save a checkpoint\n",
    "            saver.save(session, 'tf_saved/rnnlm', global_step=epoch)\n",
    "    \n",
    "        # Save final model\n",
    "        saver.save(session, trained_filename)\n",
    "        \n",
    "    print \"Done training.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print \"Loading datasets...\"\n",
    "with open(\"/usr/src/app/data/ka-comments-balanced.pickle\", \"rb\") as f:\n",
    "    comments_dataset = pickle.load(f)\n",
    "print \"Done.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reload(rnnlm)\n",
    "reload(processing)\n",
    "model_params = rnnlm.RNNParams(V=1000, H=100, num_layers=1)\n",
    "train_params = {\n",
    "    \"max_time\": 20,\n",
    "    \"batch_size\": 50,\n",
    "    \"learning_rate\": 0.25,\n",
    "    \"keep_prob\": 0.5,\n",
    "    \"num_epochs\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocabulary...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "reload(rnnlm)\n",
    "reload(processing)\n",
    "corpus = processing.Corpus(comments_dataset[\"train_data\"][\"content\"], model_params.V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding sentences...\n",
      "Processing sentences...\n",
      "Loaded 600538 sentences (1.02437e+07 tokens)\n",
      "Training set: 480430 sentences (8195446 tokens)\n",
      "Test set: 120108 sentences (2048280 tokens)\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "reload(rnnlm)\n",
    "reload(processing)\n",
    "true_training_data = corpus.generate_training_data(\n",
    "    comments_dataset[\"train_data\"][comments_dataset[\"train_data\"][\"hasVotes\"] == True][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] Starting epoch 1\n",
      "[batch 98]: seen 99000 words at 9853 wps, loss = 4.690\n",
      "[batch 208]: seen 209000 words at 10379 wps, loss = 4.451\n",
      "[batch 319]: seen 320000 words at 10606 wps, loss = 4.331\n",
      "[batch 430]: seen 431000 words at 10706 wps, loss = 4.258\n",
      "[batch 545]: seen 546000 words at 10848 wps, loss = 4.199\n",
      "[batch 659]: seen 660000 words at 10928 wps, loss = 4.152\n",
      "[batch 771]: seen 772000 words at 10964 wps, loss = 4.121\n",
      "[batch 877]: seen 878000 words at 10915 wps, loss = 4.094\n",
      "[batch 991]: seen 992000 words at 10958 wps, loss = 4.070\n",
      "[batch 1093]: seen 1094000 words at 10876 wps, loss = 4.051\n",
      "[batch 1192]: seen 1193000 words at 10783 wps, loss = 4.035\n",
      "[batch 1300]: seen 1301000 words at 10783 wps, loss = 4.019\n",
      "[batch 1408]: seen 1409000 words at 10782 wps, loss = 4.003\n",
      "[batch 1522]: seen 1523000 words at 10820 wps, loss = 3.989\n",
      "[batch 1623]: seen 1624000 words at 10770 wps, loss = 3.977\n",
      "[batch 1737]: seen 1738000 words at 10804 wps, loss = 3.965\n",
      "[batch 1850]: seen 1851000 words at 10829 wps, loss = 3.952\n",
      "[batch 1964]: seen 1965000 words at 10857 wps, loss = 3.943\n",
      "[batch 2077]: seen 2078000 words at 10875 wps, loss = 3.933\n",
      "[batch 2193]: seen 2194000 words at 10906 wps, loss = 3.924\n",
      "[batch 2307]: seen 2308000 words at 10928 wps, loss = 3.916\n",
      "[batch 2419]: seen 2420000 words at 10938 wps, loss = 3.908\n",
      "[batch 2525]: seen 2526000 words at 10923 wps, loss = 3.901\n",
      "[batch 2633]: seen 2634000 words at 10918 wps, loss = 3.895\n",
      "[batch 2746]: seen 2747000 words at 10932 wps, loss = 3.888\n",
      "[batch 2861]: seen 2862000 words at 10950 wps, loss = 3.882\n",
      "[batch 2975]: seen 2976000 words at 10964 wps, loss = 3.876\n",
      "[batch 3074]: seen 3075000 words at 10926 wps, loss = 3.871\n",
      "[batch 3180]: seen 3181000 words at 10912 wps, loss = 3.866\n",
      "[batch 3294]: seen 3295000 words at 10928 wps, loss = 3.861\n",
      "[batch 3410]: seen 3411000 words at 10948 wps, loss = 3.856\n",
      "[batch 3527]: seen 3528000 words at 10970 wps, loss = 3.851\n",
      "[batch 3646]: seen 3647000 words at 10996 wps, loss = 3.846\n",
      "[batch 3761]: seen 3762000 words at 11010 wps, loss = 3.842\n",
      "[batch 3878]: seen 3879000 words at 11029 wps, loss = 3.837\n",
      "[batch 3996]: seen 3997000 words at 11050 wps, loss = 3.833\n",
      "[batch 4108]: seen 4109000 words at 11052 wps, loss = 3.829\n",
      "[batch 4226]: seen 4227000 words at 11069 wps, loss = 3.825\n",
      "[batch 4342]: seen 4343000 words at 11082 wps, loss = 3.822\n",
      "[batch 4460]: seen 4461000 words at 11098 wps, loss = 3.818\n",
      "[batch 4576]: seen 4577000 words at 11109 wps, loss = 3.815\n",
      "[batch 4692]: seen 4693000 words at 11120 wps, loss = 3.812\n",
      "[batch 4788]: seen 4789000 words at 11083 wps, loss = 3.809\n",
      "[batch 4880]: seen 4881000 words at 11039 wps, loss = 3.806\n",
      "[batch 4975]: seen 4976000 words at 11002 wps, loss = 3.804\n",
      "[batch 5054]: seen 5055000 words at 10935 wps, loss = 3.802\n",
      "[batch 5140]: seen 5141000 words at 10885 wps, loss = 3.799\n",
      "[batch 5226]: seen 5227000 words at 10836 wps, loss = 3.797\n",
      "[batch 5314]: seen 5315000 words at 10794 wps, loss = 3.795\n",
      "[batch 5414]: seen 5415000 words at 10777 wps, loss = 3.793\n",
      "[batch 5508]: seen 5509000 words at 10749 wps, loss = 3.791\n",
      "[batch 5613]: seen 5614000 words at 10744 wps, loss = 3.788\n",
      "[batch 5711]: seen 5712000 words at 10726 wps, loss = 3.786\n",
      "[batch 5805]: seen 5806000 words at 10701 wps, loss = 3.784\n",
      "[batch 5912]: seen 5913000 words at 10700 wps, loss = 3.782\n",
      "[batch 6019]: seen 6020000 words at 10698 wps, loss = 3.780\n",
      "[batch 6124]: seen 6125000 words at 10694 wps, loss = 3.778\n",
      "[batch 6229]: seen 6230000 words at 10689 wps, loss = 3.776\n",
      "[batch 6334]: seen 6335000 words at 10685 wps, loss = 3.773\n",
      "[batch 6434]: seen 6435000 words at 10673 wps, loss = 3.771\n",
      "[batch 6531]: seen 6532000 words at 10656 wps, loss = 3.770\n",
      "[batch 6635]: seen 6636000 words at 10651 wps, loss = 3.767\n",
      "[batch 6745]: seen 6746000 words at 10656 wps, loss = 3.765\n",
      "[batch 6847]: seen 6848000 words at 10648 wps, loss = 3.764\n",
      "[batch 6941]: seen 6942000 words at 10628 wps, loss = 3.762\n",
      "[batch 7042]: seen 7043000 words at 10620 wps, loss = 3.760\n",
      "[batch 7140]: seen 7141000 words at 10606 wps, loss = 3.758\n",
      "[batch 7245]: seen 7246000 words at 10603 wps, loss = 3.756\n",
      "[batch 7356]: seen 7357000 words at 10609 wps, loss = 3.755\n",
      "[batch 7441]: seen 7442000 words at 10578 wps, loss = 3.753\n",
      "[batch 7535]: seen 7536000 words at 10561 wps, loss = 3.752\n",
      "[batch 7629]: seen 7630000 words at 10544 wps, loss = 3.750\n",
      "[batch 7735]: seen 7736000 words at 10544 wps, loss = 3.749\n",
      "[batch 7838]: seen 7839000 words at 10540 wps, loss = 3.748\n",
      "[batch 7934]: seen 7935000 words at 10526 wps, loss = 3.746\n",
      "[batch 8040]: seen 8041000 words at 10527 wps, loss = 3.745\n",
      "[batch 8149]: seen 8150000 words at 10531 wps, loss = 3.743\n",
      "[batch 8256]: seen 8257000 words at 10532 wps, loss = 3.741\n",
      "[batch 8362]: seen 8363000 words at 10533 wps, loss = 3.740\n",
      "[batch 8473]: seen 8474000 words at 10539 wps, loss = 3.738\n",
      "[batch 8584]: seen 8585000 words at 10545 wps, loss = 3.737\n",
      "[epoch 1] Completed in 0:13:43\n",
      "[epoch 1] Test set: avg. loss: 3.601  (perplexity: 36.65)\n",
      "\n",
      "Done training.\n"
     ]
    }
   ],
   "source": [
    "reload(rnnlm)\n",
    "reload(processing)\n",
    "train_model(corpus, true_training_data, \"true_all\", model_params, **train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding sentences...\n",
      "Processing sentences...\n",
      "Loaded 464182 sentences (7.00312e+06 tokens)\n",
      "Training set: 371345 sentences (5602901 tokens)\n",
      "Test set: 92837 sentences (1400223 tokens)\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "reload(rnnlm)\n",
    "reload(processing)\n",
    "false_training_data = corpus.generate_training_data(\n",
    "    comments_dataset[\"train_data\"][comments_dataset[\"train_data\"][\"hasVotes\"] == False][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] Starting epoch 1\n",
      "[batch 110]: seen 111000 words at 11020 wps, loss = 4.585\n",
      "[batch 218]: seen 219000 words at 10893 wps, loss = 4.411\n",
      "[batch 325]: seen 326000 words at 10803 wps, loss = 4.322\n",
      "[batch 425]: seen 426000 words at 10579 wps, loss = 4.252\n",
      "[batch 532]: seen 533000 words at 10594 wps, loss = 4.205\n",
      "[batch 644]: seen 645000 words at 10685 wps, loss = 4.163\n",
      "[batch 748]: seen 749000 words at 10636 wps, loss = 4.132\n",
      "[batch 856]: seen 857000 words at 10656 wps, loss = 4.105\n",
      "[batch 960]: seen 961000 words at 10621 wps, loss = 4.086\n",
      "[batch 1064]: seen 1065000 words at 10591 wps, loss = 4.068\n",
      "[batch 1171]: seen 1172000 words at 10593 wps, loss = 4.049\n",
      "[batch 1253]: seen 1254000 words at 10394 wps, loss = 4.034\n",
      "[batch 1341]: seen 1342000 words at 10270 wps, loss = 4.020\n",
      "[batch 1437]: seen 1438000 words at 10222 wps, loss = 4.009\n",
      "[batch 1542]: seen 1543000 words at 10238 wps, loss = 3.996\n",
      "[batch 1641]: seen 1642000 words at 10212 wps, loss = 3.987\n",
      "[batch 1736]: seen 1737000 words at 10170 wps, loss = 3.978\n",
      "[batch 1845]: seen 1846000 words at 10209 wps, loss = 3.968\n",
      "[batch 1945]: seen 1946000 words at 10197 wps, loss = 3.961\n",
      "[batch 2052]: seen 2053000 words at 10221 wps, loss = 3.951\n",
      "[batch 2157]: seen 2158000 words at 10231 wps, loss = 3.942\n",
      "[batch 2266]: seen 2267000 words at 10260 wps, loss = 3.934\n",
      "[batch 2367]: seen 2368000 words at 10251 wps, loss = 3.927\n",
      "[batch 2476]: seen 2477000 words at 10275 wps, loss = 3.921\n",
      "[batch 2588]: seen 2589000 words at 10308 wps, loss = 3.913\n",
      "[batch 2696]: seen 2697000 words at 10327 wps, loss = 3.908\n",
      "[batch 2804]: seen 2805000 words at 10342 wps, loss = 3.903\n",
      "[batch 2908]: seen 2909000 words at 10341 wps, loss = 3.898\n",
      "[batch 3015]: seen 3016000 words at 10351 wps, loss = 3.892\n",
      "[batch 3121]: seen 3122000 words at 10357 wps, loss = 3.887\n",
      "[batch 3216]: seen 3217000 words at 10327 wps, loss = 3.883\n",
      "[batch 3328]: seen 3329000 words at 10354 wps, loss = 3.879\n",
      "[batch 3434]: seen 3435000 words at 10359 wps, loss = 3.874\n",
      "[batch 3537]: seen 3538000 words at 10356 wps, loss = 3.870\n",
      "[batch 3641]: seen 3642000 words at 10355 wps, loss = 3.866\n",
      "[batch 3747]: seen 3748000 words at 10359 wps, loss = 3.862\n",
      "[batch 3847]: seen 3848000 words at 10348 wps, loss = 3.858\n",
      "[batch 3956]: seen 3957000 words at 10362 wps, loss = 3.855\n",
      "[batch 4059]: seen 4060000 words at 10359 wps, loss = 3.851\n",
      "[batch 4160]: seen 4161000 words at 10352 wps, loss = 3.847\n",
      "[batch 4269]: seen 4270000 words at 10363 wps, loss = 3.844\n",
      "[batch 4375]: seen 4376000 words at 10367 wps, loss = 3.841\n",
      "[batch 4485]: seen 4486000 words at 10380 wps, loss = 3.838\n",
      "[batch 4592]: seen 4593000 words at 10387 wps, loss = 3.835\n",
      "[batch 4701]: seen 4702000 words at 10397 wps, loss = 3.832\n",
      "[batch 4802]: seen 4803000 words at 10390 wps, loss = 3.829\n",
      "[batch 4908]: seen 4909000 words at 10393 wps, loss = 3.826\n",
      "[batch 5012]: seen 5013000 words at 10393 wps, loss = 3.824\n",
      "[batch 5117]: seen 5118000 words at 10394 wps, loss = 3.821\n",
      "[batch 5224]: seen 5225000 words at 10400 wps, loss = 3.818\n",
      "[batch 5328]: seen 5329000 words at 10399 wps, loss = 3.816\n",
      "[batch 5437]: seen 5438000 words at 10407 wps, loss = 3.814\n",
      "[batch 5537]: seen 5538000 words at 10399 wps, loss = 3.811\n",
      "[batch 5647]: seen 5648000 words at 10409 wps, loss = 3.809\n",
      "[batch 5755]: seen 5756000 words at 10416 wps, loss = 3.807\n",
      "[batch 5853]: seen 5854000 words at 10404 wps, loss = 3.804\n",
      "[batch 5964]: seen 5965000 words at 10416 wps, loss = 3.802\n",
      "[epoch 1] Completed in 0:09:33\n",
      "[epoch 1] Test set: avg. loss: 4.358  (perplexity: 78.08)\n",
      "\n",
      "Done training.\n"
     ]
    }
   ],
   "source": [
    "reload(rnnlm)\n",
    "reload(processing)\n",
    "train_model(corpus, false_training_data, \"false_all\", model_params, **train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for TRUE model:\n",
      "\"the quick brown fox jumps over the lazy dog\" : -17.79\n",
      "\"the fox quick brown jumps over the lazy dog\" : -17.79\n",
      "\"the fox quick brown jumps dog over the lazy\" : -18.07\n",
      "Scores for FALSE model:\n",
      "\"the quick brown fox jumps over the lazy dog\" : -21.76\n",
      "\"the fox quick brown jumps over the lazy dog\" : -21.76\n",
      "\"the fox quick brown jumps dog over the lazy\" : -21.76\n"
     ]
    }
   ],
   "source": [
    "reload(rnnlm)\n",
    "reload(processing)\n",
    "sents = [\"the quick brown fox jumps over the lazy dog\",\n",
    "         \"the fox quick brown jumps over the lazy dog\",\n",
    "        \"the fox quick brown jumps dog over the lazy\"]\n",
    "print \"Scores for TRUE model:\"\n",
    "rnnlm.load_and_score([s.split() for s in sents], corpus, model_params, \"tf_saved/rnnlm_true_all\")\n",
    "print \"Scores for FALSE model:\"\n",
    "rnnlm.load_and_score([s.split() for s in sents], corpus, model_params, \"tf_saved/rnnlm_false_all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "\n",
    "class RNNClassifierFeatureExtractor(object):\n",
    "    def __init__(self, corpus, model_params):\n",
    "        self.corpus = corpus\n",
    "        self.model_params = model_params\n",
    "        \n",
    "    def score_sentences(self, inputs, trained_filename):\n",
    "        with tf.Graph().as_default(), tf.Session() as session:\n",
    "            with tf.variable_scope(\"model\", reuse=None):\n",
    "                lm = rnnlm.RNNLM(self.model_params)\n",
    "                lm.BuildCoreGraph()\n",
    "\n",
    "            # Load the trained model\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(session, trained_filename)\n",
    "\n",
    "            # Actually run scoring\n",
    "            results = []\n",
    "            for idx, s in enumerate(inputs):\n",
    "                score = 0\n",
    "                for sent in processing.tokenize_sentences(s):\n",
    "                    score -= lm.ScoreSeq(session, sent, self.corpus.vocab)\n",
    "                results.append(score)\n",
    "                if idx % 1000 == 0:\n",
    "                    print \"  %d / %d\" % (idx, len(inputs))\n",
    "\n",
    "        return results\n",
    "        \n",
    "    def train(self, train_data):\n",
    "        return self.transform(train_data)\n",
    "        \n",
    "    def transform(self, test_data):\n",
    "        print \"True scores...\"\n",
    "        true_scores = self.score_sentences(test_data[\"content\"], \"tf_saved/rnnlm_true_all\")\n",
    "        print \"False scores...\"\n",
    "        false_scores = self.score_sentences(test_data[\"content\"], \"tf_saved/rnnlm_false_all\")\n",
    "        return scipy.sparse.csr_matrix([[x, y] for x, y in zip(true_scores, false_scores)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ka-comments-balanced dataset.\n",
      "Training feature extractor all-rnn.\n",
      "True scores...\n",
      "  0 / 48824\n",
      "  1000 / 48824\n",
      "  2000 / 48824\n",
      "  3000 / 48824\n",
      "  4000 / 48824\n",
      "  5000 / 48824\n",
      "  6000 / 48824\n",
      "  7000 / 48824\n",
      "  8000 / 48824\n",
      "  9000 / 48824\n",
      "  10000 / 48824\n",
      "  11000 / 48824\n",
      "  12000 / 48824\n",
      "  13000 / 48824\n",
      "  14000 / 48824\n",
      "  15000 / 48824\n",
      "  16000 / 48824\n",
      "  17000 / 48824\n",
      "  18000 / 48824\n",
      "  19000 / 48824\n",
      "  20000 / 48824\n",
      "  21000 / 48824\n",
      "  22000 / 48824\n",
      "  23000 / 48824\n",
      "  24000 / 48824\n",
      "  25000 / 48824\n",
      "  26000 / 48824\n",
      "  27000 / 48824\n",
      "  28000 / 48824\n",
      "  29000 / 48824\n",
      "  30000 / 48824\n",
      "  31000 / 48824\n",
      "  32000 / 48824\n",
      "  33000 / 48824\n",
      "  34000 / 48824\n",
      "  35000 / 48824\n",
      "  36000 / 48824\n",
      "  37000 / 48824\n",
      "  38000 / 48824\n",
      "  39000 / 48824\n",
      "  40000 / 48824\n",
      "  41000 / 48824\n",
      "  42000 / 48824\n",
      "  43000 / 48824\n",
      "  44000 / 48824\n",
      "  45000 / 48824\n",
      "  46000 / 48824\n",
      "  47000 / 48824\n",
      "  48000 / 48824\n",
      "False scores...\n",
      "  0 / 48824\n",
      "  1000 / 48824\n",
      "  2000 / 48824\n",
      "  3000 / 48824\n",
      "  4000 / 48824\n",
      "  5000 / 48824\n",
      "  6000 / 48824\n",
      "  7000 / 48824\n",
      "  8000 / 48824\n",
      "  9000 / 48824\n",
      "  10000 / 48824\n",
      "  11000 / 48824\n",
      "  12000 / 48824\n",
      "  13000 / 48824\n",
      "  14000 / 48824\n",
      "  15000 / 48824\n",
      "  16000 / 48824\n",
      "  17000 / 48824\n",
      "  18000 / 48824\n",
      "  19000 / 48824\n",
      "  20000 / 48824\n",
      "  21000 / 48824\n",
      "  22000 / 48824\n",
      "  23000 / 48824\n",
      "  24000 / 48824\n",
      "  25000 / 48824\n",
      "  26000 / 48824\n",
      "  27000 / 48824\n",
      "  28000 / 48824\n",
      "  29000 / 48824\n",
      "  30000 / 48824\n",
      "  31000 / 48824\n",
      "  32000 / 48824\n",
      "  33000 / 48824\n",
      "  34000 / 48824\n",
      "  35000 / 48824\n",
      "  36000 / 48824\n",
      "  37000 / 48824\n",
      "  38000 / 48824\n",
      "  39000 / 48824\n",
      "  40000 / 48824\n",
      "  41000 / 48824\n",
      "  42000 / 48824\n",
      "  43000 / 48824\n",
      "  44000 / 48824\n",
      "  45000 / 48824\n",
      "  46000 / 48824\n",
      "  47000 / 48824\n",
      "  48000 / 48824\n",
      "Generating validation set...\n",
      "True scores...\n",
      "  0 / 5000\n",
      "  1000 / 5000\n",
      "  2000 / 5000\n",
      "  3000 / 5000\n",
      "  4000 / 5000\n",
      "False scores...\n",
      "  0 / 5000\n",
      "  1000 / 5000\n",
      "  2000 / 5000\n",
      "  3000 / 5000\n",
      "  4000 / 5000\n",
      "Generating test set...\n",
      "True scores...\n",
      "  0 / 5000\n",
      "  1000 / 5000\n",
      "  2000 / 5000\n",
      "  3000 / 5000\n",
      "  4000 / 5000\n",
      "False scores...\n",
      "  0 / 5000\n",
      "  1000 / 5000\n",
      "  2000 / 5000\n",
      "  3000 / 5000\n",
      "  4000 / 5000\n",
      "Writing to disk...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "reload(rnnlm)\n",
    "reload(processing)\n",
    "reload(common)\n",
    "common.extract_features(\n",
    "    \"ka-comments-balanced\",\n",
    "    RNNClassifierFeatureExtractor(corpus, model_params),\n",
    "    \"all-rnn\", sampling=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features all-rnn.\n",
      "Training models.\n",
      "##        MultinomialNB         all-rnn accuracy: 55.7 %\n",
      "##            LinearSVC         all-rnn accuracy: 50.0 %\n",
      "##                  MLP         all-rnn accuracy: 59.2 %\n",
      "##                 MLP2         all-rnn accuracy: 60.2 %\n"
     ]
    }
   ],
   "source": [
    "reload(common)\n",
    "common.test_features(\"all-rnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
